{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d69954",
   "metadata": {},
   "source": [
    "### ğŸŒ Weekly Macro Indicator Download\n",
    "\n",
    "This section downloads weekly data for key macroeconomic signals that are used as input features for the model:\n",
    "\n",
    "| Indicator        | Source Symbol | Description |\n",
    "|------------------|---------------|-------------|\n",
    "| **VIX**          | `^VIX`        | CBOE Volatility Index (market fear gauge) |\n",
    "| **10Y Yield**    | `^TNX`        | 10-Year U.S. Treasury yield (interest rate proxy) |\n",
    "| **USD Index**    | `DX-Y.NYB`    | Strength of the U.S. dollar |\n",
    "| **Crude Oil**    | `CL=F`        | WTI Crude Oil futures price |\n",
    "\n",
    "All indicators are:\n",
    "- Downloaded at **weekly frequency** using Yahoo Finance\n",
    "- Aligned on the same date index as the ETF data\n",
    "- The 10-year yield is converted to a % by multiplying by `0.1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36deb115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "YEARS = 10\n",
    "\n",
    "# ETF list\n",
    "etf_list = [\n",
    "    'XLK', 'XLF', 'XLV', 'XLE', 'XLI', 'XLY', 'XLP', 'XLRE', 'XLU', 'XLB', 'XLC',\n",
    "    'SOXX', 'SH', 'DOG', 'RWM', 'ITA', 'JETS', 'PSQ', 'VNQ', 'SPY'\n",
    "]\n",
    "\n",
    "# Date range\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.today() - timedelta(weeks=YEARS*52)).strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"ğŸ“… Downloading data from {start_date} to {end_date}\")\n",
    "\n",
    "# Ensure dataset/ exists\n",
    "dataset_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset'))\n",
    "if not os.path.isdir(dataset_path):\n",
    "    raise FileNotFoundError(f\"ğŸš« 'dataset/' folder not found at {dataset_path}\")\n",
    "\n",
    "# Containers\n",
    "adjclose_data, volume_data, high_data, low_data = {}, {}, {}, {}\n",
    "\n",
    "# Download each ETF\n",
    "for symbol in etf_list:\n",
    "    print(f\"â¬‡ï¸ Downloading {symbol}...\")\n",
    "    data = yf.download(symbol, start=start_date, end=end_date, interval='1wk', auto_adjust=False)\n",
    "    if not data.empty:\n",
    "        adjclose_data[symbol] = data[['Adj Close']].rename(columns={'Adj Close': symbol})\n",
    "        volume_data[symbol] = data[['Volume']].rename(columns={'Volume': symbol})\n",
    "        high_data[symbol] = data[['High']].rename(columns={'High': symbol})\n",
    "        low_data[symbol] = data[['Low']].rename(columns={'Low': symbol})\n",
    "\n",
    "# Merge and clean\n",
    "def combine_and_save(data_dict, filename):\n",
    "    df = pd.concat(data_dict.values(), axis=1)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "    df = df[~df.index.duplicated(keep='first')].sort_index()\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    path = os.path.join(dataset_path, filename)\n",
    "    df.to_csv(path)\n",
    "    print(f\"âœ… Saved: {filename}\")\n",
    "    return df\n",
    "\n",
    "# Save all\n",
    "price_df = combine_and_save(adjclose_data, 'etf_prices_weekly.csv')\n",
    "volume_df = combine_and_save(volume_data, 'etf_volume_weekly.csv')\n",
    "high_df = combine_and_save(high_data, 'etf_high_weekly.csv')\n",
    "low_df = combine_and_save(low_data, 'etf_low_weekly.csv')\n",
    "\n",
    "# Preview\n",
    "price_df.head()\n",
    "\n",
    "# Macro indicator tickers on Yahoo Finance\n",
    "macro_tickers = {\n",
    "    'VIX': '^VIX',               # Volatility Index\n",
    "    '10Y_Yield': '^TNX',         # 10-Year Treasury Yield (multiply by 0.1)\n",
    "    'USD_Index': 'DX-Y.NYB',     # U.S. Dollar Index\n",
    "    'WTI_Crude': 'CL=F'          # Crude Oil (WTI)\n",
    "}\n",
    "\n",
    "# Date range matching your ETF backtest period\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.today() - timedelta(weeks=YEARS*52)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Download weekly data\n",
    "macro_data = {}\n",
    "for name, ticker in macro_tickers.items():\n",
    "    print(f\"Downloading {name} ({ticker})...\")\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, interval='1wk', auto_adjust=False)\n",
    "    macro_data[name] = data[['Close']].rename(columns={'Close': name})\n",
    "\n",
    "# Combine all macro indicators into one DataFrame\n",
    "macro_df = pd.concat(macro_data.values(), axis=1)\n",
    "\n",
    "# Fix 10Y yield scale\n",
    "if '10Y_Yield' in macro_df.columns:\n",
    "    macro_df['10Y_Yield'] = macro_df['10Y_Yield'] * 0.1\n",
    "\n",
    "# Drop missing rows\n",
    "macro_df.dropna(inplace=True)\n",
    "macro_df.columns = pd.Index(list(macro_tickers.keys()))\n",
    "\n",
    "# Save to CSV\n",
    "macro_save_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'macro_indicators_weekly.csv'))\n",
    "macro_df.to_csv(macro_save_path)\n",
    "print(f\"âœ… Macro indicators saved to: {macro_save_path}\")\n",
    "\n",
    "# Preview\n",
    "\n",
    "# macro_df = macro_df.apply(pd.to_numeric, errors='coerce')\n",
    "# macro_df.index = pd.to_datetime(macro_df.index)\n",
    "macro_df = macro_df[~macro_df.index.duplicated(keep='first')]\n",
    "macro_df.sort_index(inplace=True)\n",
    "macro_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02b9dd",
   "metadata": {},
   "source": [
    "### ğŸ§  Feature Engineering\n",
    "\n",
    "This section prepares input features for the machine learning model.\n",
    "\n",
    "#### ğŸ“ˆ ETF-Specific Features:\n",
    "For each ETF, we will compute:\n",
    "- **1-week return**: Short-term price movement\n",
    "- **3-week return**: Medium-term trend\n",
    "- **6-week return**: Momentum across a longer window\n",
    "- **Streak**: Number of consecutive up weeks\n",
    "\n",
    "#### ğŸŒ Macro Indicators:\n",
    "From the macro_df, we already have:\n",
    "- **VIX**\n",
    "- **10Y Treasury Yield**\n",
    "- **USD Index**\n",
    "- **Crude Oil Price**\n",
    "\n",
    "These will be aligned with the ETF data by date and merged in.\n",
    "\n",
    "#### ğŸ“¦ Resulting Feature Matrix:\n",
    "For each ETF on each week:\n",
    "- One row = a snapshot of that ETF and macro environment\n",
    "- Target = the **next week's return** for that ETF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c065fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from ta import momentum, trend, volume\n",
    "\n",
    "# === Paths ===\n",
    "price_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'etf_prices_weekly.csv'))\n",
    "volume_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'etf_volume_weekly.csv'))\n",
    "num_etf = 0\n",
    "\n",
    "# === Helper to load ETF CSVs ===\n",
    "def load_etf_csv(path, name='[unknown]'):\n",
    "    global num_etf\n",
    "    try:\n",
    "        header_row = pd.read_csv(path, header=None, nrows=2)\n",
    "        columns = header_row.iloc[1].tolist()[1:]\n",
    "        df = pd.read_csv(path, skiprows=3, header=None)\n",
    "        df = df.iloc[:, :len(columns) + 1]\n",
    "        df.columns = ['Date'] + columns\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        df = df.set_index('Date')\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "        num_etf = len(columns)\n",
    "        print(f\"âœ… Loaded {name} with {len(columns)} tickers\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load {name}: {e}\")\n",
    "        raise\n",
    "\n",
    "# === Load data ===\n",
    "price_df = load_etf_csv(price_path, name='ETF Prices')\n",
    "volume_df = load_etf_csv(volume_path, name='ETF Volume')\n",
    "\n",
    "# === Feature storage ===\n",
    "features_all = []\n",
    "skipped = []\n",
    "\n",
    "# === Feature generation loop ===\n",
    "for symbol in price_df.columns:\n",
    "    if symbol not in volume_df.columns:\n",
    "        print(f\"âš ï¸ Skipping {symbol}: volume data missing.\")\n",
    "        skipped.append(symbol)\n",
    "        continue\n",
    "\n",
    "    df = pd.DataFrame(index=price_df.index)\n",
    "    df['close'] = price_df[symbol]\n",
    "    df['volume'] = volume_df[symbol]\n",
    "\n",
    "    try:\n",
    "        # === Return-based technical indicators ===\n",
    "        df[f'{symbol}_ret_1w'] = df['close'].pct_change(1)\n",
    "        df[f'{symbol}_ret_3w'] = df['close'].pct_change(3)\n",
    "        df[f'{symbol}_ret_6w'] = df['close'].pct_change(6)\n",
    "\n",
    "        high = df['close'].rolling(window=14).max()\n",
    "        low = df['close'].rolling(window=14).min()\n",
    "        df[f'{symbol}_stoch_k'] = 100 * (df['close'] - low) / (high - low)\n",
    "        df[f'{symbol}_stoch_d'] = df[f'{symbol}_stoch_k'].rolling(window=3).mean()\n",
    "        df[f'{symbol}_williams_r'] = -100 * (high - df['close']) / (high - low)\n",
    "\n",
    "        df[f'{symbol}_cci'] = trend.cci(high=df['close'], low=df['close'], close=df['close'], window=20)\n",
    "        df[f'{symbol}_rsi'] = momentum.rsi(df['close'], window=14)\n",
    "        df[f'{symbol}_obv'] = volume.on_balance_volume(df['close'], df['volume'])\n",
    "\n",
    "        df[f'{symbol}_macd'] = trend.macd(df['close'])\n",
    "        df[f'{symbol}_macd_signal'] = trend.macd_signal(df['close'])\n",
    "        df[f'{symbol}_macd_diff'] = trend.macd_diff(df['close'])\n",
    "\n",
    "        # === Price/volume and its variation ===\n",
    "        df[f'{symbol}_price_change'] = df['close'].pct_change(1)\n",
    "        df[f'{symbol}_volume_change'] = df['volume'].pct_change(1)\n",
    "\n",
    "        # === Short-term KST (custom) ===\n",
    "        roc1 = df['close'].pct_change(10)\n",
    "        roc2 = df['close'].pct_change(15)\n",
    "        roc3 = df['close'].pct_change(20)\n",
    "        roc4 = df['close'].pct_change(30)\n",
    "        df[f'{symbol}_kst_short'] = (\n",
    "            roc1.rolling(10).mean() +\n",
    "            roc2.rolling(10).mean() * 2 +\n",
    "            roc3.rolling(10).mean() * 3 +\n",
    "            roc4.rolling(15).mean() * 4\n",
    "        )\n",
    "\n",
    "        derived_cols = df.columns.difference(['close', 'volume'])\n",
    "        feature_df = df[derived_cols].copy()\n",
    "\n",
    "        features_all.append(feature_df)\n",
    "        print(f\"ğŸ“ˆ {symbol}: {feature_df.dropna(how='all').shape[0]} valid rows\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {symbol}: {e}\")\n",
    "        skipped.append(symbol)\n",
    "\n",
    "# === Final merge ===\n",
    "if len(features_all) == 0:\n",
    "    raise ValueError(\"ğŸ›‘ No valid ETF features generated.\")\n",
    "\n",
    "features_df = pd.concat(features_all, axis=1).sort_index()\n",
    "features_df = features_df[~features_df.index.duplicated(keep='first')]\n",
    "\n",
    "print(\"ğŸ“€ Final feature shape:\", features_df.shape)\n",
    "\n",
    "# === Save to CSV ===\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset'))\n",
    "fname = os.path.join(base_dir, f'weekly_{num_etf}_etf_tech_features.csv')\n",
    "features_df.to_csv(fname)\n",
    "print(f\"âœ… Saved features to: {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39898fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abandoned\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# === Load datasets ===\n",
    "price_df = pd.read_csv('../dataset/etf_prices_weekly.csv', index_col=0)\n",
    "volume_df = pd.read_csv('../dataset/etf_volume_weekly.csv', index_col=0)\n",
    "high_df = pd.read_csv('../dataset/etf_high_weekly.csv', index_col=0)\n",
    "low_df = pd.read_csv('../dataset/etf_low_weekly.csv', index_col=0)\n",
    "\n",
    "# === Clean and convert ===\n",
    "for df in [price_df, volume_df, high_df, low_df]:\n",
    "    df[:] = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "    df.sort_index(inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "# === Feature generation ===\n",
    "feature_rows = []\n",
    "\n",
    "for symbol in price_df.columns:\n",
    "    close = price_df[symbol]\n",
    "    high = high_df[symbol]\n",
    "    low = low_df[symbol]\n",
    "    volume = pd.to_numeric(volume_df[symbol], errors='coerce').replace(0, np.nan)\n",
    "\n",
    "    returns_1w = close.pct_change(1)\n",
    "    returns_3w = close.pct_change(3)\n",
    "    returns_6w = close.pct_change(6)\n",
    "    streak = (close.pct_change(1) > 0).astype(int).rolling(3).sum()\n",
    "\n",
    "    log_volume = np.log(volume)\n",
    "    log_volume_norm = log_volume / log_volume.rolling(5).mean()\n",
    "\n",
    "    shock_amplify_raw = (high - low) / close\n",
    "    shock_amplify = shock_amplify_raw.rolling(3).mean()\n",
    "    shock_amplify_1w = shock_amplify.shift(1)\n",
    "    shock_amplify_3w = shock_amplify.rolling(3).mean()\n",
    "    shock_delta = shock_amplify.diff()\n",
    "\n",
    "    vol_flag = (\n",
    "        shock_amplify_raw > (shock_amplify_raw.rolling(10).mean() +\n",
    "                             2 * shock_amplify_raw.rolling(10).std())\n",
    "    ).astype(int)\n",
    "\n",
    "    rsv = (close - low.rolling(9).min()) / (high.rolling(9).max() - low.rolling(9).min()) * 100\n",
    "    k = rsv.ewm(com=2).mean()\n",
    "    d = k.ewm(com=2).mean()\n",
    "\n",
    "    kd_signal = pd.Series(0, index=close.index)\n",
    "    kd_signal[(k < 30) & (d < 30)] = 1\n",
    "    kd_signal[(k > 70) & (d > 70)] = -1\n",
    "\n",
    "    ema12 = close.ewm(span=12).mean()\n",
    "    ema26 = close.ewm(span=26).mean()\n",
    "    macd = ema12 - ema26\n",
    "    macd_slope = macd.diff()\n",
    "\n",
    "    momentum_2w = close.pct_change(2)\n",
    "\n",
    "    kd_x_shock = kd_signal * shock_amplify_3w\n",
    "    streak_x_ret6 = streak * returns_6w\n",
    "\n",
    "    for i in range(len(close)):\n",
    "        date = close.index[i]\n",
    "        try:\n",
    "            nearest_macro_index = macro_df.index.get_indexer([date], method='nearest')[0]\n",
    "            macro_row = macro_df.iloc[nearest_macro_index]\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        row = {\n",
    "            'Date': date,\n",
    "            'ETF': symbol,\n",
    "            'Return_1w': returns_1w.iloc[i],\n",
    "            'Return_3w': returns_3w.iloc[i],\n",
    "            'Return_6w': returns_6w.iloc[i],\n",
    "            'Streak_Up': streak.iloc[i],\n",
    "            'LogVolumeNorm': log_volume_norm.iloc[i],\n",
    "            'Shock_Amplify': shock_amplify.iloc[i],\n",
    "            'Shock_Amplify_1w': shock_amplify_1w.iloc[i],\n",
    "            'Shock_Amplify_3w': shock_amplify_3w.iloc[i],\n",
    "            'Shock_Delta': shock_delta.iloc[i],\n",
    "            'Vol_Flag': vol_flag.iloc[i],\n",
    "            'KD_Signal': kd_signal.iloc[i],\n",
    "            'MACD': macd.iloc[i],\n",
    "            'MACD_Slope': macd_slope.iloc[i],\n",
    "            'ROC_5w': close.pct_change(5).iloc[i],\n",
    "            'Momentum_2w': momentum_2w.iloc[i],\n",
    "            'KD_Signal_x_Shock3w': kd_x_shock.iloc[i],\n",
    "            'Streak_x_Return6w': streak_x_ret6.iloc[i],\n",
    "            'Target_Next_Week_Return': close.pct_change(1).shift(-1).iloc[i],\n",
    "            'Direction': (close.pct_change(1).shift(-1).iloc[i] > 0).astype(int),\n",
    "        }\n",
    "\n",
    "        feature_rows.append(row)\n",
    "\n",
    "# === Assemble + Clip Outliers ===\n",
    "feature_df = pd.DataFrame(feature_rows)\n",
    "feature_df.dropna(inplace=True)\n",
    "\n",
    "# Clip each feature to 1stâ€“99th percentile\n",
    "for col in feature_df.columns:\n",
    "    if col not in ['Date', 'ETF', 'Direction']:\n",
    "        lower = feature_df[col].quantile(0.01)\n",
    "        upper = feature_df[col].quantile(0.99)\n",
    "        feature_df[col] = feature_df[col].clip(lower, upper)\n",
    "\n",
    "# Optional: Flag outlier conditions\n",
    "values = feature_df.drop(columns=['Date', 'ETF', 'Direction'])\n",
    "z_scores = (values - values.mean()) / values.std()\n",
    "del values\n",
    "feature_df['Edge_Flag'] = (np.abs(z_scores) > 2.5).sum(axis=1) > 3\n",
    "\n",
    "# Save\n",
    "feature_df.to_csv('../dataset/etf_features.csv', index=False)\n",
    "print(\"âœ… Feature CSV with outlier clipping and edge flag saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e3f2fb",
   "metadata": {},
   "source": [
    "### ğŸ“Œ Deep Sector Rotation Strategy with Shock-Aware Early Exit\n",
    "\n",
    "This strategy builds on the \"Deep Sector Rotation\" approach proposed in [SSRN-4280640](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4280640), with the following modifications:\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§  Core Model (MLP)\n",
    "\n",
    "- A multi-layer perceptron (MLP) is trained to predict next-week returns for each ETF independently.\n",
    "- Features include:\n",
    "  - Past 1w, 3w, 6w returns\n",
    "  - Volume (log normalized)\n",
    "  - Macro indicators (VIX, 10Y yield, USD index, oil)\n",
    "  - Streak up count (3-week up trend)\n",
    "  - Shock Amplify features:\n",
    "    - This week\n",
    "    - 1-week lag\n",
    "    - 3-week average\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ” Weekly Rotation Rule (baseline)\n",
    "\n",
    "- Each week (e.g., Monday), predict returns for all ETFs using the MLP.\n",
    "- Rank the ETFs by predicted return.\n",
    "- Buy top-N (e.g., 3) ETFs.\n",
    "- Hold for 1 week (unless overridden by shock rule below).\n",
    "\n",
    "---\n",
    "\n",
    "#### âš¡ Shock Amplify Early Exit Rule (custom addition)\n",
    "\n",
    "- Each day (or evaluation step), check for ETFs in the portfolio with:\n",
    "  - `Shock_Amplify_3w` > +10% or < -10%\n",
    "- If triggered:\n",
    "  - Sell that ETF immediately.\n",
    "  - Immediately start a new turn (predict again, re-select top-N).\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ’¼ Goal\n",
    "\n",
    "- Combine deep learning-based prediction with handcrafted rules for volatility control.\n",
    "- Achieve more stable and responsive ETF swing trading performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DualTransformerModel(nn.Module):\n",
    "#     def __init__(self, num_features, hidden_dim=128, num_layers=2):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Choose the highest possible nhead divisor of num_features\n",
    "#         possible_heads = [h for h in [8, 4, 2, 1] if num_features % h == 0]\n",
    "#         nhead = possible_heads[0]\n",
    "\n",
    "#         self.encoder = nn.TransformerEncoder(\n",
    "#             nn.TransformerEncoderLayer(\n",
    "#                 d_model=num_features,\n",
    "#                 nhead=nhead,\n",
    "#                 dim_feedforward=hidden_dim,\n",
    "#                 batch_first=True\n",
    "#             ),\n",
    "#             num_layers=num_layers\n",
    "#         )\n",
    "#         self.attn_query = nn.Parameter(torch.randn(1, 1, num_features))\n",
    "#         self.output = nn.Linear(num_features, 1)\n",
    "\n",
    "#     def forward(self, x1, x2):\n",
    "#         x = torch.concat((x1, x2), dim=-1)\n",
    "#         enc = self.encoder(x)\n",
    "\n",
    "#         # Apply learnable attention pooling\n",
    "#         attn_scores = torch.matmul(self.attn_query, enc.transpose(1, 2))  # (1, 1, F) x (B, F, T) = (B, 1, T)\n",
    "#         attn_weights = torch.softmax(attn_scores, dim=-1)  # (B, 1, T)\n",
    "#         pooled = torch.bmm(attn_weights, enc).squeeze(1)  # (B, 1, T) x (B, T, F) = (B, 1, F) â†’ (B, F)\n",
    "\n",
    "#         return self.output(pooled).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "from heapq import heappush, heappushpop\n",
    "from torch.utils.data import DataLoader\n",
    "from model_definition import StockDataset, StockPredictionTransformer\n",
    "\n",
    "def clear_line(n=1):\n",
    "    LINE_UP = '\\033[1A'\n",
    "    LINE_CLEAR = '\\x1b[2K'\n",
    "    for _ in range(n):\n",
    "        print(LINE_UP, end=LINE_CLEAR)\n",
    "\n",
    "# è¶…åƒæ•¸è¨­å®š\n",
    "INPUT_DIM = 32\n",
    "ETF_EMBEDDING_DIM = 32\n",
    "TRANSFORMER_DIM = 64\n",
    "OUTPUT_DIM = 1 # é æ¸¬ä¸‹é€±æ¼²è·Œå¹…åº¦ (å–®ä¸€æ•¸å€¼)\n",
    "SEQ_LEN = 6\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 2\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1E-3\n",
    "NUM_EPOCHS = 300\n",
    "SAMPLING_INTERVAL = 2\n",
    "# 1. è³‡æ–™æº–å‚™ \n",
    "TEST_PERCENTAGE = 0.25\n",
    "\n",
    "etf_list = [\n",
    "    'XLK', 'XLF', 'XLV', 'XLE', 'XLI', 'XLY', 'XLP', 'XLRE', 'XLU', 'XLB', 'XLC',\n",
    "    'SOXX', 'SH', 'DOG', 'RWM', 'ITA', 'JETS', 'PSQ', 'VNQ', 'SPY'\n",
    "]\n",
    "\n",
    "# ç¯„ä¾‹ ETF è³‡æ–™ (DataFrame) - å‡è¨­ä½ çš„ ETF è³‡æ–™æ˜¯ DataFrame æ ¼å¼ï¼Œæ¯è¡Œä»£è¡¨ä¸€å¤©ä¸€å€‹ ETF çš„è³‡æ–™\n",
    "# å¯¦éš›æƒ…æ³ä½ éœ€è¦å¾ä½ çš„è³‡æ–™ä¾†æºè¼‰å…¥\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset'))\n",
    "feature_file = [res.group(0) for f in os.listdir(base_dir) if (res := re.search(r'weekly_(\\d+)_etf_tech_features.csv', f)) is not None]\n",
    "etf_data = pd.read_csv(os.path.join(base_dir, feature_file[0]))\n",
    "\n",
    "# ç¯„ä¾‹ Macro æŒ‡æ¨™è³‡æ–™ (DataFrame) - å‡è¨­ä½ çš„ Macro æŒ‡æ¨™è³‡æ–™æ˜¯ DataFrame æ ¼å¼ï¼Œæ¯è¡Œä»£è¡¨ä¸€å¤©çš„ Macro æŒ‡æ¨™\n",
    "# å¯¦éš›æƒ…æ³ä½ éœ€è¦å¾ä½ çš„è³‡æ–™ä¾†æºè¼‰å…¥\n",
    "macro_data = pd.read_csv(os.path.join(base_dir, 'macro_indicators_weekly.csv'), index_col=0)\n",
    "\n",
    "# å»ºç«‹è³‡æ–™é›†å’Œè³‡æ–™è¼‰å…¥å™¨\n",
    "dataset = StockDataset(etf_list, etf_data, macro_data, \n",
    "                       test_percentage=TEST_PERCENTAGE, \n",
    "                       sequence_length=SEQ_LEN, \n",
    "                       sampling_interval=SAMPLING_INTERVAL)\n",
    "dataset.train()\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataset.train()\n",
    "test_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False) # batch_size å¯ä»¥èˆ‡è¨“ç·´æ™‚ç›¸åŒï¼Œshuffle=False\n",
    "\n",
    "class customLoss():\n",
    "    def __init__(self):\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.mae_loss = nn.L1Loss()\n",
    "\n",
    "    def __call__(self, pred, y):\n",
    "        mse = self.mse_loss(pred, y)\n",
    "        direction_loss = 1 - (torch.sign(pred) == torch.sign(y)).float().mean()\n",
    "        return 0.5 * mse +  0.5 * direction_loss\n",
    "\n",
    "# å»ºç«‹æ¨¡å‹ã€æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = StockPredictionTransformer(ETF_EMBEDDING_DIM,\n",
    "                                   dataset.num_etf_features, \n",
    "                                   dataset.num_macros,\n",
    "                                   INPUT_DIM,\n",
    "                                   TRANSFORMER_DIM,\n",
    "                                   SEQ_LEN, \n",
    "                                   OUTPUT_DIM,\n",
    "                                   len(etf_list),\n",
    "                                   NUM_HEADS, \n",
    "                                   NUM_LAYERS).to(device)\n",
    "\n",
    "model.device = device\n",
    "criterion = customLoss() # å‡æ–¹èª¤å·®æå¤±å‡½æ•¸ (å›æ­¸ä»»å‹™)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=np.sqrt(np.exp(-1)), patience=20, min_lr=1E-7\n",
    ")\n",
    "\n",
    "k = 3\n",
    "best_k_models = []\n",
    "\n",
    "# è¨“ç·´è¿´åœˆ\n",
    "iterator = trange(NUM_EPOCHS)\n",
    "best_test_loss_epoch = [1E308, 0]\n",
    "for epoch in iterator:\n",
    "    total_loss = 0\n",
    "\n",
    "    # Train\n",
    "    model.train() # è¨­å®šæ¨¡å‹ç‚ºè¨“ç·´æ¨¡å¼\n",
    "    dataset.train()\n",
    "    for batch in dataloader:\n",
    "        etf_features = batch['etf_features'].to(model.device) # [seq_len, feature_dim]\n",
    "        macro_features = batch['macro_features'].to(model.device) # [seq_len, feature_dim]\n",
    "        targets = batch['targets'].to(model.device) # [seq_len, 1]\n",
    "        etf_indices = batch['etf_index'].to(model.device)\n",
    "\n",
    "        # å‰å‘å‚³æ’­\n",
    "        outputs = model(etf_features, macro_features, etf_indices) # [output_dim]\n",
    "\n",
    "        # è¨ˆç®—æå¤± (åªå–æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„ç›®æ¨™å€¼é€²è¡Œæ¯”è¼ƒï¼Œç¯„ä¾‹ç°¡åŒ–è™•ç†)\n",
    "        loss = criterion(outputs, targets) # targets[-1] å–æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„ç›®æ¨™å€¼ï¼Œä¸¦ç§»é™¤ batch_size ç¶­åº¦\n",
    "\n",
    "        # åå‘å‚³æ’­å’Œå„ªåŒ–\n",
    "        optimizer.zero_grad() # æ¸…ç©ºæ¢¯åº¦\n",
    "        loss.backward() # åå‘å‚³æ’­è¨ˆç®—æ¢¯åº¦\n",
    "        optimizer.step() # æ›´æ–°æ¨¡å‹åƒæ•¸\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader) \n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    dataset.test()\n",
    "    with torch.no_grad(): # é—œé–‰æ¢¯åº¦è¨ˆç®—\n",
    "        test_loss = 0\n",
    "\n",
    "        for batch in test_dataloader:\n",
    "            etf_features = batch['etf_features'].to(model.device)\n",
    "            macro_features = batch['macro_features'].to(model.device)\n",
    "            targets = batch['targets'].to(model.device)\n",
    "            etf_indices = batch['etf_index'].to(model.device)\n",
    "\n",
    "            # å‰å‘å‚³æ’­\n",
    "            outputs = model(etf_features, macro_features, etf_indices) # [output_dim]\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(dataloader) \n",
    "\n",
    "    if len(best_k_models) >= k:\n",
    "        heappushpop(best_k_models, (-avg_test_loss, model.state_dict()))\n",
    "    else:\n",
    "        heappush(best_k_models, (-avg_test_loss, model.state_dict()))\n",
    "\n",
    "    best_test_loss_epoch = [avg_test_loss, epoch + 1] if max(model[0] for model in best_k_models) == -avg_test_loss else best_test_loss_epoch\n",
    "    msg = f\"Epoch {epoch+1} lr={scheduler.get_last_lr()[0]:.2e}, Average Loss: {avg_loss:.2e}\" + \\\n",
    "          f\", Average Test Loss: {avg_test_loss:.2e}\" + \\\n",
    "          f\", Best test loss {best_test_loss_epoch[0]:.2e} at epoch {best_test_loss_epoch[1]} \"\n",
    "    iterator.set_description(msg)\n",
    "\n",
    "print(f\"Loss of top {k} best models: {[f'{-m[0]:.2e}' for m in sorted(best_k_models, key=lambda x: -x[0])]}\")\n",
    "model.load_state_dict(best_k_models[0][1])\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model:nn.Module, dataset:StockDataset, test=True):\n",
    "    # è¨­å®šæ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼\n",
    "    model.eval()\n",
    "\n",
    "    # æº–å‚™æ¸¬è©¦è³‡æ–™é›†å’Œè³‡æ–™è¼‰å…¥å™¨\n",
    "    if test:\n",
    "        dataset.test()\n",
    "    else:\n",
    "        dataset.train() # ä½¿ç”¨èˆ‡è¨“ç·´é›†ç›¸åŒçš„ ETF åˆ—è¡¨\n",
    "    test_dataloader = DataLoader(dataset, batch_size=32, shuffle=False) # batch_size å¯ä»¥èˆ‡è¨“ç·´æ™‚ç›¸åŒï¼Œshuffle=False\n",
    "\n",
    "    predictions = {} # å„²å­˜æ‰€æœ‰é æ¸¬çµæœ\n",
    "    actual_targets = {} # å„²å­˜æ‰€æœ‰çœŸå¯¦ç›®æ¨™å€¼\n",
    "    date_of_inputs = {}\n",
    "\n",
    "    with torch.no_grad(): # é—œé–‰æ¢¯åº¦è¨ˆç®—\n",
    "        for batch in test_dataloader:\n",
    "            etf_features = batch['etf_features'].to(model.device)\n",
    "            macro_features = batch['macro_features'].to(model.device)\n",
    "            targets = batch['targets'].to(model.device)\n",
    "            dates = batch['dates']\n",
    "            etf_symbol = batch['etf_symbol']\n",
    "            etf_indices = batch['etf_index'].to(model.device)\n",
    "\n",
    "            # å‰å‘å‚³æ’­\n",
    "            outputs = model(etf_features, macro_features, etf_indices) # [output_dim]\n",
    "\n",
    "            # å°‡é æ¸¬çµæœå’ŒçœŸå¯¦ç›®æ¨™å€¼è½‰æ›ç‚º NumPy array ä¸¦å„²å­˜\n",
    "            for etf, d, pred, real in zip(etf_symbol, dates, outputs.cpu().numpy(), targets.cpu().numpy()):\n",
    "                predictions[etf] = np.hstack([predictions.setdefault(etf, []), pred])\n",
    "                actual_targets[etf] = np.hstack([actual_targets.setdefault(etf, []), real])\n",
    "                date_of_inputs.setdefault(etf, []).append(d.split(',')[-1])\n",
    "\n",
    "    # # å°‡é æ¸¬çµæœå’ŒçœŸå¯¦ç›®æ¨™å€¼åˆ—è¡¨è½‰æ›ç‚º NumPy array\n",
    "\n",
    "    for etf in predictions:\n",
    "        predictions[etf] = np.array(predictions[etf])\n",
    "        actual_targets[etf] = np.array(actual_targets[etf]).flatten()\n",
    "        date_of_inputs[etf] = np.array(date_of_inputs[etf]).flatten()\n",
    "\n",
    "    return predictions, actual_targets, date_of_inputs\n",
    "\n",
    "predictions, actual_targets, date_of_inputs = evaluation(model, dataset, test=False)\n",
    "predictions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9759f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "etf = 'XLE'\n",
    "sorted_idx = date_of_inputs[etf].argsort()\n",
    "\n",
    "fig = make_subplots(rows=1, cols=1, shared_xaxes=True)\n",
    "fig.add_trace(go.Scatter(x = date_of_inputs[etf][sorted_idx], y = predictions[etf][sorted_idx], name='Predictions'), 1, 1)\n",
    "fig.add_trace(go.Scatter(x = date_of_inputs[etf][sorted_idx], y = actual_targets[etf][sorted_idx], name='Real'), 1, 1)\n",
    "fig.update_layout(title=f'{etf}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
