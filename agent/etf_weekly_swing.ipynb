{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55a81306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóïÔ∏è Downloading weekly data from 2015-05-05 to 2025-04-22\n",
      "‚¨áÔ∏è Downloading XLK...\n",
      "‚¨áÔ∏è Downloading XLF...\n",
      "‚¨áÔ∏è Downloading XLV...\n",
      "‚¨áÔ∏è Downloading XLE...\n",
      "‚¨áÔ∏è Downloading XLI...\n",
      "‚¨áÔ∏è Downloading XLY...\n",
      "‚¨áÔ∏è Downloading XLP...\n",
      "‚¨áÔ∏è Downloading XLRE...\n",
      "‚¨áÔ∏è Downloading XLU...\n",
      "‚¨áÔ∏è Downloading XLB...\n",
      "‚¨áÔ∏è Downloading XLC...\n",
      "‚¨áÔ∏è Downloading SOXX...\n",
      "‚¨áÔ∏è Downloading SH...\n",
      "‚¨áÔ∏è Downloading DOG...\n",
      "‚¨áÔ∏è Downloading RWM...\n",
      "‚¨áÔ∏è Downloading ITA...\n",
      "‚¨áÔ∏è Downloading JETS...\n",
      "‚¨áÔ∏è Downloading PSQ...\n",
      "‚¨áÔ∏è Downloading VNQ...\n",
      "‚¨áÔ∏è Downloading SPY...\n",
      "‚úÖ Saved: etf_prices_weekly.csv\n",
      "‚úÖ Saved: etf_volume_weekly.csv\n",
      "‚úÖ Saved: etf_high_weekly.csv\n",
      "‚úÖ Saved: etf_low_weekly.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>XLK</th>\n",
       "      <th>XLF</th>\n",
       "      <th>XLV</th>\n",
       "      <th>XLE</th>\n",
       "      <th>XLI</th>\n",
       "      <th>XLY</th>\n",
       "      <th>XLP</th>\n",
       "      <th>XLRE</th>\n",
       "      <th>XLU</th>\n",
       "      <th>XLB</th>\n",
       "      <th>XLC</th>\n",
       "      <th>SOXX</th>\n",
       "      <th>SH</th>\n",
       "      <th>DOG</th>\n",
       "      <th>RWM</th>\n",
       "      <th>ITA</th>\n",
       "      <th>JETS</th>\n",
       "      <th>PSQ</th>\n",
       "      <th>VNQ</th>\n",
       "      <th>SPY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>XLK</th>\n",
       "      <th>XLF</th>\n",
       "      <th>XLV</th>\n",
       "      <th>XLE</th>\n",
       "      <th>XLI</th>\n",
       "      <th>XLY</th>\n",
       "      <th>XLP</th>\n",
       "      <th>XLRE</th>\n",
       "      <th>XLU</th>\n",
       "      <th>XLB</th>\n",
       "      <th>XLC</th>\n",
       "      <th>SOXX</th>\n",
       "      <th>SH</th>\n",
       "      <th>DOG</th>\n",
       "      <th>RWM</th>\n",
       "      <th>ITA</th>\n",
       "      <th>JETS</th>\n",
       "      <th>PSQ</th>\n",
       "      <th>VNQ</th>\n",
       "      <th>SPY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-05-04</th>\n",
       "      <td>38.081734</td>\n",
       "      <td>16.607910</td>\n",
       "      <td>62.328026</td>\n",
       "      <td>54.583641</td>\n",
       "      <td>47.107685</td>\n",
       "      <td>68.427345</td>\n",
       "      <td>37.711849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.769207</td>\n",
       "      <td>42.112526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.441837</td>\n",
       "      <td>152.175919</td>\n",
       "      <td>78.749893</td>\n",
       "      <td>51.727139</td>\n",
       "      <td>54.787804</td>\n",
       "      <td>23.647642</td>\n",
       "      <td>237.650299</td>\n",
       "      <td>53.803028</td>\n",
       "      <td>178.051422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-11</th>\n",
       "      <td>38.329540</td>\n",
       "      <td>16.567669</td>\n",
       "      <td>63.015362</td>\n",
       "      <td>53.855946</td>\n",
       "      <td>47.548950</td>\n",
       "      <td>68.418388</td>\n",
       "      <td>38.156689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.978445</td>\n",
       "      <td>42.063545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.762247</td>\n",
       "      <td>151.448502</td>\n",
       "      <td>78.259270</td>\n",
       "      <td>51.239159</td>\n",
       "      <td>55.656296</td>\n",
       "      <td>23.714334</td>\n",
       "      <td>235.520874</td>\n",
       "      <td>54.210815</td>\n",
       "      <td>178.741379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-18</th>\n",
       "      <td>38.586193</td>\n",
       "      <td>16.674990</td>\n",
       "      <td>63.626335</td>\n",
       "      <td>53.542168</td>\n",
       "      <td>47.374111</td>\n",
       "      <td>68.731079</td>\n",
       "      <td>37.773209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.158844</td>\n",
       "      <td>41.745045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.208429</td>\n",
       "      <td>151.012054</td>\n",
       "      <td>78.329346</td>\n",
       "      <td>50.925438</td>\n",
       "      <td>55.633801</td>\n",
       "      <td>22.027941</td>\n",
       "      <td>233.476532</td>\n",
       "      <td>53.515575</td>\n",
       "      <td>179.204147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-25</th>\n",
       "      <td>38.382648</td>\n",
       "      <td>16.500597</td>\n",
       "      <td>63.592411</td>\n",
       "      <td>52.333786</td>\n",
       "      <td>46.466595</td>\n",
       "      <td>68.159348</td>\n",
       "      <td>37.420403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.108330</td>\n",
       "      <td>41.328583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.265490</td>\n",
       "      <td>152.248672</td>\n",
       "      <td>79.205521</td>\n",
       "      <td>51.125870</td>\n",
       "      <td>54.702290</td>\n",
       "      <td>22.008888</td>\n",
       "      <td>234.243179</td>\n",
       "      <td>52.907219</td>\n",
       "      <td>177.647583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-01</th>\n",
       "      <td>38.010948</td>\n",
       "      <td>16.628031</td>\n",
       "      <td>63.083260</td>\n",
       "      <td>51.853107</td>\n",
       "      <td>46.508232</td>\n",
       "      <td>68.355865</td>\n",
       "      <td>36.500046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.824001</td>\n",
       "      <td>40.838619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.474945</td>\n",
       "      <td>153.267014</td>\n",
       "      <td>79.906441</td>\n",
       "      <td>50.472313</td>\n",
       "      <td>54.378304</td>\n",
       "      <td>22.018415</td>\n",
       "      <td>235.776367</td>\n",
       "      <td>51.616932</td>\n",
       "      <td>176.494965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price             XLK        XLF        XLV        XLE        XLI        XLY  \\\n",
       "Ticker            XLK        XLF        XLV        XLE        XLI        XLY   \n",
       "Date                                                                           \n",
       "2015-05-04  38.081734  16.607910  62.328026  54.583641  47.107685  68.427345   \n",
       "2015-05-11  38.329540  16.567669  63.015362  53.855946  47.548950  68.418388   \n",
       "2015-05-18  38.586193  16.674990  63.626335  53.542168  47.374111  68.731079   \n",
       "2015-05-25  38.382648  16.500597  63.592411  52.333786  46.466595  68.159348   \n",
       "2015-06-01  38.010948  16.628031  63.083260  51.853107  46.508232  68.355865   \n",
       "\n",
       "Price             XLP XLRE        XLU        XLB XLC       SOXX          SH  \\\n",
       "Ticker            XLP XLRE        XLU        XLB XLC       SOXX          SH   \n",
       "Date                                                                          \n",
       "2015-05-04  37.711849  NaN  31.769207  42.112526 NaN  28.441837  152.175919   \n",
       "2015-05-11  38.156689  NaN  31.978445  42.063545 NaN  28.762247  151.448502   \n",
       "2015-05-18  37.773209  NaN  32.158844  41.745045 NaN  29.208429  151.012054   \n",
       "2015-05-25  37.420403  NaN  32.108330  41.328583 NaN  30.265490  152.248672   \n",
       "2015-06-01  36.500046  NaN  30.824001  40.838619 NaN  29.474945  153.267014   \n",
       "\n",
       "Price             DOG        RWM        ITA       JETS         PSQ        VNQ  \\\n",
       "Ticker            DOG        RWM        ITA       JETS         PSQ        VNQ   \n",
       "Date                                                                            \n",
       "2015-05-04  78.749893  51.727139  54.787804  23.647642  237.650299  53.803028   \n",
       "2015-05-11  78.259270  51.239159  55.656296  23.714334  235.520874  54.210815   \n",
       "2015-05-18  78.329346  50.925438  55.633801  22.027941  233.476532  53.515575   \n",
       "2015-05-25  79.205521  51.125870  54.702290  22.008888  234.243179  52.907219   \n",
       "2015-06-01  79.906441  50.472313  54.378304  22.018415  235.776367  51.616932   \n",
       "\n",
       "Price              SPY  \n",
       "Ticker             SPY  \n",
       "Date                    \n",
       "2015-05-04  178.051422  \n",
       "2015-05-11  178.741379  \n",
       "2015-05-18  179.204147  \n",
       "2015-05-25  177.647583  \n",
       "2015-06-01  176.494965  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ETF list\n",
    "etf_list = [\n",
    "    'XLK', 'XLF', 'XLV', 'XLE', 'XLI', 'XLY', 'XLP', 'XLRE', 'XLU', 'XLB', 'XLC',\n",
    "    'SOXX', 'SH', 'DOG', 'RWM', 'ITA', 'JETS', 'PSQ', 'VNQ', 'SPY'\n",
    "]\n",
    "\n",
    "# Date range: from 10 years ago to now\n",
    "today = datetime.today()\n",
    "back_time = today - timedelta(weeks=52 * 10)\n",
    "start_date = back_time.strftime('%Y-%m-%d')\n",
    "end_date = today.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"üóïÔ∏è Downloading weekly data from {start_date} to {end_date}\")\n",
    "\n",
    "# Ensure dataset/ exists\n",
    "dataset_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset'))\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# Containers\n",
    "adjclose_data, volume_data, high_data, low_data = {}, {}, {}, {}\n",
    "\n",
    "# Download each ETF\n",
    "for symbol in etf_list:\n",
    "    print(f\"‚¨áÔ∏è Downloading {symbol}...\")\n",
    "    data = yf.download(\n",
    "        symbol,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        interval='1wk',  # back to Yahoo's default weekly format (week ending on Friday)\n",
    "        auto_adjust=False,\n",
    "        progress=False\n",
    "    )\n",
    "    if not data.empty:\n",
    "        data = data[~data.index.duplicated(keep='first')].sort_index()\n",
    "        adjclose_data[symbol] = data[['Adj Close']].rename(columns={'Adj Close': symbol})\n",
    "        volume_data[symbol] = data[['Volume']].rename(columns={'Volume': symbol})\n",
    "        high_data[symbol] = data[['High']].rename(columns={'High': symbol})\n",
    "        low_data[symbol] = data[['Low']].rename(columns={'Low': symbol})\n",
    "\n",
    "# Merge and save\n",
    "def combine_and_save(data_dict, filename):\n",
    "    df = pd.concat(data_dict.values(), axis=1)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "    df = df[~df.index.duplicated(keep='first')].sort_index()\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    path = os.path.join(dataset_path, filename)\n",
    "    df.to_csv(path)\n",
    "    print(f\"‚úÖ Saved: {filename}\")\n",
    "    return df\n",
    "\n",
    "# Save all\n",
    "price_df = combine_and_save(adjclose_data, 'etf_prices_weekly.csv')\n",
    "volume_df = combine_and_save(volume_data, 'etf_volume_weekly.csv')\n",
    "high_df = combine_and_save(high_data, 'etf_high_weekly.csv')\n",
    "low_df = combine_and_save(low_data, 'etf_low_weekly.csv')\n",
    "\n",
    "# Preview\n",
    "price_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ecccfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Downloading macro data from 2015-05-05 to 2025-04-22\n",
      "‚¨áÔ∏è Downloading VIX (^VIX)...\n",
      "‚¨áÔ∏è Downloading 10Y_Yield (^TNX)...\n",
      "‚¨áÔ∏è Downloading USD_Index (DX-Y.NYB)...\n",
      "‚¨áÔ∏è Downloading WTI_Crude (CL=F)...\n",
      "‚úÖ Macro indicators saved to: D:\\CodingWorks\\Weekly_Swing_TransformerQT\\dataset\\macro_indicators_weekly.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>VIX</th>\n",
       "      <th>10Y_Yield</th>\n",
       "      <th>USD_Index</th>\n",
       "      <th>WTI_Crude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>^VIX</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>DX-Y.NYB</th>\n",
       "      <th>CL=F</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-05-04</th>\n",
       "      <td>12.86</td>\n",
       "      <td>0.2150</td>\n",
       "      <td>94.790001</td>\n",
       "      <td>59.389999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-11</th>\n",
       "      <td>12.38</td>\n",
       "      <td>0.2141</td>\n",
       "      <td>93.139999</td>\n",
       "      <td>59.689999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-18</th>\n",
       "      <td>12.13</td>\n",
       "      <td>0.2215</td>\n",
       "      <td>96.010002</td>\n",
       "      <td>59.720001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-25</th>\n",
       "      <td>13.84</td>\n",
       "      <td>0.2095</td>\n",
       "      <td>96.910004</td>\n",
       "      <td>60.299999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-01</th>\n",
       "      <td>14.21</td>\n",
       "      <td>0.2402</td>\n",
       "      <td>96.309998</td>\n",
       "      <td>59.130001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price         VIX 10Y_Yield  USD_Index  WTI_Crude\n",
       "Ticker       ^VIX      ^TNX   DX-Y.NYB       CL=F\n",
       "Date                                             \n",
       "2015-05-04  12.86    0.2150  94.790001  59.389999\n",
       "2015-05-11  12.38    0.2141  93.139999  59.689999\n",
       "2015-05-18  12.13    0.2215  96.010002  59.720001\n",
       "2015-05-25  13.84    0.2095  96.910004  60.299999\n",
       "2015-06-01  14.21    0.2402  96.309998  59.130001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Macro indicator tickers on Yahoo Finance\n",
    "macro_tickers = {\n",
    "    'VIX': '^VIX',               # Volatility Index\n",
    "    '10Y_Yield': '^TNX',         # 10-Year Treasury Yield (multiply by 0.1)\n",
    "    'USD_Index': 'DX-Y.NYB',     # U.S. Dollar Index (ICE Dollar Index)\n",
    "    'WTI_Crude': 'CL=F'          # Crude Oil (WTI Futures)\n",
    "}\n",
    "\n",
    "# Align with ETF backtest window using back_time\n",
    "start_date = back_time.strftime('%Y-%m-%d')\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"üìä Downloading macro data from {start_date} to {end_date}\")\n",
    "\n",
    "# Download weekly data\n",
    "macro_data = {}\n",
    "for name, ticker in macro_tickers.items():\n",
    "    print(f\"‚¨áÔ∏è Downloading {name} ({ticker})...\")\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, interval='1wk', auto_adjust=False, progress=False)\n",
    "    if not data.empty:\n",
    "        macro_data[name] = data[['Close']].rename(columns={'Close': name})\n",
    "\n",
    "# Combine all macro indicators into one DataFrame\n",
    "macro_df = pd.concat(macro_data.values(), axis=1)\n",
    "\n",
    "# Fix 10Y yield scale\n",
    "if '10Y_Yield' in macro_df.columns:\n",
    "    macro_df['10Y_Yield'] = macro_df['10Y_Yield'] * 0.1\n",
    "\n",
    "# Clean and format\n",
    "macro_df = macro_df.apply(pd.to_numeric, errors='coerce')\n",
    "macro_df.index = pd.to_datetime(macro_df.index)\n",
    "macro_df = macro_df[~macro_df.index.duplicated(keep='first')]\n",
    "macro_df.sort_index(inplace=True)\n",
    "macro_df.dropna(inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "macro_save_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'macro_indicators_weekly.csv'))\n",
    "macro_df.to_csv(macro_save_path)\n",
    "print(f\"‚úÖ Macro indicators saved to: {macro_save_path}\")\n",
    "\n",
    "# Preview\n",
    "macro_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e27fa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded ETF Prices with 20 tickers\n",
      "‚úÖ Loaded ETF Volume with 20 tickers\n",
      "üìà XLK: 521 valid rows\n",
      "üìà XLF: 521 valid rows\n",
      "üìà XLV: 521 valid rows\n",
      "üìà XLE: 521 valid rows\n",
      "üìà XLI: 521 valid rows\n",
      "üìà XLY: 521 valid rows\n",
      "üìà XLP: 521 valid rows\n",
      "üìà XLRE: 508 valid rows\n",
      "üìà XLU: 521 valid rows\n",
      "üìà XLB: 521 valid rows\n",
      "üìà XLC: 508 valid rows\n",
      "üìà SOXX: 521 valid rows\n",
      "üìà SH: 521 valid rows\n",
      "üìà DOG: 521 valid rows\n",
      "üìà RWM: 521 valid rows\n",
      "üìà ITA: 521 valid rows\n",
      "üìà JETS: 521 valid rows\n",
      "üìà PSQ: 521 valid rows\n",
      "üìà VNQ: 521 valid rows\n",
      "üìà SPY: 521 valid rows\n",
      "üìÄ Final feature shape: (521, 300)\n",
      "‚úÖ Saved features to: weekly_etf_tech_feature.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from ta import momentum, trend, volume\n",
    "\n",
    "# === Paths ===\n",
    "price_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'etf_prices_weekly.csv'))\n",
    "volume_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'etf_volume_weekly.csv'))\n",
    "\n",
    "# === Helper to load ETF CSVs ===\n",
    "def load_etf_csv(path, name='[unknown]'):\n",
    "    try:\n",
    "        header_row = pd.read_csv(path, header=None, nrows=2)\n",
    "        columns = header_row.iloc[1].tolist()[1:]\n",
    "        df = pd.read_csv(path, skiprows=3, header=None)\n",
    "        df = df.iloc[:, :len(columns) + 1]\n",
    "        df.columns = ['Date'] + columns\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        df = df.set_index('Date')\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "        print(f\"‚úÖ Loaded {name} with {len(columns)} tickers\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {name}: {e}\")\n",
    "        raise\n",
    "\n",
    "# === Load data ===\n",
    "price_df = load_etf_csv(price_path, name='ETF Prices')\n",
    "volume_df = load_etf_csv(volume_path, name='ETF Volume')\n",
    "\n",
    "# === Feature storage ===\n",
    "features_all = []\n",
    "skipped = []\n",
    "\n",
    "# === Feature generation loop ===\n",
    "for symbol in price_df.columns:\n",
    "    if symbol not in volume_df.columns:\n",
    "        print(f\"‚ö†Ô∏è Skipping {symbol}: volume data missing.\")\n",
    "        skipped.append(symbol)\n",
    "        continue\n",
    "\n",
    "    df = pd.DataFrame(index=price_df.index)\n",
    "    df['close'] = price_df[symbol]\n",
    "    df['volume'] = volume_df[symbol]\n",
    "\n",
    "    try:\n",
    "        # === Return-based technical indicators ===\n",
    "        df[f'{symbol}_ret_1w'] = df['close'].pct_change(1)\n",
    "        df[f'{symbol}_ret_3w'] = df['close'].pct_change(3)\n",
    "        df[f'{symbol}_ret_6w'] = df['close'].pct_change(6)\n",
    "\n",
    "        high = df['close'].rolling(window=14).max()\n",
    "        low = df['close'].rolling(window=14).min()\n",
    "        df[f'{symbol}_stoch_k'] = 100 * (df['close'] - low) / (high - low)\n",
    "        df[f'{symbol}_stoch_d'] = df[f'{symbol}_stoch_k'].rolling(window=3).mean()\n",
    "        df[f'{symbol}_williams_r'] = -100 * (high - df['close']) / (high - low)\n",
    "\n",
    "        df[f'{symbol}_cci'] = trend.cci(high=df['close'], low=df['close'], close=df['close'], window=20)\n",
    "        df[f'{symbol}_rsi'] = momentum.rsi(df['close'], window=14)\n",
    "        df[f'{symbol}_obv'] = volume.on_balance_volume(df['close'], df['volume'])\n",
    "\n",
    "        df[f'{symbol}_macd'] = trend.macd(df['close'])\n",
    "        df[f'{symbol}_macd_signal'] = trend.macd_signal(df['close'])\n",
    "        df[f'{symbol}_macd_diff'] = trend.macd_diff(df['close'])\n",
    "\n",
    "        # === Price/volume variation ===\n",
    "        df[f'{symbol}_price_change'] = df['close'].pct_change(1)\n",
    "        df[f'{symbol}_volume_change'] = df['volume'].pct_change(1)\n",
    "\n",
    "        # === Short-term KST (custom) ===\n",
    "        roc1 = df['close'].pct_change(10)\n",
    "        roc2 = df['close'].pct_change(15)\n",
    "        roc3 = df['close'].pct_change(20)\n",
    "        roc4 = df['close'].pct_change(30)\n",
    "        df[f'{symbol}_kst_short'] = (\n",
    "            roc1.rolling(10).mean() +\n",
    "            roc2.rolling(10).mean() * 2 +\n",
    "            roc3.rolling(10).mean() * 3 +\n",
    "            roc4.rolling(15).mean() * 4\n",
    "        )\n",
    "\n",
    "        derived_cols = df.columns.difference(['close', 'volume'])\n",
    "        feature_df = df[derived_cols].copy()\n",
    "\n",
    "        features_all.append(feature_df)\n",
    "        print(f\"üìà {symbol}: {feature_df.dropna(how='all').shape[0]} valid rows\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {symbol}: {e}\")\n",
    "        skipped.append(symbol)\n",
    "\n",
    "# === Final merge ===\n",
    "if len(features_all) == 0:\n",
    "    raise ValueError(\"üõë No valid ETF features generated.\")\n",
    "\n",
    "features_df = pd.concat(features_all, axis=1).sort_index()\n",
    "features_df = features_df[~features_df.index.duplicated(keep='first')]\n",
    "\n",
    "print(\"üìÄ Final feature shape:\", features_df.shape)\n",
    "\n",
    "# === Save to CSV ===\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset'))\n",
    "features_df.to_csv(os.path.join(base_dir, 'weekly_etf_tech_feature.csv'))\n",
    "print(f\"‚úÖ Saved features to: weekly_etf_tech_feature.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b6cb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_35768\\2028834899.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  macro_df['Price'] = pd.to_datetime(macro_df['Price'], errors='coerce')  # 'Price' is actually the 'Date'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved XLK: 521 rows, 21 features\n",
      "‚úÖ Saved XLF: 521 rows, 21 features\n",
      "‚úÖ Saved XLV: 521 rows, 21 features\n",
      "‚úÖ Saved XLE: 521 rows, 21 features\n",
      "‚úÖ Saved XLI: 521 rows, 21 features\n",
      "‚úÖ Saved XLY: 521 rows, 21 features\n",
      "‚úÖ Saved XLP: 521 rows, 21 features\n",
      "‚úÖ Saved XLRE: 521 rows, 21 features\n",
      "‚úÖ Saved XLU: 521 rows, 21 features\n",
      "‚úÖ Saved XLB: 521 rows, 21 features\n",
      "‚úÖ Saved XLC: 521 rows, 21 features\n",
      "‚úÖ Saved SOXX: 521 rows, 21 features\n",
      "‚úÖ Saved SH: 521 rows, 21 features\n",
      "‚úÖ Saved DOG: 521 rows, 21 features\n",
      "‚úÖ Saved RWM: 521 rows, 21 features\n",
      "‚úÖ Saved ITA: 521 rows, 21 features\n",
      "‚úÖ Saved JETS: 521 rows, 21 features\n",
      "‚úÖ Saved PSQ: 521 rows, 21 features\n",
      "‚úÖ Saved VNQ: 521 rows, 21 features\n",
      "‚úÖ Saved SPY: 521 rows, 21 features\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Paths ===\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset'))\n",
    "feat_path = os.path.join(base_dir, 'weekly_etf_tech_feature.csv')\n",
    "macro_path = os.path.join(base_dir, 'macro_indicators_weekly.csv')\n",
    "price_path = os.path.join(base_dir, 'etf_prices_weekly.csv')\n",
    "volume_path = os.path.join(base_dir, 'etf_volume_weekly.csv')\n",
    "\n",
    "# === Load main features ===\n",
    "feat_df = pd.read_csv(feat_path, index_col=0, parse_dates=True)\n",
    "\n",
    "# === Load and parse macro indicators ===\n",
    "macro_raw = pd.read_csv(macro_path, header=None)\n",
    "macro_start = macro_raw.index[macro_raw[0] == 'Price'].tolist()[0]\n",
    "macro_cols = macro_raw.iloc[macro_start].tolist()\n",
    "macro_df = pd.read_csv(macro_path, skiprows=macro_start + 1, header=None)\n",
    "macro_df.columns = macro_cols\n",
    "macro_df['Price'] = pd.to_datetime(macro_df['Price'], errors='coerce')  # 'Price' is actually the 'Date'\n",
    "macro_df = macro_df.rename(columns={'Price': 'Date'}).set_index('Date')\n",
    "macro_df = macro_df.apply(pd.to_numeric, errors='coerce')\n",
    "macro_df = macro_df[~macro_df.index.duplicated()].sort_index()\n",
    "\n",
    "# === Load ETF price and volume ===\n",
    "ticker_row = pd.read_csv(price_path, header=None, nrows=2).iloc[1, 1:].tolist()\n",
    "price_df = pd.read_csv(price_path, skiprows=3, header=None)\n",
    "volume_df = pd.read_csv(volume_path, skiprows=3, header=None)\n",
    "\n",
    "price_df = price_df.iloc[:, :len(ticker_row)+1]\n",
    "price_df.columns = ['Date'] + ticker_row\n",
    "price_df['Date'] = pd.to_datetime(price_df['Date'])\n",
    "price_df = price_df.set_index('Date').apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "volume_df = volume_df.iloc[:, :len(ticker_row)+1]\n",
    "volume_df.columns = ['Date'] + ticker_row\n",
    "volume_df['Date'] = pd.to_datetime(volume_df['Date'])\n",
    "volume_df = volume_df.set_index('Date').apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# === Output Directory ===\n",
    "norm_dir = os.path.join(base_dir, 'normalized_matrix')\n",
    "os.makedirs(norm_dir, exist_ok=True)\n",
    "\n",
    "# === Normalize & Save per ETF ===\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for ticker in ticker_row:\n",
    "    feat_cols = [col for col in feat_df.columns if col.startswith(f'{ticker}_')]\n",
    "    if len(feat_cols) == 0:\n",
    "        print(f\"‚ö†Ô∏è No features found for {ticker}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    df = feat_df[feat_cols].copy()\n",
    "    df.columns = [col.replace(f'{ticker}_', '') for col in df.columns]\n",
    "\n",
    "    df['price'] = price_df[ticker].pct_change()\n",
    "    df['volume'] = volume_df[ticker].pct_change()\n",
    "\n",
    "    # === Merge with true macro indicators ===\n",
    "    df = df.join(macro_df, how='left')\n",
    "\n",
    "    # === Build missing mask BEFORE filling\n",
    "    mask = df.isna().astype(float)\n",
    "\n",
    "    # === Fill and normalize only technical + pct change features, not macro\n",
    "    macro_cols = macro_df.columns\n",
    "    norm_cols = df.columns.difference(macro_cols)\n",
    "    df[norm_cols] = scaler.fit_transform(df[norm_cols].fillna(0.0))\n",
    "    df[macro_cols] = df[macro_cols].fillna(0.0)  # keep raw values for macro features\n",
    "\n",
    "    # === Save\n",
    "    df.to_csv(os.path.join(norm_dir, f'{ticker}_combined.csv'))\n",
    "    mask.to_csv(os.path.join(norm_dir, f'{ticker}_mask.csv'))\n",
    "    print(f\"‚úÖ Saved {ticker}: {df.shape[0]} rows, {df.shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e3617",
   "metadata": {},
   "source": [
    "# start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565c35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training XLK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\stock_pred\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss=0.7663, MAE=0.4937, WinRate=0.7904\n",
      "Epoch 200: Loss=0.6161, MAE=0.4236, WinRate=0.8290\n",
      "Epoch 300: Loss=0.7165, MAE=0.4700, WinRate=0.7941\n",
      "Epoch 400: Loss=0.4371, MAE=0.3391, WinRate=0.8658\n",
      "Epoch 500: Loss=0.4168, MAE=0.3201, WinRate=0.8676\n",
      "Epoch 600: Loss=0.3269, MAE=0.2742, WinRate=0.8879\n",
      "Epoch 700: Loss=0.3224, MAE=0.2707, WinRate=0.8989\n",
      "Epoch 800: Loss=0.2896, MAE=0.2534, WinRate=0.9026\n",
      "Epoch 900: Loss=0.2875, MAE=0.2478, WinRate=0.8971\n",
      "Epoch 1000: Loss=0.2595, MAE=0.2371, WinRate=0.9154\n",
      "Epoch 1100: Loss=0.2733, MAE=0.2374, WinRate=0.8934\n",
      "Epoch 1200: Loss=0.2317, MAE=0.2101, WinRate=0.9118\n",
      "Epoch 1300: Loss=0.2352, MAE=0.2137, WinRate=0.9136\n",
      "Epoch 1400: Loss=0.2436, MAE=0.2164, WinRate=0.9099\n",
      "Epoch 1500: Loss=0.2458, MAE=0.2268, WinRate=0.9136\n",
      "Epoch 1600: Loss=0.2108, MAE=0.1994, WinRate=0.9210\n",
      "Epoch 1700: Loss=0.2011, MAE=0.1915, WinRate=0.9191\n",
      "Epoch 1800: Loss=0.1928, MAE=0.1862, WinRate=0.9338\n",
      "Epoch 1900: Loss=0.1909, MAE=0.1816, WinRate=0.9228\n",
      "Epoch 2000: Loss=0.2044, MAE=0.1903, WinRate=0.9173\n",
      "Epoch 2100: Loss=0.1831, MAE=0.1746, WinRate=0.9246\n",
      "Epoch 2200: Loss=0.1959, MAE=0.1811, WinRate=0.9228\n",
      "Epoch 2300: Loss=0.1686, MAE=0.1639, WinRate=0.9283\n",
      "Epoch 2400: Loss=0.1585, MAE=0.1604, WinRate=0.9393\n",
      "Epoch 2500: Loss=0.1934, MAE=0.1837, WinRate=0.9265\n",
      "Epoch 2600: Loss=0.1699, MAE=0.1670, WinRate=0.9412\n",
      "Epoch 2700: Loss=0.1677, MAE=0.1640, WinRate=0.9338\n",
      "Epoch 2800: Loss=0.1438, MAE=0.1457, WinRate=0.9393\n",
      "Epoch 2900: Loss=0.1828, MAE=0.1755, WinRate=0.9320\n",
      "Epoch 3000: Loss=0.1452, MAE=0.1477, WinRate=0.9412\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ..\\model_weights does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 137\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# === Execute ===\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m etf \u001b[38;5;129;01min\u001b[39;00m etf_list:\n\u001b[1;32m--> 137\u001b[0m     \u001b[43mtrain_one_etf\u001b[49m\u001b[43m(\u001b[49m\u001b[43metf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 133\u001b[0m, in \u001b[0;36mtrain_one_etf\u001b[1;34m(etf)\u001b[0m\n\u001b[0;32m    125\u001b[0m out \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m: weights,\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m: score,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m: score \u001b[38;5;241m/\u001b[39m weight_sum\n\u001b[0;32m    131\u001b[0m }\n\u001b[0;32m    132\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00metf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_top\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 133\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock_pred\\lib\\site-packages\\torch\\serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    850\u001b[0m         _save(\n\u001b[0;32m    851\u001b[0m             obj,\n\u001b[0;32m    852\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    856\u001b[0m         )\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock_pred\\lib\\site-packages\\torch\\serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock_pred\\lib\\site-packages\\torch\\serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory ..\\model_weights does not exist."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'normalized_matrix'))\n",
    "\n",
    "# === Dataset ===\n",
    "class ETFDataset(nn.Module):\n",
    "    def __init__(self, combined_csv, mask_csv, seq_len=4):\n",
    "        self.raw_X = pd.read_csv(combined_csv, index_col=0).values.astype(np.float32)\n",
    "        self.raw_M = pd.read_csv(mask_csv, index_col=0).values.astype(np.float32)\n",
    "        self.seq_len = seq_len\n",
    "        self.X, self.M, self.y = [], [], []\n",
    "        for i in range(seq_len, len(self.raw_X)):\n",
    "            self.X.append(self.raw_X[i-seq_len:i])\n",
    "            self.M.append(self.raw_M[i-seq_len:i])\n",
    "            self.y.append(self.raw_X[i, 0])  # first column is return\n",
    "        self.X = np.stack(self.X)\n",
    "        self.M = np.stack(self.M)\n",
    "        self.y = np.array(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.X[idx]),\n",
    "            torch.tensor(self.M[idx]),\n",
    "            torch.tensor(self.y[idx])\n",
    "        )\n",
    "\n",
    "# === Model with Attention Pooling ===\n",
    "class DualTransformerModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=128, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Choose the highest possible nhead divisor of num_features\n",
    "        possible_heads = [h for h in [8, 4, 2, 1] if num_features % h == 0]\n",
    "        nhead = possible_heads[0]\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=num_features,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=hidden_dim,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.attn_query = nn.Parameter(torch.randn(1, 1, num_features))\n",
    "        self.output = nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x_masked = x * (1 - mask)\n",
    "        enc = self.encoder(x_masked)\n",
    "\n",
    "        # Apply learnable attention pooling\n",
    "        attn_scores = torch.matmul(self.attn_query, enc.transpose(1, 2))  # (1, 1, F) x (B, F, T) = (B, 1, T)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)  # (B, 1, T)\n",
    "        pooled = torch.bmm(attn_weights, enc).squeeze(1)  # (B, 1, T) x (B, T, F) = (B, 1, F) ‚Üí (B, F)\n",
    "\n",
    "        return self.output(pooled).squeeze(-1)\n",
    "\n",
    "# === Training Params ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 0.001\n",
    "max_epochs = 3000\n",
    "seq_len = 4\n",
    "\n",
    "# === Training Function ===\n",
    "def train_one_etf(etf):\n",
    "    print(f\"\\nüöÄ Training {etf}\")\n",
    "    feat_path = os.path.join(data_dir, f\"{etf}_combined.csv\")\n",
    "    mask_path = os.path.join(data_dir, f\"{etf}_mask.csv\")\n",
    "    dataset = ETFDataset(feat_path, mask_path, seq_len=seq_len)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    X, _, _ = next(iter(loader))\n",
    "    model = DualTransformerModel(num_features=X.shape[-1]).to(device)\n",
    "    model.device = device\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    mae_loss = nn.L1Loss()\n",
    "\n",
    "    best_models = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss, total_mae, total_win = 0, 0, 0\n",
    "\n",
    "        for X, M, y in loader:\n",
    "            X, M, y = X.to(model.device), M.to(model.device), y.to(model.device)\n",
    "            pred = model(X, M)\n",
    "\n",
    "            mse = mse_loss(pred, y)\n",
    "            mae = mae_loss(pred, y)\n",
    "            direction_loss = 1 - (torch.sign(pred) == torch.sign(y)).float().mean()\n",
    "            loss = mse + 0.5 * mae + 0.5 * direction_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_mae += mae.item()\n",
    "            total_win += (torch.sign(pred) == torch.sign(y)).float().mean().item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        avg_mae = total_mae / len(loader)\n",
    "        avg_win = total_win / len(loader)\n",
    "        score = avg_win * 0.6 + (1 - avg_mae) * 0.4\n",
    "        best_models.append((score, model.state_dict(), avg_win, avg_mae))\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, MAE={avg_mae:.4f}, WinRate={avg_win:.4f}\")\n",
    "\n",
    "    # Save top 5 models\n",
    "    top5 = sorted(best_models, key=lambda x: -x[0])[:5]\n",
    "    weight_sum = sum(x[0] for x in top5)\n",
    "\n",
    "    # Ensure model_weights folder exists\n",
    "    save_dir = os.path.join(\"..\", \"model_weights\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for i, (score, weights, win, mae) in enumerate(top5):\n",
    "        out = {\n",
    "            'weights': weights,\n",
    "            'score': score,\n",
    "            'win_rate': win,\n",
    "            'mae': mae,\n",
    "            'weight': score / weight_sum\n",
    "        }\n",
    "        save_path = os.path.join(save_dir, f\"{etf}_top{i+1}.pt\")\n",
    "        torch.save(out, save_path)\n",
    "\n",
    "\n",
    "# === Execute ===\n",
    "for etf in etf_list:\n",
    "    train_one_etf(etf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb75d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Config ===\n",
    "weights_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'model_weights'))\n",
    "feature_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'normalized_matrix'))\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# === Load feature names from any example ETF\n",
    "first_etf = etf_list[0]\n",
    "example_path = os.path.join(feature_dir, f'{first_etf}_combined.csv')\n",
    "example_features = pd.read_csv(example_path, index_col=0)\n",
    "factor_names = example_features.columns.tolist()\n",
    "\n",
    "# === Collect factor importance across ETFs\n",
    "factor_weights = {}\n",
    "\n",
    "for etf in etf_list:\n",
    "    weights = []\n",
    "    for i in range(1, 6):\n",
    "        model_path = os.path.join(weights_dir, f\"{etf}_top{i}.pt\")\n",
    "        if not os.path.exists(model_path):\n",
    "            continue\n",
    "\n",
    "        data = torch.load(model_path, map_location='cpu')\n",
    "        linear_weights = data['weights'].get('output.weight', None)\n",
    "        if linear_weights is None:\n",
    "            print(f\"‚ö†Ô∏è {etf}_top{i}.pt missing output.weight\")\n",
    "            continue\n",
    "\n",
    "        linear_weights = linear_weights.squeeze().numpy()\n",
    "        if len(linear_weights) != len(factor_names):\n",
    "            print(f\"‚ö†Ô∏è {etf}_top{i}.pt: weight dim {len(linear_weights)} != {len(factor_names)} features\")\n",
    "            continue\n",
    "\n",
    "        weights.append(linear_weights * data.get('weight', 1.0))\n",
    "\n",
    "    if weights:\n",
    "        factor_weights[etf] = sum(weights)\n",
    "\n",
    "# === Create heatmap DataFrame\n",
    "weight_df = pd.DataFrame(factor_weights, index=factor_names).T\n",
    "\n",
    "# === Normalize (optional)\n",
    "weight_df = weight_df.div(weight_df.abs().max(axis=1), axis=0)\n",
    "\n",
    "# === Plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(weight_df, cmap=\"coolwarm\", center=0, annot=False)\n",
    "plt.title(\"üéØ Factor Importance per ETF (Weighted Top 5 Models)\", fontsize=14)\n",
    "plt.xlabel(\"Factors\")\n",
    "plt.ylabel(\"ETFs\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608bc50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "# === Paths ===\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'normalized_matrix'))\n",
    "model_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'model_weights'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Gather predictions\n",
    "plot_data = []\n",
    "\n",
    "for etf in etf_list:\n",
    "    try:\n",
    "        feat_path = os.path.join(data_dir, f\"{etf}_combined.csv\")\n",
    "        mask_path = os.path.join(data_dir, f\"{etf}_mask.csv\")\n",
    "        model_path = os.path.join(model_dir, f\"{etf}_top1.pt\")\n",
    "\n",
    "        df_feat = pd.read_csv(feat_path, index_col=0).astype('float32')\n",
    "        df_mask = pd.read_csv(mask_path, index_col=0).astype('float32')\n",
    "\n",
    "        if len(df_feat) < 5:\n",
    "            continue\n",
    "\n",
    "        # === Prepare sequence (4 weeks) and target (next week)\n",
    "        X_seq = []\n",
    "        M_seq = []\n",
    "        y_seq = []\n",
    "\n",
    "        for i in range(4, len(df_feat)):\n",
    "            X_seq.append(df_feat.iloc[i-4:i].values)\n",
    "            M_seq.append(df_mask.iloc[i-4:i].values)\n",
    "            y_seq.append(df_feat.iloc[i, 0])  # 1-week future return\n",
    "\n",
    "        X_seq = torch.tensor(np.array(X_seq), dtype=torch.float32).to(device)\n",
    "        M_seq = torch.tensor(np.array(M_seq), dtype=torch.float32).to(device)\n",
    "        y_seq = np.array(y_seq)\n",
    "\n",
    "        model = DualTransformerModel(num_features=X_seq.shape[-1]).to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device)['weights'])\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(X_seq, M_seq).cpu().numpy()\n",
    "\n",
    "        date_index = df_feat.index[4:]\n",
    "        df_plot = pd.DataFrame({'Date': date_index, 'Predicted': pred, 'Actual': y_seq}).set_index('Date')\n",
    "        plot_data.append((etf, df_plot.iloc[-50:]))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading/predicting {etf}: {e}\")\n",
    "\n",
    "# === Plotting\n",
    "fig = plt.figure(figsize=(20, 4 * ((len(plot_data) + 3) // 4)))\n",
    "gs = gridspec.GridSpec((len(plot_data) + 3) // 4, 4, figure=fig)\n",
    "\n",
    "for i, (etf, df) in enumerate(plot_data):\n",
    "    ax = fig.add_subplot(gs[i])\n",
    "    ax.plot(df.index, df['Actual'], label='Actual', color='blue')\n",
    "    ax.plot(df.index, df['Predicted'], label='Predicted', color='red')\n",
    "    ax.set_title(etf)\n",
    "    ax.legend()\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "fig.suptitle(\"üìà ETF Weekly Return Prediction vs. Actual\", fontsize=18)\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# === Prediction Class ===\n",
    "class WeeklyETFPredictor:\n",
    "    def __init__(self, model_dir, data_dir, record_dir, price_path, device=None):\n",
    "        self.model_dir = model_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.record_dir = record_dir\n",
    "        self.price_path = price_path\n",
    "        os.makedirs(self.record_dir, exist_ok=True)\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def predict(self):\n",
    "        # Get the nearest past Monday ‚Äî this represents the start of the prediction week\n",
    "        today = datetime.today()\n",
    "        days_back = today.weekday() % 7\n",
    "        monday = today - timedelta(days=days_back)\n",
    "        monday_dt = pd.to_datetime(monday.strftime(\"%Y-%m-%d\"))\n",
    "        date_str = monday_dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Load ETF price data for reference\n",
    "        price_df = pd.read_csv(\n",
    "            self.price_path,\n",
    "            index_col=0,\n",
    "            parse_dates=True,\n",
    "            date_parser=lambda x: pd.to_datetime(x, format=\"%Y-%m-%d\", errors='coerce')\n",
    "        )\n",
    "        price_df = price_df[price_df.index.notna()]\n",
    "        price_df = price_df[~price_df.index.duplicated()].sort_index()\n",
    "\n",
    "        print(f\"üîç Checking last available price dates: {price_df.index[-5:].to_list()}\")\n",
    "        print(f\"üìÖ Using feature/price data for week starting on: {monday_dt.date()}\")\n",
    "\n",
    "        summary = []\n",
    "\n",
    "        for fname in os.listdir(self.data_dir):\n",
    "            if not fname.endswith(\"_combined.csv\"):\n",
    "                continue\n",
    "\n",
    "            etf = fname.replace(\"_combined.csv\", \"\")\n",
    "            feat_path = os.path.join(self.data_dir, f\"{etf}_combined.csv\")\n",
    "            mask_path = os.path.join(self.data_dir, f\"{etf}_mask.csv\")\n",
    "\n",
    "            try:\n",
    "                df_feat = pd.read_csv(feat_path, index_col=0, parse_dates=True)\n",
    "                df_mask = pd.read_csv(mask_path, index_col=0, parse_dates=True)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading data for {etf}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if monday_dt not in df_feat.index:\n",
    "                print(f\"‚ö†Ô∏è No feature data for {etf} on {monday_dt.date()}\")\n",
    "                continue\n",
    "\n",
    "            if etf not in price_df.columns or monday_dt not in price_df.index:\n",
    "                print(f\"‚ö†Ô∏è Price info missing for {etf} on {monday_dt.date()}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                x_real = torch.tensor(df_feat.loc[monday_dt].values.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "                x_mask = torch.tensor(df_mask.loc[monday_dt].values.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "                price = float(price_df.loc[monday_dt, etf])\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Tensor prep or price parse failed for {etf}: {e}\")\n",
    "                continue\n",
    "\n",
    "            scores, preds, maes, winrates = [], [], [], []\n",
    "            for i in range(1, 6):\n",
    "                path = os.path.join(self.model_dir, f\"{etf}_top{i}.pt\")\n",
    "                if not os.path.exists(path): continue\n",
    "\n",
    "                try:\n",
    "                    checkpoint = torch.load(path, map_location=self.device)\n",
    "                    model = DualTransformerModel(num_features=x_real.shape[-1]).to(self.device)\n",
    "                    model.load_state_dict(checkpoint['weights'])\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        pred = model(x_real, x_mask).item()\n",
    "                    preds.append(pred)\n",
    "                    maes.append(checkpoint['mae'])\n",
    "                    winrates.append(checkpoint['win_rate'])\n",
    "                    scores.append(checkpoint['weight'])\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Model predict error for {etf} top{i}: {e}\")\n",
    "\n",
    "            if len(preds) == 0:\n",
    "                continue\n",
    "\n",
    "            pred_return = sum(p * w for p, w in zip(preds, scores))\n",
    "            avg_mae = sum(maes) / len(maes)\n",
    "            avg_win = sum(winrates) / len(winrates)\n",
    "\n",
    "            if pred_return >= 0:\n",
    "                target_up = round(price * (1 + pred_return / 100 + pred_return * avg_mae / 100), 2)\n",
    "                stop_down = round(price * (1 - pred_return * avg_mae / 100), 2)\n",
    "                buy_price = round(price, 2)\n",
    "            else:\n",
    "                target_up = stop_down = buy_price = \"X\"\n",
    "\n",
    "            summary.append({\n",
    "                'ETF': etf,\n",
    "                'PredictedReturn': round(pred_return, 4),\n",
    "                'MAE': round(avg_mae, 4),\n",
    "                'WinRate': round(avg_win, 4),\n",
    "                'BuyPrice': buy_price,\n",
    "                'Target‚Üë': target_up,\n",
    "                'Stop‚Üì': stop_down\n",
    "            })\n",
    "\n",
    "        summary_df = pd.DataFrame(summary)\n",
    "        if summary_df.empty:\n",
    "            print(f\"‚ö†Ô∏è No valid predictions generated for {date_str}\")\n",
    "            return summary_df\n",
    "\n",
    "        summary_df = summary_df.sort_values(by='PredictedReturn', ascending=False)\n",
    "\n",
    "        # === Output\n",
    "        print(f\"\\nüìä Weekly ETF Prediction Summary for {date_str}:\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "\n",
    "        output_path = os.path.join(self.record_dir, f\"{date_str}_predict_record.csv\")\n",
    "        summary_df.to_csv(output_path, index=False)\n",
    "        print(f\"üìÅ Saved prediction to: {output_path}\")\n",
    "\n",
    "        return summary_df\n",
    "\n",
    "# === Usage\n",
    "predictor = WeeklyETFPredictor(\n",
    "    model_dir=\"../model_weights\",\n",
    "    data_dir=\"../dataset/normalized_matrix\",\n",
    "    record_dir=\"../dataset/predict_record\",\n",
    "    price_path=\"../dataset/etf_prices_weekly.csv\"\n",
    ")\n",
    "predictor.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de35d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from model import DualTransformerModel  # Ensure correct path\n",
    "\n",
    "class ETFBacktester:\n",
    "    def __init__(self, model_dir, data_dir, price_path, initial_cash=2000, benchmark_symbol='SPY', seq_len=4):\n",
    "        self.model_dir = model_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.price_path = price_path\n",
    "        self.initial_cash = initial_cash\n",
    "        self.benchmark_symbol = benchmark_symbol\n",
    "        self.seq_len = seq_len\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def run(self):\n",
    "        print(\"üîÅ Loading price data...\")\n",
    "        price_df = pd.read_csv(self.price_path, index_col=0)\n",
    "        price_df.index = pd.to_datetime(price_df.index, errors='coerce')\n",
    "        price_df = price_df.apply(pd.to_numeric, errors='coerce')\n",
    "        price_df = price_df[~price_df.index.duplicated()].sort_index()\n",
    "        print(f\"üìà Loaded price data with {len(price_df)} entries.\")\n",
    "\n",
    "        etf_list = [fname.replace('_combined.csv', '') for fname in os.listdir(self.data_dir) if fname.endswith('_combined.csv')]\n",
    "        print(f\"üîç ETF list found: {etf_list}\")\n",
    "\n",
    "        cash = self.initial_cash\n",
    "        holdings = {}\n",
    "        portfolio_values = []\n",
    "        benchmark_values = []\n",
    "        dates = []\n",
    "        trade_log = []\n",
    "        value_log = []\n",
    "\n",
    "        for i in range(self.seq_len, len(price_df) - 1):  # Ensure enough history\n",
    "            date = price_df.index[i]\n",
    "            prev_dates = price_df.index[i - self.seq_len:i]\n",
    "            next_date = price_df.index[i + 1]\n",
    "            print(f\"\\nüìÖ Processing week: {date.date()} -> {next_date.date()}\")\n",
    "\n",
    "            # === SELL ALL HOLDINGS ===\n",
    "            for etf, info in list(holdings.items()):\n",
    "                if etf not in price_df.columns or next_date not in price_df.index:\n",
    "                    continue\n",
    "                try:\n",
    "                    sell_price = float(price_df.loc[next_date, etf])\n",
    "                except Exception:\n",
    "                    continue\n",
    "                shares = info['Shares']\n",
    "                buy_price = info['BuyPrice']\n",
    "                sell_value = shares * sell_price\n",
    "                actual_return = (sell_price - buy_price) / buy_price\n",
    "                predicted_return = info.get('PredictedReturn', None)\n",
    "                cash += sell_value\n",
    "                trade_log.append({\n",
    "                    'Date': next_date.strftime('%Y-%m-%d'),\n",
    "                    'ETF': etf,\n",
    "                    'Action': 'Sell',\n",
    "                    'Price': sell_price,\n",
    "                    'Shares': shares,\n",
    "                    'Value': sell_value,\n",
    "                    'BuyPrice': buy_price,\n",
    "                    'PredictedReturn': predicted_return,\n",
    "                    'ActualReturn': actual_return * 100\n",
    "                })\n",
    "            holdings.clear()\n",
    "\n",
    "            # === PREDICT TOP 2 ===\n",
    "            week_predictions = []\n",
    "            for etf in etf_list:\n",
    "                feat_path = os.path.join(self.data_dir, f\"{etf}_combined.csv\")\n",
    "                mask_path = os.path.join(self.data_dir, f\"{etf}_mask.csv\")\n",
    "                if not os.path.exists(feat_path) or not os.path.exists(mask_path):\n",
    "                    continue\n",
    "\n",
    "                feat_df = pd.read_csv(feat_path, index_col=0, parse_dates=True)\n",
    "                mask_df = pd.read_csv(mask_path, index_col=0, parse_dates=True)\n",
    "\n",
    "                if not all(d in feat_df.index for d in prev_dates):\n",
    "                    continue\n",
    "\n",
    "                x_seq = torch.tensor(feat_df.loc[prev_dates].values.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "                x_mask = torch.tensor(mask_df.loc[prev_dates].values.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "\n",
    "                scores, preds = [], []\n",
    "                for j in range(1, 6):\n",
    "                    model_path = os.path.join(self.model_dir, f\"{etf}_top{j}.pt\")\n",
    "                    if not os.path.exists(model_path):\n",
    "                        continue\n",
    "                    checkpoint = torch.load(model_path, map_location=self.device)\n",
    "                    model = DualTransformerModel(num_features=x_seq.shape[-1]).to(self.device)\n",
    "                    model.load_state_dict(checkpoint['weights'])\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        pred = model(x_seq, x_mask).item()\n",
    "                    preds.append(pred)\n",
    "                    scores.append(checkpoint['weight'])\n",
    "\n",
    "                if preds:\n",
    "                    pred_return = sum(p * w for p, w in zip(preds, scores))\n",
    "                    try:\n",
    "                        price = float(price_df.loc[date, etf])\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    if not np.isnan(price):\n",
    "                        week_predictions.append({\n",
    "                            'ETF': etf,\n",
    "                            'BuyPrice': price,\n",
    "                            'PredictedReturn': float(pred_return)\n",
    "                        })\n",
    "\n",
    "            week_predictions = sorted(week_predictions, key=lambda x: -x['PredictedReturn'])[:2]\n",
    "\n",
    "            budget_per_etf = cash / 2\n",
    "            for item in week_predictions:\n",
    "                etf = item['ETF']\n",
    "                price = item['BuyPrice']\n",
    "                shares = int(budget_per_etf // price)\n",
    "                cost = shares * price\n",
    "                if shares <= 0 or cost > cash:\n",
    "                    continue\n",
    "                cash -= cost\n",
    "                holdings[etf] = {'Shares': shares, 'BuyPrice': price, 'PredictedReturn': item['PredictedReturn']}\n",
    "                trade_log.append({\n",
    "                    'Date': date.strftime('%Y-%m-%d'),\n",
    "                    'ETF': etf,\n",
    "                    'Action': 'Buy',\n",
    "                    'Price': price,\n",
    "                    'Shares': shares,\n",
    "                    'Value': cost,\n",
    "                    'PredictedReturn': item['PredictedReturn']\n",
    "                })\n",
    "\n",
    "            # === Weekly Portfolio Valuation\n",
    "            value = cash\n",
    "            for etf, info in holdings.items():\n",
    "                try:\n",
    "                    price = float(price_df.loc[next_date, etf])\n",
    "                except Exception:\n",
    "                    price = 0\n",
    "                value += info['Shares'] * price\n",
    "\n",
    "            portfolio_values.append(value)\n",
    "            dates.append(next_date.strftime('%Y-%m-%d'))\n",
    "            value_log.append({\n",
    "                'Date': next_date.strftime('%Y-%m-%d'),\n",
    "                'Value': value,\n",
    "                'Cash': cash,\n",
    "                'Holdings': {k: dict(v) for k, v in holdings.items()}\n",
    "            })\n",
    "\n",
    "            if self.benchmark_symbol in price_df.columns:\n",
    "                base_price = price_df[self.benchmark_symbol].iloc[0]\n",
    "                current_price = price_df.loc[next_date, self.benchmark_symbol]\n",
    "                benchmark_values.append(self.initial_cash * (current_price / base_price))\n",
    "            else:\n",
    "                benchmark_values.append(value)\n",
    "\n",
    "        # === Save logs\n",
    "        result_df = pd.DataFrame({\n",
    "            'Date': dates,\n",
    "            'PortfolioValue': portfolio_values,\n",
    "            'BenchmarkValue': benchmark_values\n",
    "        })\n",
    "        result_df['Date'] = pd.to_datetime(result_df['Date'], errors='coerce')\n",
    "        result_df.set_index('Date', inplace=True)\n",
    "\n",
    "        pd.DataFrame(trade_log).to_csv(\"../dataset/backtest_trade_log.csv\", index=False)\n",
    "        pd.DataFrame(value_log).to_csv(\"../dataset/backtest_value_log.csv\", index=False)\n",
    "\n",
    "        # === Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(result_df['PortfolioValue'], label='Strategy Portfolio')\n",
    "        plt.plot(result_df['BenchmarkValue'], label=f'{self.benchmark_symbol} Benchmark')\n",
    "        plt.title(\"Backtest Result: Strategy vs. Benchmark\")\n",
    "        plt.ylabel(\"Portfolio Value (USD)\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        xticks_idx = result_df.index[::52]\n",
    "        xticks_labels = [d.strftime('%Y') for d in xticks_idx]\n",
    "        plt.xticks(xticks_idx, xticks_labels)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nüí∞ Final Portfolio Value: ${result_df['PortfolioValue'].iloc[-1]:,.2f}\")\n",
    "        print(f\"üìà Final Benchmark Value: ${result_df['BenchmarkValue'].iloc[-1]:,.2f}\")\n",
    "        print(f\"üéØ Strategy Return: {((result_df['PortfolioValue'].iloc[-1] / self.initial_cash - 1) * 100):.2f}%\")\n",
    "        print(f\"üìä Benchmark Return: {((result_df['BenchmarkValue'].iloc[-1] / self.initial_cash - 1) * 100):.2f}%\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "# === Execute\n",
    "backtester = ETFBacktester(\n",
    "    model_dir=\"../model_weights\",\n",
    "    data_dir=\"../dataset/normalized_matrix\",\n",
    "    price_path=\"../dataset/etf_prices_weekly.csv\",\n",
    "    initial_cash=2000,\n",
    "    benchmark_symbol=\"SPY\",\n",
    "    seq_len=4  # Match the transformer input\n",
    ")\n",
    "result_df = backtester.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stock_pred)",
   "language": "python",
   "name": "stock_pred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
