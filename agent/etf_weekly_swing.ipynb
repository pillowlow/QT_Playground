{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55a81306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóïÔ∏è Downloading weekly data from 2015-05-08 to 2025-04-25\n",
      "‚¨áÔ∏è Downloading XLK...\n",
      "‚¨áÔ∏è Downloading XLF...\n",
      "‚¨áÔ∏è Downloading XLV...\n",
      "‚¨áÔ∏è Downloading XLE...\n",
      "‚¨áÔ∏è Downloading XLI...\n",
      "‚¨áÔ∏è Downloading XLY...\n",
      "‚¨áÔ∏è Downloading XLP...\n",
      "‚¨áÔ∏è Downloading XLRE...\n",
      "‚¨áÔ∏è Downloading XLU...\n",
      "‚¨áÔ∏è Downloading XLB...\n",
      "‚¨áÔ∏è Downloading XLC...\n",
      "‚¨áÔ∏è Downloading SOXX...\n",
      "‚¨áÔ∏è Downloading SH...\n",
      "‚¨áÔ∏è Downloading DOG...\n",
      "‚¨áÔ∏è Downloading RWM...\n",
      "‚¨áÔ∏è Downloading ITA...\n",
      "‚¨áÔ∏è Downloading JETS...\n",
      "‚¨áÔ∏è Downloading PSQ...\n",
      "‚¨áÔ∏è Downloading VNQ...\n",
      "‚úÖ Saved: etf_prices_weekly.csv\n",
      "‚úÖ Saved: etf_volume_weekly.csv\n",
      "‚úÖ Saved: etf_high_weekly.csv\n",
      "‚úÖ Saved: etf_low_weekly.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Date</th>\n",
       "      <th>XLK</th>\n",
       "      <th>XLF</th>\n",
       "      <th>XLV</th>\n",
       "      <th>XLE</th>\n",
       "      <th>XLI</th>\n",
       "      <th>XLY</th>\n",
       "      <th>XLP</th>\n",
       "      <th>XLRE</th>\n",
       "      <th>XLU</th>\n",
       "      <th>XLB</th>\n",
       "      <th>XLC</th>\n",
       "      <th>SOXX</th>\n",
       "      <th>SH</th>\n",
       "      <th>DOG</th>\n",
       "      <th>RWM</th>\n",
       "      <th>ITA</th>\n",
       "      <th>JETS</th>\n",
       "      <th>PSQ</th>\n",
       "      <th>VNQ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th>XLK</th>\n",
       "      <th>XLF</th>\n",
       "      <th>XLV</th>\n",
       "      <th>XLE</th>\n",
       "      <th>XLI</th>\n",
       "      <th>XLY</th>\n",
       "      <th>XLP</th>\n",
       "      <th>XLRE</th>\n",
       "      <th>XLU</th>\n",
       "      <th>XLB</th>\n",
       "      <th>XLC</th>\n",
       "      <th>SOXX</th>\n",
       "      <th>SH</th>\n",
       "      <th>DOG</th>\n",
       "      <th>RWM</th>\n",
       "      <th>ITA</th>\n",
       "      <th>JETS</th>\n",
       "      <th>PSQ</th>\n",
       "      <th>VNQ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-05-08</th>\n",
       "      <td>2015-05-08</td>\n",
       "      <td>38.081741</td>\n",
       "      <td>16.607914</td>\n",
       "      <td>62.328041</td>\n",
       "      <td>54.583641</td>\n",
       "      <td>47.107685</td>\n",
       "      <td>68.427338</td>\n",
       "      <td>37.711849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.769207</td>\n",
       "      <td>42.112530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.441837</td>\n",
       "      <td>153.447144</td>\n",
       "      <td>78.749908</td>\n",
       "      <td>51.727150</td>\n",
       "      <td>54.787785</td>\n",
       "      <td>23.647642</td>\n",
       "      <td>237.650406</td>\n",
       "      <td>53.803043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-15</th>\n",
       "      <td>2015-05-15</td>\n",
       "      <td>38.329540</td>\n",
       "      <td>16.567669</td>\n",
       "      <td>63.015366</td>\n",
       "      <td>53.855934</td>\n",
       "      <td>47.548946</td>\n",
       "      <td>68.418404</td>\n",
       "      <td>38.156696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.978456</td>\n",
       "      <td>42.063541</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.762247</td>\n",
       "      <td>152.713654</td>\n",
       "      <td>78.259262</td>\n",
       "      <td>51.239155</td>\n",
       "      <td>55.656300</td>\n",
       "      <td>23.714334</td>\n",
       "      <td>235.520844</td>\n",
       "      <td>54.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-22</th>\n",
       "      <td>2015-05-22</td>\n",
       "      <td>38.586185</td>\n",
       "      <td>16.674992</td>\n",
       "      <td>63.626339</td>\n",
       "      <td>53.542175</td>\n",
       "      <td>47.374115</td>\n",
       "      <td>68.731079</td>\n",
       "      <td>37.773212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.158836</td>\n",
       "      <td>41.745052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.208433</td>\n",
       "      <td>152.273544</td>\n",
       "      <td>78.329346</td>\n",
       "      <td>50.925442</td>\n",
       "      <td>55.633801</td>\n",
       "      <td>22.027943</td>\n",
       "      <td>233.476547</td>\n",
       "      <td>53.515572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-29</th>\n",
       "      <td>2015-05-29</td>\n",
       "      <td>38.382652</td>\n",
       "      <td>16.500593</td>\n",
       "      <td>63.592400</td>\n",
       "      <td>52.333775</td>\n",
       "      <td>46.466610</td>\n",
       "      <td>68.159340</td>\n",
       "      <td>37.420418</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.108334</td>\n",
       "      <td>41.328587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.265497</td>\n",
       "      <td>153.520508</td>\n",
       "      <td>79.205505</td>\n",
       "      <td>51.125870</td>\n",
       "      <td>54.702290</td>\n",
       "      <td>22.008886</td>\n",
       "      <td>234.243179</td>\n",
       "      <td>52.907204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-05</th>\n",
       "      <td>2015-06-05</td>\n",
       "      <td>38.010941</td>\n",
       "      <td>16.628035</td>\n",
       "      <td>63.083271</td>\n",
       "      <td>51.853123</td>\n",
       "      <td>46.508221</td>\n",
       "      <td>68.355865</td>\n",
       "      <td>36.500038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.823999</td>\n",
       "      <td>40.838623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.474941</td>\n",
       "      <td>154.547379</td>\n",
       "      <td>79.906471</td>\n",
       "      <td>50.472309</td>\n",
       "      <td>54.378307</td>\n",
       "      <td>22.018414</td>\n",
       "      <td>235.776413</td>\n",
       "      <td>51.616924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price             Date        XLK        XLF        XLV        XLE        XLI  \\\n",
       "Ticker                        XLK        XLF        XLV        XLE        XLI   \n",
       "Date                                                                            \n",
       "2015-05-08  2015-05-08  38.081741  16.607914  62.328041  54.583641  47.107685   \n",
       "2015-05-15  2015-05-15  38.329540  16.567669  63.015366  53.855934  47.548946   \n",
       "2015-05-22  2015-05-22  38.586185  16.674992  63.626339  53.542175  47.374115   \n",
       "2015-05-29  2015-05-29  38.382652  16.500593  63.592400  52.333775  46.466610   \n",
       "2015-06-05  2015-06-05  38.010941  16.628035  63.083271  51.853123  46.508221   \n",
       "\n",
       "Price             XLY        XLP XLRE        XLU        XLB XLC       SOXX  \\\n",
       "Ticker            XLY        XLP XLRE        XLU        XLB XLC       SOXX   \n",
       "Date                                                                         \n",
       "2015-05-08  68.427338  37.711849  NaN  31.769207  42.112530 NaN  28.441837   \n",
       "2015-05-15  68.418404  38.156696  NaN  31.978456  42.063541 NaN  28.762247   \n",
       "2015-05-22  68.731079  37.773212  NaN  32.158836  41.745052 NaN  29.208433   \n",
       "2015-05-29  68.159340  37.420418  NaN  32.108334  41.328587 NaN  30.265497   \n",
       "2015-06-05  68.355865  36.500038  NaN  30.823999  40.838623 NaN  29.474941   \n",
       "\n",
       "Price               SH        DOG        RWM        ITA       JETS  \\\n",
       "Ticker              SH        DOG        RWM        ITA       JETS   \n",
       "Date                                                                 \n",
       "2015-05-08  153.447144  78.749908  51.727150  54.787785  23.647642   \n",
       "2015-05-15  152.713654  78.259262  51.239155  55.656300  23.714334   \n",
       "2015-05-22  152.273544  78.329346  50.925442  55.633801  22.027943   \n",
       "2015-05-29  153.520508  79.205505  51.125870  54.702290  22.008886   \n",
       "2015-06-05  154.547379  79.906471  50.472309  54.378307  22.018414   \n",
       "\n",
       "Price              PSQ        VNQ  \n",
       "Ticker             PSQ        VNQ  \n",
       "Date                               \n",
       "2015-05-08  237.650406  53.803043  \n",
       "2015-05-15  235.520844  54.210800  \n",
       "2015-05-22  233.476547  53.515572  \n",
       "2015-05-29  234.243179  52.907204  \n",
       "2015-06-05  235.776413  51.616924  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# === ETF list ===\n",
    "etf_list = [\n",
    "    'XLK', 'XLF', 'XLV', 'XLE', 'XLI', 'XLY', 'XLP', 'XLRE', 'XLU', 'XLB', 'XLC',\n",
    "    'SOXX', 'SH', 'DOG', 'RWM', 'ITA', 'JETS', 'PSQ', 'VNQ'\n",
    "]\n",
    "\n",
    "# === Date range ===\n",
    "today = datetime.today()\n",
    "start_date = (today - timedelta(weeks=52 * 10)).strftime('%Y-%m-%d')\n",
    "end_date = today.strftime('%Y-%m-%d')\n",
    "print(f\"üóïÔ∏è Downloading weekly data from {start_date} to {end_date} (until last friday)\")\n",
    "\n",
    "# === Output folder ===\n",
    "dataset_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset'))\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# === Containers ===\n",
    "adjclose_data, volume_data, high_data, low_data = {}, {}, {}, {}\n",
    "\n",
    "# === Shift logic ===\n",
    "def shift_to_next_friday(date):\n",
    "    weekday = date.weekday()\n",
    "    days_until_friday = (4 - weekday + 7) % 7\n",
    "    return date + timedelta(days=days_until_friday)\n",
    "\n",
    "# === Download each ETF ===\n",
    "for symbol in etf_list:\n",
    "    print(f\"‚¨áÔ∏è Downloading {symbol}...\")\n",
    "    data = yf.download(\n",
    "        symbol,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        interval='1wk',\n",
    "        auto_adjust=False,\n",
    "        progress=False\n",
    "    )\n",
    "    if not data.empty:\n",
    "        data = data[~data.index.duplicated(keep='first')].sort_index()\n",
    "        data.index = data.index.to_series().apply(shift_to_next_friday)\n",
    "\n",
    "        # ‚ùå Filter out future dates after shifting\n",
    "        data = data[data.index <= pd.Timestamp(today.date())]\n",
    "\n",
    "        # ‚úÖ Store\n",
    "        adjclose_data[symbol] = data[['Adj Close']].rename(columns={'Adj Close': symbol})\n",
    "        volume_data[symbol] = data[['Volume']].rename(columns={'Volume': symbol})\n",
    "        high_data[symbol] = data[['High']].rename(columns={'High': symbol})\n",
    "        low_data[symbol] = data[['Low']].rename(columns={'Low': symbol})\n",
    "\n",
    "# === Merge and Save in clean format ===\n",
    "def combine_and_save(data_dict, filename):\n",
    "    df = pd.concat(data_dict.values(), axis=1)\n",
    "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "    df = df[df.index < pd.Timestamp(today.date())]  # üö´ Final future filtering\n",
    "    df = df[~df.index.duplicated(keep='first')].sort_index()\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    df.insert(0, 'Date', df.index.strftime('%Y-%m-%d'))\n",
    "    df.to_csv(os.path.join(dataset_path, filename), index=False)\n",
    "    print(f\"‚úÖ Saved: {filename}\")\n",
    "    return df\n",
    "\n",
    "# === Save all ===\n",
    "price_df = combine_and_save(adjclose_data, 'etf_prices_weekly.csv')\n",
    "volume_df = combine_and_save(volume_data, 'etf_volume_weekly.csv')\n",
    "high_df = combine_and_save(high_data, 'etf_high_weekly.csv')\n",
    "low_df = combine_and_save(low_data, 'etf_low_weekly.csv')\n",
    "\n",
    "# === Preview ===\n",
    "price_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4ecccfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Downloading macro indicators from 2015-05-08 to 2025-04-25\n",
      "‚¨áÔ∏è Downloading VIX (^VIX)...\n",
      "‚¨áÔ∏è Downloading 10Y_Yield (^TNX)...\n",
      "‚¨áÔ∏è Downloading 2Y_Yield (^IRX)...\n",
      "‚¨áÔ∏è Downloading USD_Index (DX-Y.NYB)...\n",
      "‚¨áÔ∏è Downloading WTI_Crude (CL=F)...\n",
      "‚¨áÔ∏è Downloading SPY_Price (SPY)...\n",
      "‚¨áÔ∏è Downloading QQQ_Price (QQQ)...\n",
      "‚úÖ Macro indicators saved to: D:\\CodingWorks\\Weekly_Swing_TransformerQT\\dataset\\macro_indicators_weekly.csv\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# === Macro indicator tickers ===\n",
    "macro_tickers = {\n",
    "    'VIX': '^VIX',\n",
    "    '10Y_Yield': '^TNX',   # Scale * 0.1\n",
    "    '2Y_Yield': '^IRX',    # Scale * 0.01\n",
    "    'USD_Index': 'DX-Y.NYB',\n",
    "    'WTI_Crude': 'CL=F',\n",
    "    'SPY_Price': 'SPY',\n",
    "    'QQQ_Price': 'QQQ'\n",
    "}\n",
    "\n",
    "# === Date range ===\n",
    "today = datetime.today()\n",
    "back_time = today - timedelta(weeks=52 * 10)\n",
    "start_date = back_time.strftime('%Y-%m-%d')\n",
    "end_date = today.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"üìä Downloading macro indicators from {start_date} to {end_date}\")\n",
    "\n",
    "# === Output path ===\n",
    "macro_save_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'macro_indicators_weekly.csv'))\n",
    "\n",
    "# === Shift helper ===\n",
    "def shift_to_next_friday(date):\n",
    "    days_ahead = (4 - date.weekday() + 7) % 7\n",
    "    return date + timedelta(days=days_ahead)\n",
    "\n",
    "# === Download each macro ticker ===\n",
    "macro_data = {}\n",
    "for name, ticker in macro_tickers.items():\n",
    "    print(f\"‚¨áÔ∏è Downloading {name} ({ticker})...\")\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, interval='1wk', auto_adjust=False, progress=False)\n",
    "    if not data.empty:\n",
    "        data = data[~data.index.duplicated(keep='first')].sort_index()\n",
    "        data.index = pd.to_datetime([shift_to_next_friday(d) for d in data.index])\n",
    "        #filter futrue data\n",
    "        data = data[data.index < pd.Timestamp(today.date())]\n",
    "        macro_data[name] = data[['Close']].rename(columns={'Close': name})\n",
    "\n",
    "# === Combine into single DataFrame ===\n",
    "macro_df = pd.concat(macro_data.values(), axis=1)\n",
    "macro_df.index = pd.to_datetime(macro_df.index)\n",
    "macro_df = macro_df[~macro_df.index.duplicated(keep='first')].sort_index()\n",
    "\n",
    "# === Apply scaling to yields ===\n",
    "if '10Y_Yield' in macro_df.columns:\n",
    "    macro_df['10Y_Yield'] *= 0.1\n",
    "if '2Y_Yield' in macro_df.columns:\n",
    "    macro_df['2Y_Yield'] *= 0.01\n",
    "\n",
    "# === Final cleanup ===\n",
    "macro_df.dropna(axis=0, how='all', inplace=True)\n",
    "macro_df.insert(0, 'Date', macro_df.index.strftime('%Y-%m-%d'))\n",
    "macro_df.to_csv(macro_save_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Macro indicators saved to: {macro_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e27fa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded ETF Prices with 19 tickers\n",
      "‚úÖ Loaded ETF Volume with 19 tickers\n",
      "üìà XLK: 521 valid rows\n",
      "üìà XLF: 521 valid rows\n",
      "üìà XLV: 521 valid rows\n",
      "üìà XLE: 521 valid rows\n",
      "üìà XLI: 521 valid rows\n",
      "üìà XLY: 521 valid rows\n",
      "üìà XLP: 521 valid rows\n",
      "üìà XLRE: 508 valid rows\n",
      "üìà XLU: 521 valid rows\n",
      "üìà XLB: 521 valid rows\n",
      "üìà XLC: 508 valid rows\n",
      "üìà SOXX: 521 valid rows\n",
      "üìà SH: 521 valid rows\n",
      "üìà DOG: 521 valid rows\n",
      "üìà RWM: 521 valid rows\n",
      "üìà ITA: 521 valid rows\n",
      "üìà JETS: 521 valid rows\n",
      "üìà PSQ: 521 valid rows\n",
      "üìà VNQ: 521 valid rows\n",
      "üìÄ Final feature shape: (521, 247)\n",
      "‚úÖ Saved features to: weekly_etf_tech_feature.csv\n"
     ]
    }
   ],
   "source": [
    "# calculate TA signals\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from ta import momentum, trend, volume\n",
    "\n",
    "# === Paths ===\n",
    "price_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'etf_prices_weekly.csv'))\n",
    "volume_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'etf_volume_weekly.csv'))\n",
    "\n",
    "# === Helper to load ETF CSVs ===\n",
    "def load_etf_csv(path, name='[unknown]'):\n",
    "    try:\n",
    "        header_row = pd.read_csv(path, header=None, nrows=2)\n",
    "        columns = header_row.iloc[1].tolist()[1:]\n",
    "        df = pd.read_csv(path, skiprows=3, header=None)\n",
    "        df = df.iloc[:, :len(columns) + 1]\n",
    "        df.columns = ['Date'] + columns\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        df = df.set_index('Date')\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "        print(f\"‚úÖ Loaded {name} with {len(columns)} tickers\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {name}: {e}\")\n",
    "        raise\n",
    "\n",
    "# === Load data ===\n",
    "price_df = load_etf_csv(price_path, name='ETF Prices')\n",
    "volume_df = load_etf_csv(volume_path, name='ETF Volume')\n",
    "\n",
    "# === Feature storage ===\n",
    "features_all = []\n",
    "skipped = []\n",
    "\n",
    "# === Feature generation loop ===\n",
    "for symbol in price_df.columns:\n",
    "    if symbol not in volume_df.columns:\n",
    "        print(f\"‚ö†Ô∏è Skipping {symbol}: volume data missing.\")\n",
    "        skipped.append(symbol)\n",
    "        continue\n",
    "\n",
    "    df = pd.DataFrame(index=price_df.index)\n",
    "    df['close'] = price_df[symbol]\n",
    "    df['volume'] = volume_df[symbol]\n",
    "\n",
    "    try:\n",
    "        # === Return-based technical indicators ===\n",
    "        df[f'{symbol}_ret_1w'] = df['close'].pct_change(1)\n",
    "        df[f'{symbol}_ret_3w'] = df['close'].pct_change(3)\n",
    "        df[f'{symbol}_ret_6w'] = df['close'].pct_change(6)\n",
    "\n",
    "        high = df['close'].rolling(window=14).max()\n",
    "        low = df['close'].rolling(window=14).min()\n",
    "        df[f'{symbol}_stoch_k'] = 100 * (df['close'] - low) / (high - low)\n",
    "        df[f'{symbol}_stoch_d'] = df[f'{symbol}_stoch_k'].rolling(window=3).mean()\n",
    "        df[f'{symbol}_williams_r'] = -100 * (high - df['close']) / (high - low)\n",
    "\n",
    "        df[f'{symbol}_cci'] = trend.cci(high=df['close'], low=df['close'], close=df['close'], window=20)\n",
    "        df[f'{symbol}_rsi'] = momentum.rsi(df['close'], window=14)\n",
    "        df[f'{symbol}_obv'] = volume.on_balance_volume(df['close'], df['volume'])\n",
    "\n",
    "        df[f'{symbol}_macd'] = trend.macd(df['close'])\n",
    "        df[f'{symbol}_macd_signal'] = trend.macd_signal(df['close'])\n",
    "        df[f'{symbol}_macd_diff'] = trend.macd_diff(df['close'])\n",
    "\n",
    "        # === Short-term KST (custom) ===\n",
    "        roc1 = df['close'].pct_change(10)\n",
    "        roc2 = df['close'].pct_change(15)\n",
    "        roc3 = df['close'].pct_change(20)\n",
    "        roc4 = df['close'].pct_change(30)\n",
    "        df[f'{symbol}_kst_short'] = (\n",
    "            roc1.rolling(10).mean() +\n",
    "            roc2.rolling(10).mean() * 2 +\n",
    "            roc3.rolling(10).mean() * 3 +\n",
    "            roc4.rolling(15).mean() * 4\n",
    "        )\n",
    "\n",
    "        derived_cols = df.columns.difference(['close', 'volume'])\n",
    "        feature_df = df[derived_cols].copy()\n",
    "\n",
    "        features_all.append(feature_df)\n",
    "        print(f\"üìà {symbol}: {feature_df.dropna(how='all').shape[0]} valid rows\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {symbol}: {e}\")\n",
    "        skipped.append(symbol)\n",
    "\n",
    "# === Final merge ===\n",
    "if len(features_all) == 0:\n",
    "    raise ValueError(\"üõë No valid ETF features generated.\")\n",
    "\n",
    "features_df = pd.concat(features_all, axis=1).sort_index()\n",
    "features_df = features_df[~features_df.index.duplicated(keep='first')]\n",
    "\n",
    "print(\"üìÄ Final feature shape:\", features_df.shape)\n",
    "\n",
    "# === Save to CSV ===\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset'))\n",
    "features_df.to_csv(os.path.join(base_dir, 'weekly_etf_tech_feature.csv'))\n",
    "print(f\"‚úÖ Saved features to: weekly_etf_tech_feature.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b6cb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßæ Tickers detected: ['DOG', 'ITA', 'JETS', 'PSQ', 'RWM', 'SH', 'SOXX', 'VNQ', 'XLB', 'XLC', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLRE', 'XLU', 'XLV', 'XLY']\n",
      "‚úÖ Saved DOG: 521 rows, 21 features\n",
      "‚úÖ Saved ITA: 521 rows, 21 features\n",
      "‚úÖ Saved JETS: 521 rows, 21 features\n",
      "‚úÖ Saved PSQ: 521 rows, 21 features\n",
      "‚úÖ Saved RWM: 521 rows, 21 features\n",
      "‚úÖ Saved SH: 521 rows, 21 features\n",
      "‚úÖ Saved SOXX: 521 rows, 21 features\n",
      "‚úÖ Saved VNQ: 521 rows, 21 features\n",
      "‚úÖ Saved XLB: 521 rows, 21 features\n",
      "‚úÖ Saved XLC: 521 rows, 21 features\n",
      "‚úÖ Saved XLE: 521 rows, 21 features\n",
      "‚úÖ Saved XLF: 521 rows, 21 features\n",
      "‚úÖ Saved XLI: 521 rows, 21 features\n",
      "‚úÖ Saved XLK: 521 rows, 21 features\n",
      "‚úÖ Saved XLP: 521 rows, 21 features\n",
      "‚úÖ Saved XLRE: 521 rows, 21 features\n",
      "‚úÖ Saved XLU: 521 rows, 21 features\n",
      "‚úÖ Saved XLV: 521 rows, 21 features\n",
      "‚úÖ Saved XLY: 521 rows, 21 features\n"
     ]
    }
   ],
   "source": [
    "# normalizing matrix add masks\n",
    "\n",
    "\n",
    "# Normalize and mask using consistent CSV structure\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Paths ===\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset'))\n",
    "feat_path = os.path.join(base_dir, 'weekly_etf_tech_feature.csv')\n",
    "macro_path = os.path.join(base_dir, 'macro_indicators_weekly.csv')\n",
    "price_path = os.path.join(base_dir, 'etf_prices_weekly.csv')\n",
    "\n",
    "# === Load Data ===\n",
    "feat_df = pd.read_csv(feat_path)\n",
    "macro_df = pd.read_csv(macro_path)\n",
    "price_df = pd.read_csv(price_path)\n",
    "\n",
    "# === Set Index ===\n",
    "feat_df.set_index('Date', inplace=True)\n",
    "macro_df.set_index('Date', inplace=True)\n",
    "price_df.set_index('Date', inplace=True)\n",
    "\n",
    "# === Convert index to datetime\n",
    "today = pd.Timestamp.today().normalize()\n",
    "feat_df.index = pd.to_datetime(feat_df.index)\n",
    "macro_df.index = pd.to_datetime(macro_df.index)\n",
    "price_df.index = pd.to_datetime(price_df.index)\n",
    "\n",
    "feat_df = feat_df[feat_df.index < today]\n",
    "macro_df = macro_df[macro_df.index < today]\n",
    "price_df = price_df[price_df.index < today]\n",
    "\n",
    "# === Drop duplicated rows if any\n",
    "feat_df = feat_df[~feat_df.index.duplicated()]\n",
    "macro_df = macro_df[~macro_df.index.duplicated()]\n",
    "price_df = price_df[~price_df.index.duplicated()]\n",
    "\n",
    "# === Output Directory ===\n",
    "norm_dir = os.path.join(base_dir, 'normalized_matrix')\n",
    "os.makedirs(norm_dir, exist_ok=True)\n",
    "\n",
    "# === Scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# === Detect unique tickers\n",
    "all_cols = feat_df.columns\n",
    "tickers = sorted(set(col.split('_')[0] for col in all_cols if '_' in col))\n",
    "print(f\"üßæ Tickers detected: {tickers}\")\n",
    "\n",
    "for ticker in tickers:\n",
    "    feat_cols = [c for c in feat_df.columns if c.startswith(f\"{ticker}_\")]\n",
    "    if ticker not in price_df.columns:\n",
    "        print(f\"‚ö†Ô∏è Skipped {ticker}: no matching price data.\")\n",
    "        continue\n",
    "\n",
    "    # Build DataFrame with date index\n",
    "    df = pd.DataFrame(index=feat_df.index)\n",
    "\n",
    "    # Add technical indicators\n",
    "    for col in feat_cols:\n",
    "        clean_name = col.replace(f\"{ticker}_\", \"\")\n",
    "        df[clean_name] = feat_df[col]\n",
    "\n",
    "    # Add price\n",
    "    df.insert(0, 'price', price_df[ticker])\n",
    "\n",
    "    # Add macro indicators\n",
    "    df = df.join(macro_df, how='left')\n",
    "\n",
    "    # Create mask before filling NAs\n",
    "    mask = df.isna().astype(float)\n",
    "\n",
    "    # Custom masking logic\n",
    "    if ticker == 'PSQ':\n",
    "        mask_cols = [c for c in df.columns if 'QQQ' in c]\n",
    "        df[mask_cols] = 0.0\n",
    "        mask[mask_cols] = 1.0\n",
    "    if ticker == 'SH':\n",
    "        mask_cols = [c for c in df.columns if 'SPY' in c]\n",
    "        df[mask_cols] = 0.0\n",
    "        mask[mask_cols] = 1.0\n",
    "\n",
    "    # Normalize technical (non-macro) features\n",
    "    macro_cols = macro_df.columns\n",
    "    norm_cols = df.columns.difference(macro_cols.union({'price'}))\n",
    "\n",
    "    if df[norm_cols].dropna(how='all').empty:\n",
    "        print(f\"‚ö†Ô∏è Skipped {ticker}: no valid technical features.\")\n",
    "        continue\n",
    "\n",
    "    df[norm_cols] = scaler.fit_transform(df[norm_cols].fillna(0.0))\n",
    "    df[macro_cols] = df[macro_cols].fillna(0.0)\n",
    "\n",
    "    # === Save\n",
    "    df.to_csv(os.path.join(norm_dir, f'{ticker}_combined.csv'))\n",
    "    mask.to_csv(os.path.join(norm_dir, f'{ticker}_mask.csv'))\n",
    "    print(f\"‚úÖ Saved {ticker}: {df.shape[0]} rows, {df.shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e3617",
   "metadata": {},
   "source": [
    "# start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1565c35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training XLK\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "‚ùå Target column XLK_price_change not found in D:\\CodingWorks\\Weekly_Swing_TransformerQT\\dataset\\normalized_matrix\\XLK_combined.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 157\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# === Execute ===\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m etf \u001b[38;5;129;01min\u001b[39;00m etf_list:\n\u001b[1;32m--> 157\u001b[0m     \u001b[43mtrain_one_etf\u001b[49m\u001b[43m(\u001b[49m\u001b[43metf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 93\u001b[0m, in \u001b[0;36mtrain_one_etf\u001b[1;34m(etf)\u001b[0m\n\u001b[0;32m     91\u001b[0m feat_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00metf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_combined.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m mask_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00metf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mask.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mETFDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprice_change\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     96\u001b[0m X, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(loader))\n",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m, in \u001b[0;36mETFDataset.__init__\u001b[1;34m(self, combined_csv, mask_csv, etf, return_feature, seq_len)\u001b[0m\n\u001b[0;32m     19\u001b[0m target_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00metf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_feature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå Target column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined_csv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìå Predicting: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(target_col)\n",
      "\u001b[1;31mValueError\u001b[0m: ‚ùå Target column XLK_price_change not found in D:\\CodingWorks\\Weekly_Swing_TransformerQT\\dataset\\normalized_matrix\\XLK_combined.csv"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'normalized_matrix'))\n",
    "\n",
    "# === Dataset ===\n",
    "class ETFDataset(Dataset):\n",
    "    def __init__(self, combined_csv, mask_csv, etf, return_feature=\"price_change\", seq_len=4):\n",
    "        self.df = pd.read_csv(combined_csv, index_col=0)\n",
    "        self.mask = pd.read_csv(mask_csv, index_col=0)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Extract the correct target column\n",
    "        target_col = f\"{etf}_{return_feature}\"\n",
    "        if target_col not in self.df.columns:\n",
    "            raise ValueError(f\"‚ùå Target column {target_col} not found in {combined_csv}\")\n",
    "\n",
    "        print(f\"üìå Predicting: {target_col}\")\n",
    "\n",
    "        self.target_index = self.df.columns.get_loc(target_col)\n",
    "        self.raw_X = self.df.values.astype(np.float32)\n",
    "        self.raw_M = self.mask.values.astype(np.float32)\n",
    "\n",
    "        self.X, self.M, self.y = [], [], []\n",
    "        for i in range(seq_len, len(self.raw_X)):\n",
    "            self.X.append(self.raw_X[i-seq_len:i])\n",
    "            self.M.append(self.raw_M[i-seq_len:i])\n",
    "            self.y.append(self.raw_X[i, self.target_index])\n",
    "\n",
    "        self.X = np.stack(self.X)\n",
    "        self.M = np.stack(self.M)\n",
    "        self.y = np.array(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.X[idx]),\n",
    "            torch.tensor(self.M[idx]),\n",
    "            torch.tensor(self.y[idx])\n",
    "        )\n",
    "\n",
    "\n",
    "# === Model with Attention Pooling ===\n",
    "class DualTransformerModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=128, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Choose the highest possible nhead divisor of num_features\n",
    "        possible_heads = [h for h in [8, 4, 2, 1] if num_features % h == 0]\n",
    "        nhead = possible_heads[0]\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=num_features,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=hidden_dim,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.attn_query = nn.Parameter(torch.randn(1, 1, num_features))\n",
    "        self.output = nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x_masked = x * (1 - mask)\n",
    "        enc = self.encoder(x_masked)\n",
    "\n",
    "        # Apply learnable attention pooling\n",
    "        attn_scores = torch.matmul(self.attn_query, enc.transpose(1, 2))  # (1, 1, F) x (B, F, T) = (B, 1, T)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)  # (B, 1, T)\n",
    "        pooled = torch.bmm(attn_weights, enc).squeeze(1)  # (B, 1, T) x (B, T, F) = (B, 1, F) ‚Üí (B, F)\n",
    "\n",
    "        return self.output(pooled).squeeze(-1)\n",
    "\n",
    "# === Training Params ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 0.001\n",
    "max_epochs = 3000\n",
    "seq_len = 4\n",
    "\n",
    "# === Training Function ===\n",
    "def train_one_etf(etf):\n",
    "    print(f\"\\nüöÄ Training {etf}\")\n",
    "    feat_path = os.path.join(data_dir, f\"{etf}_combined.csv\")\n",
    "    mask_path = os.path.join(data_dir, f\"{etf}_mask.csv\")\n",
    "    dataset = ETFDataset(feat_path, mask_path, etf=etf, return_feature=\"price_change\", seq_len=seq_len)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    X, _, _ = next(iter(loader))\n",
    "    model = DualTransformerModel(num_features=X.shape[-1]).to(device)\n",
    "    model.device = device\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    mae_loss = nn.L1Loss()\n",
    "\n",
    "    best_models = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss, total_mae, total_win = 0, 0, 0\n",
    "\n",
    "        for X, M, y in loader:\n",
    "            X, M, y = X.to(model.device), M.to(model.device), y.to(model.device)\n",
    "            pred = model(X, M)\n",
    "\n",
    "            mse = mse_loss(pred, y)\n",
    "            mae = mae_loss(pred, y)\n",
    "            direction_loss = 1 - (torch.sign(pred) == torch.sign(y)).float().mean()\n",
    "            loss = 0.4*mse + 0.25 * mae + 0.35 * direction_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_mae += mae.item()\n",
    "            total_win += (torch.sign(pred) == torch.sign(y)).float().mean().item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        avg_mae = total_mae / len(loader)\n",
    "        avg_win = total_win / len(loader)\n",
    "        score = avg_win * 0.6 + (1 - avg_mae) * 0.4\n",
    "        best_models.append((score, model.state_dict(), avg_win, avg_mae))\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, MAE={avg_mae:.4f}, WinRate={avg_win:.4f}\")\n",
    "\n",
    "    # Save top 5 models\n",
    "    top5 = sorted(best_models, key=lambda x: -x[0])[:5]\n",
    "    weight_sum = sum(x[0] for x in top5)\n",
    "\n",
    "    # Ensure model_weights folder exists\n",
    "    save_dir = os.path.join(\"..\", \"model_weights\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for i, (score, weights, win, mae) in enumerate(top5):\n",
    "        out = {\n",
    "            'weights': weights,\n",
    "            'score': score,\n",
    "            'win_rate': win,\n",
    "            'mae': mae,\n",
    "            'weight': score / weight_sum\n",
    "        }\n",
    "        save_path = os.path.join(save_dir, f\"{etf}_top{i+1}.pt\")\n",
    "        torch.save(out, save_path)\n",
    "\n",
    "# === Execute ===\n",
    "for etf in etf_list:\n",
    "    train_one_etf(etf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb75d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "from datetime import timedelta\n",
    "\n",
    "# === Paths ===\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'normalized_matrix'))\n",
    "model_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'model_weights'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Gather predictions\n",
    "plot_data = []\n",
    "\n",
    "for etf in etf_list:\n",
    "    try:\n",
    "        feat_path = os.path.join(data_dir, f\"{etf}_combined.csv\")\n",
    "        mask_path = os.path.join(data_dir, f\"{etf}_mask.csv\")\n",
    "        model_path = os.path.join(model_dir, f\"{etf}_top1.pt\")\n",
    "\n",
    "        df_feat = pd.read_csv(feat_path, index_col=0).astype('float32')\n",
    "        df_mask = pd.read_csv(mask_path, index_col=0).astype('float32')\n",
    "\n",
    "        if len(df_feat) < 5:\n",
    "            continue\n",
    "\n",
    "        X_seq = []\n",
    "        M_seq = []\n",
    "        y_seq = []\n",
    "        price_seq = df_feat['price_change'].copy()\n",
    "\n",
    "        for i in range(4, len(df_feat)):\n",
    "            X_seq.append(df_feat.iloc[i-4:i].values)\n",
    "            M_seq.append(df_mask.iloc[i-4:i].values)\n",
    "            return_col = \"price_change\"\n",
    "            target_index = df_feat.columns.get_loc(return_col)\n",
    "            y_seq.append(df_feat.iloc[i, target_index])\n",
    "\n",
    "        X_seq = torch.tensor(np.array(X_seq), dtype=torch.float32).to(device)\n",
    "        M_seq = torch.tensor(np.array(M_seq), dtype=torch.float32).to(device)\n",
    "        y_seq = np.array(y_seq)\n",
    "\n",
    "        model = DualTransformerModel(num_features=X_seq.shape[-1]).to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device)['weights'])\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(X_seq, M_seq).cpu().numpy()\n",
    "\n",
    "        date_index = pd.to_datetime(df_feat.index[4:])\n",
    "        price_base = df_feat['price_change'].shift(1).iloc[4:].reset_index(drop=True)\n",
    "        actual_prices = (1 + y_seq).cumprod() * 100\n",
    "        predicted_prices = (1 + pred).cumprod() * 100\n",
    "\n",
    "\n",
    "        df_plot = pd.DataFrame({\n",
    "            'Date': date_index,\n",
    "            'Predicted': predicted_prices,\n",
    "            'Actual': actual_prices\n",
    "        }).set_index('Date')\n",
    "        plot_data.append((etf, df_plot.iloc[-50:]))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading/predicting {etf}: {e}\")\n",
    "\n",
    "# === Plotting\n",
    "fig = plt.figure(figsize=(20, 4 * ((len(plot_data) + 3) // 4)))\n",
    "gs = gridspec.GridSpec((len(plot_data) + 3) // 4, 4, figure=fig)\n",
    "\n",
    "for i, (etf, df) in enumerate(plot_data):\n",
    "    ax = fig.add_subplot(gs[i])\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    ax.plot(df.index, df['Actual'], label='Actual (from price)', color='blue')\n",
    "    ax.plot(df.index, df['Predicted'], label='Predicted (simulated price)', color='red')\n",
    "    ax.set_title(etf)\n",
    "    ax.legend()\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    for j, (pred, actual) in enumerate(zip(df['Predicted'], df['Actual'])):\n",
    "        color = 'lightgreen' if np.sign(pred - actual) == 0 else 'lightgrey'\n",
    "        start = df.index[j]\n",
    "        end = start + timedelta(days=7)\n",
    "        ax.axvspan(start, end, color=color, alpha=0.2)\n",
    "\n",
    "fig.suptitle(\"üìà ETF Price Trajectory Simulated from Prediction vs. Actual\", fontsize=18)\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# === Prediction Class ===\n",
    "class WeeklyETFPredictor:\n",
    "    def __init__(self, model_dir, data_dir, record_dir, price_path, device=None):\n",
    "        self.model_dir = model_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.record_dir = record_dir\n",
    "        self.price_path = price_path\n",
    "        os.makedirs(self.record_dir, exist_ok=True)\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def predict(self):\n",
    "        # Get the nearest past Monday ‚Äî this represents the start of the prediction week\n",
    "        today = datetime.today()\n",
    "        days_back = today.weekday() % 7\n",
    "        monday = today - timedelta(days=days_back)\n",
    "        monday_dt = pd.to_datetime(monday.strftime(\"%Y-%m-%d\"))\n",
    "        date_str = monday_dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Load ETF price data for reference\n",
    "        price_df = pd.read_csv(\n",
    "            self.price_path,\n",
    "            index_col=0,\n",
    "            parse_dates=True,\n",
    "            date_parser=lambda x: pd.to_datetime(x, format=\"%Y-%m-%d\", errors='coerce')\n",
    "        )\n",
    "        price_df = price_df[price_df.index.notna()]\n",
    "        price_df = price_df[~price_df.index.duplicated()].sort_index()\n",
    "\n",
    "        print(f\"üîç Checking last available price dates: {price_df.index[-5:].to_list()}\")\n",
    "        print(f\"üìÖ Using feature/price data for week starting on: {monday_dt.date()}\")\n",
    "\n",
    "        summary = []\n",
    "\n",
    "        for fname in os.listdir(self.data_dir):\n",
    "            if not fname.endswith(\"_combined.csv\"):\n",
    "                continue\n",
    "\n",
    "            etf = fname.replace(\"_combined.csv\", \"\")\n",
    "            feat_path = os.path.join(self.data_dir, f\"{etf}_combined.csv\")\n",
    "            mask_path = os.path.join(self.data_dir, f\"{etf}_mask.csv\")\n",
    "\n",
    "            try:\n",
    "                df_feat = pd.read_csv(feat_path, index_col=0, parse_dates=True)\n",
    "                df_mask = pd.read_csv(mask_path, index_col=0, parse_dates=True)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading data for {etf}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if monday_dt not in df_feat.index:\n",
    "                print(f\"‚ö†Ô∏è No feature data for {etf} on {monday_dt.date()}\")\n",
    "                continue\n",
    "\n",
    "            if etf not in price_df.columns or monday_dt not in price_df.index:\n",
    "                print(f\"‚ö†Ô∏è Price info missing for {etf} on {monday_dt.date()}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Collect last 4 weeks (or pad if at beginning)\n",
    "                idx_pos = df_feat.index.get_loc(monday_dt)\n",
    "                if idx_pos < 3:\n",
    "                    # Pad beginning if not enough data\n",
    "                    x_real = df_feat.iloc[[0]* (4 - idx_pos) + list(range(idx_pos + 1))].values\n",
    "                    x_mask = df_mask.iloc[[0]* (4 - idx_pos) + list(range(idx_pos + 1))].values\n",
    "                else:\n",
    "                    x_real = df_feat.iloc[idx_pos-3:idx_pos+1].values\n",
    "                    x_mask = df_mask.iloc[idx_pos-3:idx_pos+1].values\n",
    "                \n",
    "                # Final tensor format: [1, 4, num_features]\n",
    "                x_real = torch.tensor(x_real.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "                x_mask = torch.tensor(x_mask.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "\n",
    "                price = float(price_df.loc[monday_dt, etf])\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Tensor prep or price parse failed for {etf}: {e}\")\n",
    "                continue\n",
    "\n",
    "            scores, preds, maes, winrates = [], [], [], []\n",
    "            for i in range(1, 6):\n",
    "                path = os.path.join(self.model_dir, f\"{etf}_top{i}.pt\")\n",
    "                if not os.path.exists(path): continue\n",
    "\n",
    "                try:\n",
    "                    checkpoint = torch.load(path, map_location=self.device)\n",
    "                    model = DualTransformerModel(num_features=x_real.shape[-1]).to(self.device)\n",
    "                    model.load_state_dict(checkpoint['weights'])\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        pred = model(x_real, x_mask).item()\n",
    "                    preds.append(pred)\n",
    "                    maes.append(checkpoint['mae'])\n",
    "                    winrates.append(checkpoint['win_rate'])\n",
    "                    scores.append(checkpoint['weight'])\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Model predict error for {etf} top{i}: {e}\")\n",
    "\n",
    "            if len(preds) == 0:\n",
    "                continue\n",
    "\n",
    "            pred_return = sum(p * w for p, w in zip(preds, scores))\n",
    "            avg_mae = sum(maes) / len(maes)\n",
    "            avg_win = sum(winrates) / len(winrates)\n",
    "\n",
    "            if pred_return >= 0:\n",
    "                target_up = round(price * (1 + pred_return / 100 + pred_return * avg_mae / 100), 2)\n",
    "                stop_down = round(price * (1 - pred_return * avg_mae / 100), 2)\n",
    "                buy_price = round(price, 2)\n",
    "            else:\n",
    "                target_up = stop_down = buy_price = \"X\"\n",
    "\n",
    "            summary.append({\n",
    "                'ETF': etf,\n",
    "                'PredictedReturn': round(pred_return, 4),\n",
    "                'MAE': round(avg_mae, 4),\n",
    "                'WinRate': round(avg_win, 4),\n",
    "                'BuyPrice': buy_price,\n",
    "                'Target‚Üë': target_up,\n",
    "                'Stop‚Üì': stop_down\n",
    "            })\n",
    "\n",
    "        summary_df = pd.DataFrame(summary)\n",
    "        if summary_df.empty:\n",
    "            print(f\"‚ö†Ô∏è No valid predictions generated for {date_str}\")\n",
    "            return summary_df\n",
    "\n",
    "        summary_df = summary_df.sort_values(by='PredictedReturn', ascending=False)\n",
    "\n",
    "        # === Output\n",
    "        print(f\"\\nüìä Weekly ETF Prediction Summary for {date_str}:\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "\n",
    "        output_path = os.path.join(self.record_dir, f\"{date_str}_predict_record.csv\")\n",
    "        summary_df.to_csv(output_path, index=False)\n",
    "        print(f\"üìÅ Saved prediction to: {output_path}\")\n",
    "\n",
    "        return summary_df\n",
    "\n",
    "# === Usage\n",
    "predictor = WeeklyETFPredictor(\n",
    "    model_dir=\"../model_weights\",\n",
    "    data_dir=\"../dataset/normalized_matrix\",\n",
    "    record_dir=\"../dataset/predict_record\",\n",
    "    price_path=\"../dataset/etf_prices_weekly.csv\"\n",
    ")\n",
    "predictor.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc8ee02-b643-4899-b86d-97abbe56ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Config ===\n",
    "weights_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'model_weights'))\n",
    "feature_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'normalized_matrix'))\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "# === Load feature names from any example ETF\n",
    "first_etf = etf_list[0]\n",
    "example_path = os.path.join(feature_dir, f'{first_etf}_combined.csv')\n",
    "example_features = pd.read_csv(example_path, index_col=0)\n",
    "factor_names = example_features.columns.tolist()\n",
    "\n",
    "# === Collect factor importance across ETFs\n",
    "factor_weights = {}\n",
    "\n",
    "for etf in etf_list:\n",
    "    weights = []\n",
    "    for i in range(1, 6):\n",
    "        model_path = os.path.join(weights_dir, f\"{etf}_top{i}.pt\")\n",
    "        if not os.path.exists(model_path):\n",
    "            continue\n",
    "\n",
    "        data = torch.load(model_path, map_location='cpu')\n",
    "        linear_weights = data['weights'].get('output.weight', None)\n",
    "        if linear_weights is None:\n",
    "            print(f\"‚ö†Ô∏è {etf}_top{i}.pt missing output.weight\")\n",
    "            continue\n",
    "\n",
    "        linear_weights = linear_weights.squeeze().numpy()\n",
    "        if len(linear_weights) != len(factor_names):\n",
    "            print(f\"‚ö†Ô∏è {etf}_top{i}.pt: weight dim {len(linear_weights)} != {len(factor_names)} features\")\n",
    "            continue\n",
    "\n",
    "        weights.append(linear_weights * data.get('weight', 1.0))\n",
    "\n",
    "    if weights:\n",
    "        factor_weights[etf] = sum(weights)\n",
    "\n",
    "# === Create heatmap DataFrame\n",
    "weight_df = pd.DataFrame(factor_weights, index=factor_names).T\n",
    "\n",
    "# === Normalize (optional)\n",
    "weight_df = weight_df.div(weight_df.abs().max(axis=1), axis=0)\n",
    "\n",
    "# === Plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(weight_df, cmap=\"coolwarm\", center=0, annot=False)\n",
    "plt.title(\"üéØ Factor Importance per ETF (Weighted Top 5 Models)\", fontsize=14)\n",
    "plt.xlabel(\"Factors\")\n",
    "plt.ylabel(\"ETFs\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0df0c-95c4-4958-a1a7-40177b95abd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*enable_nested_tensor.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*weights_only=False.*\")\n",
    "\n",
    "class ETFBacktester:\n",
    "    def __init__(self, model_dir, data_dir, price_path, initial_cash=2000, benchmark_symbol='SPY', seq_len=4):\n",
    "        self.model_dir = model_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.price_path = price_path\n",
    "        self.initial_cash = initial_cash\n",
    "        self.benchmark_symbol = benchmark_symbol\n",
    "        self.seq_len = seq_len\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def run(self):\n",
    "        print(\"üîÅ Loading price data...\")\n",
    "        price_df = pd.read_csv(self.price_path, index_col=0)\n",
    "        price_df.index = pd.to_datetime(price_df.index, errors='coerce')\n",
    "        price_df = price_df[price_df.index.notna()]\n",
    "        price_df = price_df.apply(pd.to_numeric, errors='coerce')\n",
    "        price_df = price_df[~price_df.index.duplicated()].sort_index()\n",
    "        print(f\"üìà Loaded price data with {len(price_df)} entries.\")\n",
    "\n",
    "        etf_list = [fname.replace('_combined.csv', '') for fname in os.listdir(self.data_dir) if fname.endswith('_combined.csv')]\n",
    "        print(f\"üîç ETF list found: {etf_list}\")\n",
    "\n",
    "        cash = self.initial_cash\n",
    "        holdings = {}\n",
    "        portfolio_values = []\n",
    "        benchmark_values = []\n",
    "        dates = []\n",
    "        trade_log = []\n",
    "        value_log = []\n",
    "        portfolio_changes = []\n",
    "\n",
    "        for i in range(self.seq_len, len(price_df) - 1):\n",
    "            date = price_df.index[i]\n",
    "            prev_dates = price_df.index[i - self.seq_len:i]\n",
    "            next_date = price_df.index[i + 1]\n",
    "            print(f\"\\nüìÖ Processing week: {date.date()} -> {next_date.date()}\")\n",
    "\n",
    "            for etf, info in list(holdings.items()):\n",
    "                if etf not in price_df.columns or next_date not in price_df.index:\n",
    "                    continue\n",
    "                try:\n",
    "                    sell_price = float(price_df.loc[next_date, etf])\n",
    "                except Exception:\n",
    "                    continue\n",
    "                shares = info['Shares']\n",
    "                buy_price = info['BuyPrice']\n",
    "                sell_value = shares * sell_price\n",
    "                actual_return = (sell_price - buy_price) / buy_price\n",
    "                predicted_return = info.get('PredictedReturn', None)\n",
    "                cash += sell_value\n",
    "                trade_log.append({\n",
    "                    'Date': next_date.strftime('%Y-%m-%d'),\n",
    "                    'ETF': etf,\n",
    "                    'Action': 'Sell',\n",
    "                    'Price': sell_price,\n",
    "                    'Shares': shares,\n",
    "                    'Value': sell_value,\n",
    "                    'BuyPrice': buy_price,\n",
    "                    'PredictedReturn': predicted_return,\n",
    "                    'ActualReturn': actual_return * 100\n",
    "                })\n",
    "            holdings.clear()\n",
    "\n",
    "            week_predictions = []\n",
    "            for etf in etf_list:\n",
    "                feat_path = os.path.join(self.data_dir, f\"{etf}_combined.csv\")\n",
    "                mask_path = os.path.join(self.data_dir, f\"{etf}_mask.csv\")\n",
    "                if not os.path.exists(feat_path) or not os.path.exists(mask_path):\n",
    "                    continue\n",
    "\n",
    "                feat_df = pd.read_csv(feat_path, index_col=0, parse_dates=True)\n",
    "                mask_df = pd.read_csv(mask_path, index_col=0, parse_dates=True)\n",
    "\n",
    "                if not all(d in feat_df.index for d in prev_dates):\n",
    "                    continue\n",
    "\n",
    "                x_seq = torch.tensor(feat_df.loc[prev_dates].values.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "                x_mask = torch.tensor(mask_df.loc[prev_dates].values.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "\n",
    "                scores, preds = [], []\n",
    "                for j in range(1, 6):\n",
    "                    model_path = os.path.join(self.model_dir, f\"{etf}_top{j}.pt\")\n",
    "                    if not os.path.exists(model_path):\n",
    "                        continue\n",
    "                    checkpoint = torch.load(model_path, map_location=self.device)\n",
    "                    model = DualTransformerModel(num_features=x_seq.shape[-1]).to(self.device)\n",
    "                    model.load_state_dict(checkpoint['weights'])\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        pred = model(x_seq, x_mask).item()\n",
    "                    preds.append(pred)\n",
    "                    scores.append(checkpoint['weight'])\n",
    "\n",
    "                if preds:\n",
    "                    pred_return = sum(p * w for p, w in zip(preds, scores))\n",
    "                    try:\n",
    "                        price = float(price_df.loc[date, etf])\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    if not np.isnan(price):\n",
    "                        week_predictions.append({\n",
    "                            'ETF': etf,\n",
    "                            'BuyPrice': price,\n",
    "                            'PredictedReturn': float(pred_return)\n",
    "                        })\n",
    "\n",
    "            week_predictions = sorted(week_predictions, key=lambda x: -x['PredictedReturn'])[:2]\n",
    "\n",
    "            budget_per_etf = cash / 2\n",
    "            for item in week_predictions:\n",
    "                etf = item['ETF']\n",
    "                price = item['BuyPrice']\n",
    "                shares = int(budget_per_etf // price)\n",
    "                cost = shares * price\n",
    "                if shares <= 0 or cost > cash:\n",
    "                    continue\n",
    "                cash -= cost\n",
    "                holdings[etf] = {'Shares': shares, 'BuyPrice': price, 'PredictedReturn': item['PredictedReturn']}\n",
    "                trade_log.append({\n",
    "                    'Date': date.strftime('%Y-%m-%d'),\n",
    "                    'ETF': etf,\n",
    "                    'Action': 'Buy',\n",
    "                    'Price': price,\n",
    "                    'Shares': shares,\n",
    "                    'Value': cost,\n",
    "                    'PredictedReturn': item['PredictedReturn']\n",
    "                })\n",
    "\n",
    "            value = cash\n",
    "            for etf, info in holdings.items():\n",
    "                try:\n",
    "                    price = float(price_df.loc[next_date, etf])\n",
    "                except Exception:\n",
    "                    price = 0\n",
    "                value += info['Shares'] * price\n",
    "\n",
    "            prev_value = portfolio_values[-1] if portfolio_values else self.initial_cash\n",
    "            change_pct = ((value - prev_value) / prev_value) * 100\n",
    "\n",
    "            portfolio_values.append(value)\n",
    "            portfolio_changes.append(change_pct)\n",
    "            dates.append(next_date.strftime('%Y-%m-%d'))\n",
    "            value_log.append({\n",
    "                'Date': next_date.strftime('%Y-%m-%d'),\n",
    "                'Value': value,\n",
    "                'Cash': cash,\n",
    "                'Change%': change_pct,\n",
    "                'Holdings': {k: dict(v) for k, v in holdings.items()}\n",
    "            })\n",
    "\n",
    "            if self.benchmark_symbol in price_df.columns:\n",
    "                base_price = price_df[self.benchmark_symbol].iloc[0]\n",
    "                current_price = price_df.loc[next_date, self.benchmark_symbol]\n",
    "                benchmark_values.append(self.initial_cash * (current_price / base_price))\n",
    "            else:\n",
    "                benchmark_values.append(value)\n",
    "\n",
    "        result_df = pd.DataFrame({\n",
    "            'Date': dates,\n",
    "            'PortfolioValue': portfolio_values,\n",
    "            'BenchmarkValue': benchmark_values,\n",
    "            'PortfolioChange%': portfolio_changes\n",
    "        })\n",
    "        result_df['Date'] = pd.to_datetime(result_df['Date'], errors='coerce')\n",
    "        result_df.set_index('Date', inplace=True)\n",
    "\n",
    "        pd.DataFrame(trade_log).to_csv(\"../dataset/backtest_trade_log.csv\", index=False)\n",
    "        pd.DataFrame(value_log).to_csv(\"../dataset/backtest_value_log.csv\", index=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(result_df['PortfolioValue'], label='Strategy Portfolio')\n",
    "        plt.plot(result_df['BenchmarkValue'], label=f'{self.benchmark_symbol} Benchmark')\n",
    "        plt.title(\"Backtest Result: Strategy vs. Benchmark\")\n",
    "        plt.ylabel(\"Portfolio Value (USD)\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        xticks_idx = result_df.index[::52]\n",
    "        xticks_labels = [d.strftime('%Y') for d in xticks_idx]\n",
    "        plt.xticks(xticks_idx, xticks_labels)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nüí∞ Final Portfolio Value: ${result_df['PortfolioValue'].iloc[-1]:,.2f}\")\n",
    "        print(f\"üìà Final Benchmark Value: ${result_df['BenchmarkValue'].iloc[-1]:,.2f}\")\n",
    "        print(f\"üéØ Strategy Return: {((result_df['PortfolioValue'].iloc[-1] / self.initial_cash - 1) * 100):.2f}%\")\n",
    "        print(f\"üìä Benchmark Return: {((result_df['BenchmarkValue'].iloc[-1] / self.initial_cash - 1) * 100):.2f}%\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "# === Execute\n",
    "backtester = ETFBacktester(\n",
    "    model_dir=\"../model_weights\",\n",
    "    data_dir=\"../dataset/normalized_matrix\",\n",
    "    price_path=\"../dataset/etf_prices_weekly.csv\",\n",
    "    initial_cash=2000,\n",
    "    benchmark_symbol=\"SPY\",\n",
    "    seq_len=4\n",
    ")\n",
    "\n",
    "result_df = backtester.run()  # ‚¨ÖÔ∏è THIS must be included!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e71f08-ed44-4637-8b03-5d26a303f1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stock_pred)",
   "language": "python",
   "name": "stock_pred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
