{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55a81306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóïÔ∏è Downloading weekly data from 2015-05-08 to 2025-04-25\n",
      "‚¨áÔ∏è Downloading XLK...\n",
      "‚¨áÔ∏è Downloading XLF...\n",
      "‚¨áÔ∏è Downloading XLV...\n",
      "‚¨áÔ∏è Downloading XLE...\n",
      "‚¨áÔ∏è Downloading XLI...\n",
      "‚¨áÔ∏è Downloading XLY...\n",
      "‚¨áÔ∏è Downloading XLP...\n",
      "‚¨áÔ∏è Downloading XLRE...\n",
      "‚¨áÔ∏è Downloading XLU...\n",
      "‚¨áÔ∏è Downloading XLB...\n",
      "‚¨áÔ∏è Downloading XLC...\n",
      "‚¨áÔ∏è Downloading SOXX...\n",
      "‚¨áÔ∏è Downloading SH...\n",
      "‚¨áÔ∏è Downloading DOG...\n",
      "‚¨áÔ∏è Downloading RWM...\n",
      "‚¨áÔ∏è Downloading ITA...\n",
      "‚¨áÔ∏è Downloading JETS...\n",
      "‚¨áÔ∏è Downloading PSQ...\n",
      "‚¨áÔ∏è Downloading VNQ...\n",
      "‚úÖ Saved: etf_prices_weekly.csv\n",
      "‚úÖ Saved: etf_volume_weekly.csv\n",
      "‚úÖ Saved: etf_high_weekly.csv\n",
      "‚úÖ Saved: etf_low_weekly.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>XLK</th>\n",
       "      <th>XLF</th>\n",
       "      <th>XLV</th>\n",
       "      <th>XLE</th>\n",
       "      <th>XLI</th>\n",
       "      <th>XLY</th>\n",
       "      <th>XLP</th>\n",
       "      <th>XLRE</th>\n",
       "      <th>XLU</th>\n",
       "      <th>XLB</th>\n",
       "      <th>XLC</th>\n",
       "      <th>SOXX</th>\n",
       "      <th>SH</th>\n",
       "      <th>DOG</th>\n",
       "      <th>RWM</th>\n",
       "      <th>ITA</th>\n",
       "      <th>JETS</th>\n",
       "      <th>PSQ</th>\n",
       "      <th>VNQ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>XLK</th>\n",
       "      <th>XLF</th>\n",
       "      <th>XLV</th>\n",
       "      <th>XLE</th>\n",
       "      <th>XLI</th>\n",
       "      <th>XLY</th>\n",
       "      <th>XLP</th>\n",
       "      <th>XLRE</th>\n",
       "      <th>XLU</th>\n",
       "      <th>XLB</th>\n",
       "      <th>XLC</th>\n",
       "      <th>SOXX</th>\n",
       "      <th>SH</th>\n",
       "      <th>DOG</th>\n",
       "      <th>RWM</th>\n",
       "      <th>ITA</th>\n",
       "      <th>JETS</th>\n",
       "      <th>PSQ</th>\n",
       "      <th>VNQ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-05-08</th>\n",
       "      <td>38.081753</td>\n",
       "      <td>16.607916</td>\n",
       "      <td>62.328011</td>\n",
       "      <td>54.583633</td>\n",
       "      <td>47.107685</td>\n",
       "      <td>68.427345</td>\n",
       "      <td>37.711845</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.769205</td>\n",
       "      <td>42.112526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.441839</td>\n",
       "      <td>153.447159</td>\n",
       "      <td>78.749893</td>\n",
       "      <td>51.727146</td>\n",
       "      <td>54.787785</td>\n",
       "      <td>23.647642</td>\n",
       "      <td>237.650345</td>\n",
       "      <td>53.803036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-15</th>\n",
       "      <td>38.329540</td>\n",
       "      <td>16.567667</td>\n",
       "      <td>63.015362</td>\n",
       "      <td>53.855953</td>\n",
       "      <td>47.548954</td>\n",
       "      <td>68.418373</td>\n",
       "      <td>38.156696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.978468</td>\n",
       "      <td>42.063534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.762249</td>\n",
       "      <td>152.713638</td>\n",
       "      <td>78.259262</td>\n",
       "      <td>51.239155</td>\n",
       "      <td>55.656307</td>\n",
       "      <td>23.714336</td>\n",
       "      <td>235.520828</td>\n",
       "      <td>54.210838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-22</th>\n",
       "      <td>38.586193</td>\n",
       "      <td>16.674988</td>\n",
       "      <td>63.626335</td>\n",
       "      <td>53.542160</td>\n",
       "      <td>47.374107</td>\n",
       "      <td>68.731056</td>\n",
       "      <td>37.773205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.158833</td>\n",
       "      <td>41.745060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.208433</td>\n",
       "      <td>152.273560</td>\n",
       "      <td>78.329353</td>\n",
       "      <td>50.925449</td>\n",
       "      <td>55.633808</td>\n",
       "      <td>22.027941</td>\n",
       "      <td>233.476562</td>\n",
       "      <td>53.515553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-29</th>\n",
       "      <td>38.382641</td>\n",
       "      <td>16.500589</td>\n",
       "      <td>63.592384</td>\n",
       "      <td>52.333801</td>\n",
       "      <td>46.466602</td>\n",
       "      <td>68.159348</td>\n",
       "      <td>37.420410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.108330</td>\n",
       "      <td>41.328590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.265490</td>\n",
       "      <td>153.520477</td>\n",
       "      <td>79.205528</td>\n",
       "      <td>51.125870</td>\n",
       "      <td>54.702297</td>\n",
       "      <td>22.008884</td>\n",
       "      <td>234.243195</td>\n",
       "      <td>52.907211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-05</th>\n",
       "      <td>38.010937</td>\n",
       "      <td>16.628036</td>\n",
       "      <td>63.083260</td>\n",
       "      <td>51.853115</td>\n",
       "      <td>46.508217</td>\n",
       "      <td>68.355865</td>\n",
       "      <td>36.500050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.823996</td>\n",
       "      <td>40.838608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.474951</td>\n",
       "      <td>154.547379</td>\n",
       "      <td>79.906441</td>\n",
       "      <td>50.472309</td>\n",
       "      <td>54.378284</td>\n",
       "      <td>22.018414</td>\n",
       "      <td>235.776413</td>\n",
       "      <td>51.616940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price             XLK        XLF        XLV        XLE        XLI        XLY  \\\n",
       "Ticker            XLK        XLF        XLV        XLE        XLI        XLY   \n",
       "Date                                                                           \n",
       "2015-05-08  38.081753  16.607916  62.328011  54.583633  47.107685  68.427345   \n",
       "2015-05-15  38.329540  16.567667  63.015362  53.855953  47.548954  68.418373   \n",
       "2015-05-22  38.586193  16.674988  63.626335  53.542160  47.374107  68.731056   \n",
       "2015-05-29  38.382641  16.500589  63.592384  52.333801  46.466602  68.159348   \n",
       "2015-06-05  38.010937  16.628036  63.083260  51.853115  46.508217  68.355865   \n",
       "\n",
       "Price             XLP XLRE        XLU        XLB XLC       SOXX          SH  \\\n",
       "Ticker            XLP XLRE        XLU        XLB XLC       SOXX          SH   \n",
       "Date                                                                          \n",
       "2015-05-08  37.711845  NaN  31.769205  42.112526 NaN  28.441839  153.447159   \n",
       "2015-05-15  38.156696  NaN  31.978468  42.063534 NaN  28.762249  152.713638   \n",
       "2015-05-22  37.773205  NaN  32.158833  41.745060 NaN  29.208433  152.273560   \n",
       "2015-05-29  37.420410  NaN  32.108330  41.328590 NaN  30.265490  153.520477   \n",
       "2015-06-05  36.500050  NaN  30.823996  40.838608 NaN  29.474951  154.547379   \n",
       "\n",
       "Price             DOG        RWM        ITA       JETS         PSQ        VNQ  \n",
       "Ticker            DOG        RWM        ITA       JETS         PSQ        VNQ  \n",
       "Date                                                                           \n",
       "2015-05-08  78.749893  51.727146  54.787785  23.647642  237.650345  53.803036  \n",
       "2015-05-15  78.259262  51.239155  55.656307  23.714336  235.520828  54.210838  \n",
       "2015-05-22  78.329353  50.925449  55.633808  22.027941  233.476562  53.515553  \n",
       "2015-05-29  79.205528  51.125870  54.702297  22.008884  234.243195  52.907211  \n",
       "2015-06-05  79.906441  50.472309  54.378284  22.018414  235.776413  51.616940  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downlaod and shift the price to correct data (firday end)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# === ETF list ===\n",
    "etf_list = [\n",
    "    'XLK', 'XLF', 'XLV', 'XLE', 'XLI', 'XLY', 'XLP', 'XLRE', 'XLU', 'XLB', 'XLC',\n",
    "    'SOXX', 'SH', 'DOG', 'RWM', 'ITA', 'JETS', 'PSQ',  'VNQ'\n",
    "]\n",
    "\n",
    "# === Date range ===\n",
    "today = datetime.today()\n",
    "start_date = (today - timedelta(weeks=52 * 10)).strftime('%Y-%m-%d')\n",
    "end_date = today.strftime('%Y-%m-%d')\n",
    "print(f\"üóïÔ∏è Downloading weekly data from {start_date} to {end_date}\")\n",
    "\n",
    "# === Output folder ===\n",
    "dataset_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset'))\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# === Containers ===\n",
    "adjclose_data, volume_data, high_data, low_data = {}, {}, {}, {}\n",
    "\n",
    "# === Download each ETF ===\n",
    "for symbol in etf_list:\n",
    "    print(f\"‚¨áÔ∏è Downloading {symbol}...\")\n",
    "    data = yf.download(\n",
    "        symbol,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        interval='1wk',\n",
    "        auto_adjust=False,\n",
    "        progress=False\n",
    "    )\n",
    "    if not data.empty:\n",
    "        data = data[~data.index.duplicated(keep='first')].sort_index()\n",
    "\n",
    "        # üîÅ Shift to next Friday\n",
    "        def shift_to_next_friday(date):\n",
    "            weekday = date.weekday()\n",
    "            days_until_friday = (4 - weekday + 7) % 7\n",
    "            return date + timedelta(days=days_until_friday)\n",
    "\n",
    "        data.index = data.index.to_series().apply(shift_to_next_friday)\n",
    "\n",
    "        # ‚ùå Filter out Fridays after today\n",
    "        data = data[data.index <= pd.Timestamp(today.date())]\n",
    "\n",
    "        # ‚úÖ Store\n",
    "        adjclose_data[symbol] = data[['Adj Close']].rename(columns={'Adj Close': symbol})\n",
    "        volume_data[symbol] = data[['Volume']].rename(columns={'Volume': symbol})\n",
    "        high_data[symbol] = data[['High']].rename(columns={'High': symbol})\n",
    "        low_data[symbol] = data[['Low']].rename(columns={'Low': symbol})\n",
    "\n",
    "# === Merge and Save ===\n",
    "def combine_and_save(data_dict, filename):\n",
    "    df = pd.concat(data_dict.values(), axis=1)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "    df = df[~df.index.duplicated(keep='first')].sort_index()\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    path = os.path.join(dataset_path, filename)\n",
    "    df.to_csv(path)\n",
    "    print(f\"‚úÖ Saved: {filename}\")\n",
    "    return df\n",
    "\n",
    "# === Save all ===\n",
    "price_df = combine_and_save(adjclose_data, 'etf_prices_weekly.csv')\n",
    "volume_df = combine_and_save(volume_data, 'etf_volume_weekly.csv')\n",
    "high_df = combine_and_save(high_data, 'etf_high_weekly.csv')\n",
    "low_df = combine_and_save(low_data, 'etf_low_weekly.csv')\n",
    "\n",
    "# === Preview ===\n",
    "price_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ecccfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Downloading macro data from 2015-05-08 to 2025-04-25\n",
      "‚¨áÔ∏è Downloading VIX (^VIX)...\n",
      "‚¨áÔ∏è Downloading 10Y_Yield (^TNX)...\n",
      "‚¨áÔ∏è Downloading 2Y_Yield (^IRX)...\n",
      "‚¨áÔ∏è Downloading USD_Index (DX-Y.NYB)...\n",
      "‚¨áÔ∏è Downloading WTI_Crude (CL=F)...\n",
      "‚¨áÔ∏è Downloading SPY_Price (SPY)...\n",
      "‚¨áÔ∏è Downloading QQQ_Price (QQQ)...\n",
      "‚úÖ Macro indicators saved to: D:\\CodingWorks\\Weekly_Swing_TransformerQT\\dataset\\macro_indicators_weekly.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>VIX</th>\n",
       "      <th>10Y_Yield</th>\n",
       "      <th>2Y_Yield</th>\n",
       "      <th>USD_Index</th>\n",
       "      <th>WTI_Crude</th>\n",
       "      <th>SPY_Price</th>\n",
       "      <th>QQQ_Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>^VIX</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>^IRX</th>\n",
       "      <th>DX-Y.NYB</th>\n",
       "      <th>CL=F</th>\n",
       "      <th>SPY</th>\n",
       "      <th>QQQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-05-08</th>\n",
       "      <td>12.86</td>\n",
       "      <td>0.2150</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>94.790001</td>\n",
       "      <td>59.389999</td>\n",
       "      <td>211.619995</td>\n",
       "      <td>108.690002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-15</th>\n",
       "      <td>12.38</td>\n",
       "      <td>0.2141</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>93.139999</td>\n",
       "      <td>59.689999</td>\n",
       "      <td>212.440002</td>\n",
       "      <td>109.580002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-22</th>\n",
       "      <td>12.13</td>\n",
       "      <td>0.2215</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>96.010002</td>\n",
       "      <td>59.720001</td>\n",
       "      <td>212.990005</td>\n",
       "      <td>110.470001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-29</th>\n",
       "      <td>13.84</td>\n",
       "      <td>0.2095</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>96.910004</td>\n",
       "      <td>60.299999</td>\n",
       "      <td>211.139999</td>\n",
       "      <td>110.050003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-05</th>\n",
       "      <td>14.21</td>\n",
       "      <td>0.2402</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>96.309998</td>\n",
       "      <td>59.130001</td>\n",
       "      <td>209.770004</td>\n",
       "      <td>109.300003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price         VIX 10Y_Yield 2Y_Yield  USD_Index  WTI_Crude   SPY_Price  \\\n",
       "Ticker       ^VIX      ^TNX     ^IRX   DX-Y.NYB       CL=F         SPY   \n",
       "2015-05-08  12.86    0.2150  0.00003  94.790001  59.389999  211.619995   \n",
       "2015-05-15  12.38    0.2141  0.00010  93.139999  59.689999  212.440002   \n",
       "2015-05-22  12.13    0.2215  0.00010  96.010002  59.720001  212.990005   \n",
       "2015-05-29  13.84    0.2095  0.00003  96.910004  60.299999  211.139999   \n",
       "2015-06-05  14.21    0.2402  0.00010  96.309998  59.130001  209.770004   \n",
       "\n",
       "Price        QQQ_Price  \n",
       "Ticker             QQQ  \n",
       "2015-05-08  108.690002  \n",
       "2015-05-15  109.580002  \n",
       "2015-05-22  110.470001  \n",
       "2015-05-29  110.050003  \n",
       "2015-06-05  109.300003  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save all the micro indicators\n",
    "\n",
    "import os \n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Macro indicator tickers on Yahoo Finance\n",
    "macro_tickers = {\n",
    "    'VIX': '^VIX',               # Volatility Index\n",
    "    '10Y_Yield': '^TNX',         # 10-Year Treasury Yield (multiply by 0.1)\n",
    "    '2Y_Yield': '^IRX',          # 2-Year Treasury Yield (multiply by 0.01)\n",
    "    'USD_Index': 'DX-Y.NYB',     # U.S. Dollar Index (ICE Dollar Index)\n",
    "    'WTI_Crude': 'CL=F',         # Crude Oil (WTI Futures)\n",
    "    'SPY_Price': 'SPY',          # S&P 500 ETF price\n",
    "    'QQQ_Price': 'QQQ'           # NASDAQ 100 ETF price\n",
    "}\n",
    "\n",
    "# Align with ETF backtest window using back_time\n",
    "today = datetime.today()\n",
    "back_time = today - timedelta(weeks=52 * 10)\n",
    "start_date = back_time.strftime('%Y-%m-%d')\n",
    "end_date = today.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"\\U0001F4CA Downloading macro data from {start_date} to {end_date}\")\n",
    "\n",
    "# Download weekly data\n",
    "macro_data = {}\n",
    "for name, ticker in macro_tickers.items():\n",
    "    print(f\"‚¨áÔ∏è Downloading {name} ({ticker})...\")\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, interval='1wk', auto_adjust=False, progress=False)\n",
    "    if not data.empty:\n",
    "        data = data[~data.index.duplicated(keep='first')].sort_index()\n",
    "\n",
    "        # Shift index to the next Friday\n",
    "        def shift_to_next_friday(date):\n",
    "            days_ahead = (4 - date.weekday() + 7) % 7\n",
    "            return date + timedelta(days=days_ahead)\n",
    "\n",
    "        data.index = pd.to_datetime([shift_to_next_friday(d) for d in data.index])\n",
    "\n",
    "        # Filter out dates beyond today\n",
    "        data = data[data.index <= pd.Timestamp(today.date())]\n",
    "\n",
    "        macro_data[name] = data[['Close']].rename(columns={'Close': name})\n",
    "\n",
    "# Combine all macro indicators into one DataFrame\n",
    "macro_df = pd.concat(macro_data.values(), axis=1)\n",
    "\n",
    "# Fix yield scale\n",
    "if '10Y_Yield' in macro_df.columns:\n",
    "    macro_df['10Y_Yield'] *= 0.1\n",
    "if '2Y_Yield' in macro_df.columns:\n",
    "    macro_df['2Y_Yield'] *= 0.01\n",
    "\n",
    "# Clean and format\n",
    "macro_df = macro_df.apply(pd.to_numeric, errors='coerce')\n",
    "macro_df.index = pd.to_datetime(macro_df.index)\n",
    "macro_df = macro_df[~macro_df.index.duplicated(keep='first')]\n",
    "macro_df.sort_index(inplace=True)\n",
    "macro_df.dropna(inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "macro_save_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'macro_indicators_weekly.csv'))\n",
    "macro_df.to_csv(macro_save_path)\n",
    "print(f\"‚úÖ Macro indicators saved to: {macro_save_path}\")\n",
    "\n",
    "# Preview\n",
    "macro_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e27fa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded ETF Prices with 19 tickers\n",
      "‚úÖ Loaded ETF Volume with 19 tickers\n",
      "üìà XLK: 521 valid rows\n",
      "üìà XLF: 521 valid rows\n",
      "üìà XLV: 521 valid rows\n",
      "üìà XLE: 521 valid rows\n",
      "üìà XLI: 521 valid rows\n",
      "üìà XLY: 521 valid rows\n",
      "üìà XLP: 521 valid rows\n",
      "üìà XLRE: 508 valid rows\n",
      "üìà XLU: 521 valid rows\n",
      "üìà XLB: 521 valid rows\n",
      "üìà XLC: 508 valid rows\n",
      "üìà SOXX: 521 valid rows\n",
      "üìà SH: 521 valid rows\n",
      "üìà DOG: 521 valid rows\n",
      "üìà RWM: 521 valid rows\n",
      "üìà ITA: 521 valid rows\n",
      "üìà JETS: 521 valid rows\n",
      "üìà PSQ: 521 valid rows\n",
      "üìà VNQ: 521 valid rows\n",
      "üìÄ Final feature shape: (521, 247)\n",
      "‚úÖ Saved features to: weekly_etf_tech_feature.csv\n"
     ]
    }
   ],
   "source": [
    "# calculate TA signals\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from ta import momentum, trend, volume\n",
    "\n",
    "# === Paths ===\n",
    "price_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'etf_prices_weekly.csv'))\n",
    "volume_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'etf_volume_weekly.csv'))\n",
    "\n",
    "# === Helper to load ETF CSVs ===\n",
    "def load_etf_csv(path, name='[unknown]'):\n",
    "    try:\n",
    "        header_row = pd.read_csv(path, header=None, nrows=2)\n",
    "        columns = header_row.iloc[1].tolist()[1:]\n",
    "        df = pd.read_csv(path, skiprows=3, header=None)\n",
    "        df = df.iloc[:, :len(columns) + 1]\n",
    "        df.columns = ['Date'] + columns\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        df = df.set_index('Date')\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "        print(f\"‚úÖ Loaded {name} with {len(columns)} tickers\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {name}: {e}\")\n",
    "        raise\n",
    "\n",
    "# === Load data ===\n",
    "price_df = load_etf_csv(price_path, name='ETF Prices')\n",
    "volume_df = load_etf_csv(volume_path, name='ETF Volume')\n",
    "\n",
    "# === Feature storage ===\n",
    "features_all = []\n",
    "skipped = []\n",
    "\n",
    "# === Feature generation loop ===\n",
    "for symbol in price_df.columns:\n",
    "    if symbol not in volume_df.columns:\n",
    "        print(f\"‚ö†Ô∏è Skipping {symbol}: volume data missing.\")\n",
    "        skipped.append(symbol)\n",
    "        continue\n",
    "\n",
    "    df = pd.DataFrame(index=price_df.index)\n",
    "    df['close'] = price_df[symbol]\n",
    "    df['volume'] = volume_df[symbol]\n",
    "\n",
    "    try:\n",
    "        # === Return-based technical indicators ===\n",
    "        df[f'{symbol}_ret_1w'] = df['close'].pct_change(1)\n",
    "        df[f'{symbol}_ret_3w'] = df['close'].pct_change(3)\n",
    "        df[f'{symbol}_ret_6w'] = df['close'].pct_change(6)\n",
    "\n",
    "        high = df['close'].rolling(window=14).max()\n",
    "        low = df['close'].rolling(window=14).min()\n",
    "        df[f'{symbol}_stoch_k'] = 100 * (df['close'] - low) / (high - low)\n",
    "        df[f'{symbol}_stoch_d'] = df[f'{symbol}_stoch_k'].rolling(window=3).mean()\n",
    "        df[f'{symbol}_williams_r'] = -100 * (high - df['close']) / (high - low)\n",
    "\n",
    "        df[f'{symbol}_cci'] = trend.cci(high=df['close'], low=df['close'], close=df['close'], window=20)\n",
    "        df[f'{symbol}_rsi'] = momentum.rsi(df['close'], window=14)\n",
    "        df[f'{symbol}_obv'] = volume.on_balance_volume(df['close'], df['volume'])\n",
    "\n",
    "        df[f'{symbol}_macd'] = trend.macd(df['close'])\n",
    "        df[f'{symbol}_macd_signal'] = trend.macd_signal(df['close'])\n",
    "        df[f'{symbol}_macd_diff'] = trend.macd_diff(df['close'])\n",
    "\n",
    "        # === Short-term KST (custom) ===\n",
    "        roc1 = df['close'].pct_change(10)\n",
    "        roc2 = df['close'].pct_change(15)\n",
    "        roc3 = df['close'].pct_change(20)\n",
    "        roc4 = df['close'].pct_change(30)\n",
    "        df[f'{symbol}_kst_short'] = (\n",
    "            roc1.rolling(10).mean() +\n",
    "            roc2.rolling(10).mean() * 2 +\n",
    "            roc3.rolling(10).mean() * 3 +\n",
    "            roc4.rolling(15).mean() * 4\n",
    "        )\n",
    "\n",
    "        derived_cols = df.columns.difference(['close', 'volume'])\n",
    "        feature_df = df[derived_cols].copy()\n",
    "\n",
    "        features_all.append(feature_df)\n",
    "        print(f\"üìà {symbol}: {feature_df.dropna(how='all').shape[0]} valid rows\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {symbol}: {e}\")\n",
    "        skipped.append(symbol)\n",
    "\n",
    "# === Final merge ===\n",
    "if len(features_all) == 0:\n",
    "    raise ValueError(\"üõë No valid ETF features generated.\")\n",
    "\n",
    "features_df = pd.concat(features_all, axis=1).sort_index()\n",
    "features_df = features_df[~features_df.index.duplicated(keep='first')]\n",
    "\n",
    "print(\"üìÄ Final feature shape:\", features_df.shape)\n",
    "\n",
    "# === Save to CSV ===\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset'))\n",
    "features_df.to_csv(os.path.join(base_dir, 'weekly_etf_tech_feature.csv'))\n",
    "print(f\"‚úÖ Saved features to: weekly_etf_tech_feature.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97b6cb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_38148\\956442140.py:16: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  macro_df = pd.read_csv(macro_path, index_col=0, parse_dates=True)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_38148\\956442140.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  price_df = pd.read_csv(price_path, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßæ Tickers detected: ['DOG', 'ITA', 'JETS', 'PSQ', 'RWM', 'SH', 'SOXX', 'VNQ', 'XLB', 'XLC', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLRE', 'XLU', 'XLV', 'XLY']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 13)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m macro_cols \u001b[38;5;241m=\u001b[39m macro_df\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m     66\u001b[0m norm_cols \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdifference(macro_cols\u001b[38;5;241m.\u001b[39munion({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m}))\n\u001b[1;32m---> 67\u001b[0m df[norm_cols] \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnorm_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m df[macro_cols] \u001b[38;5;241m=\u001b[39m df[macro_cols]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0.0\u001b[39m)  \u001b[38;5;66;03m# keep macro as is\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Save outputs\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock_pred\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock_pred\\lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    904\u001b[0m             (\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m         )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock_pred\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock_pred\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock_pred\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:930\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \n\u001b[0;32m    900\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    929\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 930\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    938\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock_pred\\lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock_pred\\lib\\site-packages\\sklearn\\utils\\validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1133\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1134\u001b[0m         )\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1137\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 13)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "# normalizing matrix add masks\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Paths ===\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset'))\n",
    "feat_path = os.path.join(base_dir, 'weekly_etf_tech_feature.csv')\n",
    "macro_path = os.path.join(base_dir, 'macro_indicators_weekly.csv')\n",
    "price_path = os.path.join(base_dir, 'etf_prices_weekly.csv')\n",
    "\n",
    "# === Load Data (with correct offset logic) ===\n",
    "feat_df = pd.read_csv(feat_path, index_col=0, skiprows=1, parse_dates=True)\n",
    "macro_df = pd.read_csv(macro_path, index_col=0, skiprows=2, parse_dates=True)\n",
    "price_df = pd.read_csv(price_path, index_col=0, skiprows=3, parse_dates=True)\n",
    "\n",
    "# Align index\n",
    "feat_df.index = pd.to_datetime(feat_df.index)\n",
    "macro_df.index = pd.to_datetime(macro_df.index)\n",
    "price_df.index = pd.to_datetime(price_df.index)\n",
    "\n",
    "# Drop duplicate dates if any\n",
    "feat_df = feat_df[~feat_df.index.duplicated()]\n",
    "macro_df = macro_df[~macro_df.index.duplicated()]\n",
    "price_df = price_df[~price_df.index.duplicated()]\n",
    "\n",
    "# === Output Directory ===\n",
    "norm_dir = os.path.join(base_dir, 'normalized_matrix')\n",
    "os.makedirs(norm_dir, exist_ok=True)\n",
    "\n",
    "# === Scaler\n",
    "scaler = StandardScaler()\n",
    "tickers = sorted(set(col.split('_')[0] for col in feat_df.columns if '_' in col))\n",
    "\n",
    "print(f\"üßæ Tickers detected: {tickers}\")\n",
    "\n",
    "for ticker in tickers:\n",
    "    feat_cols = [c for c in feat_df.columns if c.startswith(f'{ticker}_')]\n",
    "    if len(feat_cols) == 0 or ticker not in price_df.columns:\n",
    "        continue\n",
    "\n",
    "    df = feat_df[feat_cols].copy()\n",
    "    df.columns = [c.replace(f\"{ticker}_\", \"\") for c in df.columns]\n",
    "\n",
    "    # Add raw price\n",
    "    df.insert(0, 'price', price_df[ticker].reindex(df.index))\n",
    "\n",
    "    # Join macro\n",
    "    df = df.join(macro_df, how='left')\n",
    "\n",
    "    # Mask before filling NA\n",
    "    mask = df.isna().astype(float)\n",
    "\n",
    "    # Special mask: inverse ETFs\n",
    "    if ticker == 'PSQ':\n",
    "        qqq_cols = [c for c in df.columns if 'QQQ' in c]\n",
    "        df[qqq_cols] = 0.0\n",
    "        mask[qqq_cols] = 1.0\n",
    "    if ticker == 'SH':\n",
    "        spy_cols = [c for c in df.columns if 'SPY' in c]\n",
    "        df[spy_cols] = 0.0\n",
    "        mask[spy_cols] = 1.0\n",
    "\n",
    "    # Normalize technical features\n",
    "    macro_cols = macro_df.columns\n",
    "    norm_cols = df.columns.difference(macro_cols.union({'price'}))\n",
    "    if len(df[norm_cols]) == 0 or df[norm_cols].dropna().empty:\n",
    "        print(f\"‚ö†Ô∏è Skipped {ticker}: no valid data to normalize.\")\n",
    "        continue\n",
    "\n",
    "    df[norm_cols] = scaler.fit_transform(df[norm_cols].fillna(0.0))\n",
    "    df[macro_cols] = df[macro_cols].fillna(0.0)\n",
    "\n",
    "    df.to_csv(os.path.join(norm_dir, f'{ticker}_combined.csv'))\n",
    "    mask.to_csv(os.path.join(norm_dir, f'{ticker}_mask.csv'))\n",
    "    print(f\"‚úÖ Saved {ticker}: {df.shape[0]} rows, {df.shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e3617",
   "metadata": {},
   "source": [
    "# start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1565c35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training XLK\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "‚ùå Target column XLK_price_change not found in D:\\CodingWorks\\Weekly_Swing_TransformerQT\\dataset\\normalized_matrix\\XLK_combined.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 157\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# === Execute ===\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m etf \u001b[38;5;129;01min\u001b[39;00m etf_list:\n\u001b[1;32m--> 157\u001b[0m     \u001b[43mtrain_one_etf\u001b[49m\u001b[43m(\u001b[49m\u001b[43metf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 93\u001b[0m, in \u001b[0;36mtrain_one_etf\u001b[1;34m(etf)\u001b[0m\n\u001b[0;32m     91\u001b[0m feat_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00metf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_combined.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m mask_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00metf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mask.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mETFDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprice_change\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     96\u001b[0m X, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(loader))\n",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m, in \u001b[0;36mETFDataset.__init__\u001b[1;34m(self, combined_csv, mask_csv, etf, return_feature, seq_len)\u001b[0m\n\u001b[0;32m     19\u001b[0m target_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00metf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_feature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå Target column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined_csv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìå Predicting: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(target_col)\n",
      "\u001b[1;31mValueError\u001b[0m: ‚ùå Target column XLK_price_change not found in D:\\CodingWorks\\Weekly_Swing_TransformerQT\\dataset\\normalized_matrix\\XLK_combined.csv"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'normalized_matrix'))\n",
    "\n",
    "# === Dataset ===\n",
    "class ETFDataset(Dataset):\n",
    "    def __init__(self, combined_csv, mask_csv, etf, return_feature=\"price_change\", seq_len=4):\n",
    "        self.df = pd.read_csv(combined_csv, index_col=0)\n",
    "        self.mask = pd.read_csv(mask_csv, index_col=0)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Extract the correct target column\n",
    "        target_col = f\"{etf}_{return_feature}\"\n",
    "        if target_col not in self.df.columns:\n",
    "            raise ValueError(f\"‚ùå Target column {target_col} not found in {combined_csv}\")\n",
    "\n",
    "        print(f\"üìå Predicting: {target_col}\")\n",
    "\n",
    "        self.target_index = self.df.columns.get_loc(target_col)\n",
    "        self.raw_X = self.df.values.astype(np.float32)\n",
    "        self.raw_M = self.mask.values.astype(np.float32)\n",
    "\n",
    "        self.X, self.M, self.y = [], [], []\n",
    "        for i in range(seq_len, len(self.raw_X)):\n",
    "            self.X.append(self.raw_X[i-seq_len:i])\n",
    "            self.M.append(self.raw_M[i-seq_len:i])\n",
    "            self.y.append(self.raw_X[i, self.target_index])\n",
    "\n",
    "        self.X = np.stack(self.X)\n",
    "        self.M = np.stack(self.M)\n",
    "        self.y = np.array(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.X[idx]),\n",
    "            torch.tensor(self.M[idx]),\n",
    "            torch.tensor(self.y[idx])\n",
    "        )\n",
    "\n",
    "\n",
    "# === Model with Attention Pooling ===\n",
    "class DualTransformerModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=128, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Choose the highest possible nhead divisor of num_features\n",
    "        possible_heads = [h for h in [8, 4, 2, 1] if num_features % h == 0]\n",
    "        nhead = possible_heads[0]\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=num_features,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=hidden_dim,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.attn_query = nn.Parameter(torch.randn(1, 1, num_features))\n",
    "        self.output = nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x_masked = x * (1 - mask)\n",
    "        enc = self.encoder(x_masked)\n",
    "\n",
    "        # Apply learnable attention pooling\n",
    "        attn_scores = torch.matmul(self.attn_query, enc.transpose(1, 2))  # (1, 1, F) x (B, F, T) = (B, 1, T)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)  # (B, 1, T)\n",
    "        pooled = torch.bmm(attn_weights, enc).squeeze(1)  # (B, 1, T) x (B, T, F) = (B, 1, F) ‚Üí (B, F)\n",
    "\n",
    "        return self.output(pooled).squeeze(-1)\n",
    "\n",
    "# === Training Params ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 0.001\n",
    "max_epochs = 3000\n",
    "seq_len = 4\n",
    "\n",
    "# === Training Function ===\n",
    "def train_one_etf(etf):\n",
    "    print(f\"\\nüöÄ Training {etf}\")\n",
    "    feat_path = os.path.join(data_dir, f\"{etf}_combined.csv\")\n",
    "    mask_path = os.path.join(data_dir, f\"{etf}_mask.csv\")\n",
    "    dataset = ETFDataset(feat_path, mask_path, etf=etf, return_feature=\"price_change\", seq_len=seq_len)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    X, _, _ = next(iter(loader))\n",
    "    model = DualTransformerModel(num_features=X.shape[-1]).to(device)\n",
    "    model.device = device\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    mae_loss = nn.L1Loss()\n",
    "\n",
    "    best_models = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss, total_mae, total_win = 0, 0, 0\n",
    "\n",
    "        for X, M, y in loader:\n",
    "            X, M, y = X.to(model.device), M.to(model.device), y.to(model.device)\n",
    "            pred = model(X, M)\n",
    "\n",
    "            mse = mse_loss(pred, y)\n",
    "            mae = mae_loss(pred, y)\n",
    "            direction_loss = 1 - (torch.sign(pred) == torch.sign(y)).float().mean()\n",
    "            loss = 0.4*mse + 0.25 * mae + 0.35 * direction_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_mae += mae.item()\n",
    "            total_win += (torch.sign(pred) == torch.sign(y)).float().mean().item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        avg_mae = total_mae / len(loader)\n",
    "        avg_win = total_win / len(loader)\n",
    "        score = avg_win * 0.6 + (1 - avg_mae) * 0.4\n",
    "        best_models.append((score, model.state_dict(), avg_win, avg_mae))\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, MAE={avg_mae:.4f}, WinRate={avg_win:.4f}\")\n",
    "\n",
    "    # Save top 5 models\n",
    "    top5 = sorted(best_models, key=lambda x: -x[0])[:5]\n",
    "    weight_sum = sum(x[0] for x in top5)\n",
    "\n",
    "    # Ensure model_weights folder exists\n",
    "    save_dir = os.path.join(\"..\", \"model_weights\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for i, (score, weights, win, mae) in enumerate(top5):\n",
    "        out = {\n",
    "            'weights': weights,\n",
    "            'score': score,\n",
    "            'win_rate': win,\n",
    "            'mae': mae,\n",
    "            'weight': score / weight_sum\n",
    "        }\n",
    "        save_path = os.path.join(save_dir, f\"{etf}_top{i+1}.pt\")\n",
    "        torch.save(out, save_path)\n",
    "\n",
    "# === Execute ===\n",
    "for etf in etf_list:\n",
    "    train_one_etf(etf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb75d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "from datetime import timedelta\n",
    "\n",
    "# === Paths ===\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'normalized_matrix'))\n",
    "model_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'model_weights'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Gather predictions\n",
    "plot_data = []\n",
    "\n",
    "for etf in etf_list:\n",
    "    try:\n",
    "        feat_path = os.path.join(data_dir, f\"{etf}_combined.csv\")\n",
    "        mask_path = os.path.join(data_dir, f\"{etf}_mask.csv\")\n",
    "        model_path = os.path.join(model_dir, f\"{etf}_top1.pt\")\n",
    "\n",
    "        df_feat = pd.read_csv(feat_path, index_col=0).astype('float32')\n",
    "        df_mask = pd.read_csv(mask_path, index_col=0).astype('float32')\n",
    "\n",
    "        if len(df_feat) < 5:\n",
    "            continue\n",
    "\n",
    "        X_seq = []\n",
    "        M_seq = []\n",
    "        y_seq = []\n",
    "        price_seq = df_feat['price_change'].copy()\n",
    "\n",
    "        for i in range(4, len(df_feat)):\n",
    "            X_seq.append(df_feat.iloc[i-4:i].values)\n",
    "            M_seq.append(df_mask.iloc[i-4:i].values)\n",
    "            return_col = \"price_change\"\n",
    "            target_index = df_feat.columns.get_loc(return_col)\n",
    "            y_seq.append(df_feat.iloc[i, target_index])\n",
    "\n",
    "        X_seq = torch.tensor(np.array(X_seq), dtype=torch.float32).to(device)\n",
    "        M_seq = torch.tensor(np.array(M_seq), dtype=torch.float32).to(device)\n",
    "        y_seq = np.array(y_seq)\n",
    "\n",
    "        model = DualTransformerModel(num_features=X_seq.shape[-1]).to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device)['weights'])\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(X_seq, M_seq).cpu().numpy()\n",
    "\n",
    "        date_index = pd.to_datetime(df_feat.index[4:])\n",
    "        price_base = df_feat['price_change'].shift(1).iloc[4:].reset_index(drop=True)\n",
    "        actual_prices = (1 + y_seq).cumprod() * 100\n",
    "        predicted_prices = (1 + pred).cumprod() * 100\n",
    "\n",
    "\n",
    "        df_plot = pd.DataFrame({\n",
    "            'Date': date_index,\n",
    "            'Predicted': predicted_prices,\n",
    "            'Actual': actual_prices\n",
    "        }).set_index('Date')\n",
    "        plot_data.append((etf, df_plot.iloc[-50:]))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading/predicting {etf}: {e}\")\n",
    "\n",
    "# === Plotting\n",
    "fig = plt.figure(figsize=(20, 4 * ((len(plot_data) + 3) // 4)))\n",
    "gs = gridspec.GridSpec((len(plot_data) + 3) // 4, 4, figure=fig)\n",
    "\n",
    "for i, (etf, df) in enumerate(plot_data):\n",
    "    ax = fig.add_subplot(gs[i])\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    ax.plot(df.index, df['Actual'], label='Actual (from price)', color='blue')\n",
    "    ax.plot(df.index, df['Predicted'], label='Predicted (simulated price)', color='red')\n",
    "    ax.set_title(etf)\n",
    "    ax.legend()\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    for j, (pred, actual) in enumerate(zip(df['Predicted'], df['Actual'])):\n",
    "        color = 'lightgreen' if np.sign(pred - actual) == 0 else 'lightgrey'\n",
    "        start = df.index[j]\n",
    "        end = start + timedelta(days=7)\n",
    "        ax.axvspan(start, end, color=color, alpha=0.2)\n",
    "\n",
    "fig.suptitle(\"üìà ETF Price Trajectory Simulated from Prediction vs. Actual\", fontsize=18)\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# === Prediction Class ===\n",
    "class WeeklyETFPredictor:\n",
    "    def __init__(self, model_dir, data_dir, record_dir, price_path, device=None):\n",
    "        self.model_dir = model_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.record_dir = record_dir\n",
    "        self.price_path = price_path\n",
    "        os.makedirs(self.record_dir, exist_ok=True)\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def predict(self):\n",
    "        # Get the nearest past Monday ‚Äî this represents the start of the prediction week\n",
    "        today = datetime.today()\n",
    "        days_back = today.weekday() % 7\n",
    "        monday = today - timedelta(days=days_back)\n",
    "        monday_dt = pd.to_datetime(monday.strftime(\"%Y-%m-%d\"))\n",
    "        date_str = monday_dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Load ETF price data for reference\n",
    "        price_df = pd.read_csv(\n",
    "            self.price_path,\n",
    "            index_col=0,\n",
    "            parse_dates=True,\n",
    "            date_parser=lambda x: pd.to_datetime(x, format=\"%Y-%m-%d\", errors='coerce')\n",
    "        )\n",
    "        price_df = price_df[price_df.index.notna()]\n",
    "        price_df = price_df[~price_df.index.duplicated()].sort_index()\n",
    "\n",
    "        print(f\"üîç Checking last available price dates: {price_df.index[-5:].to_list()}\")\n",
    "        print(f\"üìÖ Using feature/price data for week starting on: {monday_dt.date()}\")\n",
    "\n",
    "        summary = []\n",
    "\n",
    "        for fname in os.listdir(self.data_dir):\n",
    "            if not fname.endswith(\"_combined.csv\"):\n",
    "                continue\n",
    "\n",
    "            etf = fname.replace(\"_combined.csv\", \"\")\n",
    "            feat_path = os.path.join(self.data_dir, f\"{etf}_combined.csv\")\n",
    "            mask_path = os.path.join(self.data_dir, f\"{etf}_mask.csv\")\n",
    "\n",
    "            try:\n",
    "                df_feat = pd.read_csv(feat_path, index_col=0, parse_dates=True)\n",
    "                df_mask = pd.read_csv(mask_path, index_col=0, parse_dates=True)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading data for {etf}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if monday_dt not in df_feat.index:\n",
    "                print(f\"‚ö†Ô∏è No feature data for {etf} on {monday_dt.date()}\")\n",
    "                continue\n",
    "\n",
    "            if etf not in price_df.columns or monday_dt not in price_df.index:\n",
    "                print(f\"‚ö†Ô∏è Price info missing for {etf} on {monday_dt.date()}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Collect last 4 weeks (or pad if at beginning)\n",
    "                idx_pos = df_feat.index.get_loc(monday_dt)\n",
    "                if idx_pos < 3:\n",
    "                    # Pad beginning if not enough data\n",
    "                    x_real = df_feat.iloc[[0]* (4 - idx_pos) + list(range(idx_pos + 1))].values\n",
    "                    x_mask = df_mask.iloc[[0]* (4 - idx_pos) + list(range(idx_pos + 1))].values\n",
    "                else:\n",
    "                    x_real = df_feat.iloc[idx_pos-3:idx_pos+1].values\n",
    "                    x_mask = df_mask.iloc[idx_pos-3:idx_pos+1].values\n",
    "                \n",
    "                # Final tensor format: [1, 4, num_features]\n",
    "                x_real = torch.tensor(x_real.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "                x_mask = torch.tensor(x_mask.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "\n",
    "                price = float(price_df.loc[monday_dt, etf])\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Tensor prep or price parse failed for {etf}: {e}\")\n",
    "                continue\n",
    "\n",
    "            scores, preds, maes, winrates = [], [], [], []\n",
    "            for i in range(1, 6):\n",
    "                path = os.path.join(self.model_dir, f\"{etf}_top{i}.pt\")\n",
    "                if not os.path.exists(path): continue\n",
    "\n",
    "                try:\n",
    "                    checkpoint = torch.load(path, map_location=self.device)\n",
    "                    model = DualTransformerModel(num_features=x_real.shape[-1]).to(self.device)\n",
    "                    model.load_state_dict(checkpoint['weights'])\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        pred = model(x_real, x_mask).item()\n",
    "                    preds.append(pred)\n",
    "                    maes.append(checkpoint['mae'])\n",
    "                    winrates.append(checkpoint['win_rate'])\n",
    "                    scores.append(checkpoint['weight'])\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Model predict error for {etf} top{i}: {e}\")\n",
    "\n",
    "            if len(preds) == 0:\n",
    "                continue\n",
    "\n",
    "            pred_return = sum(p * w for p, w in zip(preds, scores))\n",
    "            avg_mae = sum(maes) / len(maes)\n",
    "            avg_win = sum(winrates) / len(winrates)\n",
    "\n",
    "            if pred_return >= 0:\n",
    "                target_up = round(price * (1 + pred_return / 100 + pred_return * avg_mae / 100), 2)\n",
    "                stop_down = round(price * (1 - pred_return * avg_mae / 100), 2)\n",
    "                buy_price = round(price, 2)\n",
    "            else:\n",
    "                target_up = stop_down = buy_price = \"X\"\n",
    "\n",
    "            summary.append({\n",
    "                'ETF': etf,\n",
    "                'PredictedReturn': round(pred_return, 4),\n",
    "                'MAE': round(avg_mae, 4),\n",
    "                'WinRate': round(avg_win, 4),\n",
    "                'BuyPrice': buy_price,\n",
    "                'Target‚Üë': target_up,\n",
    "                'Stop‚Üì': stop_down\n",
    "            })\n",
    "\n",
    "        summary_df = pd.DataFrame(summary)\n",
    "        if summary_df.empty:\n",
    "            print(f\"‚ö†Ô∏è No valid predictions generated for {date_str}\")\n",
    "            return summary_df\n",
    "\n",
    "        summary_df = summary_df.sort_values(by='PredictedReturn', ascending=False)\n",
    "\n",
    "        # === Output\n",
    "        print(f\"\\nüìä Weekly ETF Prediction Summary for {date_str}:\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "\n",
    "        output_path = os.path.join(self.record_dir, f\"{date_str}_predict_record.csv\")\n",
    "        summary_df.to_csv(output_path, index=False)\n",
    "        print(f\"üìÅ Saved prediction to: {output_path}\")\n",
    "\n",
    "        return summary_df\n",
    "\n",
    "# === Usage\n",
    "predictor = WeeklyETFPredictor(\n",
    "    model_dir=\"../model_weights\",\n",
    "    data_dir=\"../dataset/normalized_matrix\",\n",
    "    record_dir=\"../dataset/predict_record\",\n",
    "    price_path=\"../dataset/etf_prices_weekly.csv\"\n",
    ")\n",
    "predictor.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc8ee02-b643-4899-b86d-97abbe56ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Config ===\n",
    "weights_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'model_weights'))\n",
    "feature_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'normalized_matrix'))\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "# === Load feature names from any example ETF\n",
    "first_etf = etf_list[0]\n",
    "example_path = os.path.join(feature_dir, f'{first_etf}_combined.csv')\n",
    "example_features = pd.read_csv(example_path, index_col=0)\n",
    "factor_names = example_features.columns.tolist()\n",
    "\n",
    "# === Collect factor importance across ETFs\n",
    "factor_weights = {}\n",
    "\n",
    "for etf in etf_list:\n",
    "    weights = []\n",
    "    for i in range(1, 6):\n",
    "        model_path = os.path.join(weights_dir, f\"{etf}_top{i}.pt\")\n",
    "        if not os.path.exists(model_path):\n",
    "            continue\n",
    "\n",
    "        data = torch.load(model_path, map_location='cpu')\n",
    "        linear_weights = data['weights'].get('output.weight', None)\n",
    "        if linear_weights is None:\n",
    "            print(f\"‚ö†Ô∏è {etf}_top{i}.pt missing output.weight\")\n",
    "            continue\n",
    "\n",
    "        linear_weights = linear_weights.squeeze().numpy()\n",
    "        if len(linear_weights) != len(factor_names):\n",
    "            print(f\"‚ö†Ô∏è {etf}_top{i}.pt: weight dim {len(linear_weights)} != {len(factor_names)} features\")\n",
    "            continue\n",
    "\n",
    "        weights.append(linear_weights * data.get('weight', 1.0))\n",
    "\n",
    "    if weights:\n",
    "        factor_weights[etf] = sum(weights)\n",
    "\n",
    "# === Create heatmap DataFrame\n",
    "weight_df = pd.DataFrame(factor_weights, index=factor_names).T\n",
    "\n",
    "# === Normalize (optional)\n",
    "weight_df = weight_df.div(weight_df.abs().max(axis=1), axis=0)\n",
    "\n",
    "# === Plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(weight_df, cmap=\"coolwarm\", center=0, annot=False)\n",
    "plt.title(\"üéØ Factor Importance per ETF (Weighted Top 5 Models)\", fontsize=14)\n",
    "plt.xlabel(\"Factors\")\n",
    "plt.ylabel(\"ETFs\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0df0c-95c4-4958-a1a7-40177b95abd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*enable_nested_tensor.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*weights_only=False.*\")\n",
    "\n",
    "class ETFBacktester:\n",
    "    def __init__(self, model_dir, data_dir, price_path, initial_cash=2000, benchmark_symbol='SPY', seq_len=4):\n",
    "        self.model_dir = model_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.price_path = price_path\n",
    "        self.initial_cash = initial_cash\n",
    "        self.benchmark_symbol = benchmark_symbol\n",
    "        self.seq_len = seq_len\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def run(self):\n",
    "        print(\"üîÅ Loading price data...\")\n",
    "        price_df = pd.read_csv(self.price_path, index_col=0)\n",
    "        price_df.index = pd.to_datetime(price_df.index, errors='coerce')\n",
    "        price_df = price_df[price_df.index.notna()]\n",
    "        price_df = price_df.apply(pd.to_numeric, errors='coerce')\n",
    "        price_df = price_df[~price_df.index.duplicated()].sort_index()\n",
    "        print(f\"üìà Loaded price data with {len(price_df)} entries.\")\n",
    "\n",
    "        etf_list = [fname.replace('_combined.csv', '') for fname in os.listdir(self.data_dir) if fname.endswith('_combined.csv')]\n",
    "        print(f\"üîç ETF list found: {etf_list}\")\n",
    "\n",
    "        cash = self.initial_cash\n",
    "        holdings = {}\n",
    "        portfolio_values = []\n",
    "        benchmark_values = []\n",
    "        dates = []\n",
    "        trade_log = []\n",
    "        value_log = []\n",
    "        portfolio_changes = []\n",
    "\n",
    "        for i in range(self.seq_len, len(price_df) - 1):\n",
    "            date = price_df.index[i]\n",
    "            prev_dates = price_df.index[i - self.seq_len:i]\n",
    "            next_date = price_df.index[i + 1]\n",
    "            print(f\"\\nüìÖ Processing week: {date.date()} -> {next_date.date()}\")\n",
    "\n",
    "            for etf, info in list(holdings.items()):\n",
    "                if etf not in price_df.columns or next_date not in price_df.index:\n",
    "                    continue\n",
    "                try:\n",
    "                    sell_price = float(price_df.loc[next_date, etf])\n",
    "                except Exception:\n",
    "                    continue\n",
    "                shares = info['Shares']\n",
    "                buy_price = info['BuyPrice']\n",
    "                sell_value = shares * sell_price\n",
    "                actual_return = (sell_price - buy_price) / buy_price\n",
    "                predicted_return = info.get('PredictedReturn', None)\n",
    "                cash += sell_value\n",
    "                trade_log.append({\n",
    "                    'Date': next_date.strftime('%Y-%m-%d'),\n",
    "                    'ETF': etf,\n",
    "                    'Action': 'Sell',\n",
    "                    'Price': sell_price,\n",
    "                    'Shares': shares,\n",
    "                    'Value': sell_value,\n",
    "                    'BuyPrice': buy_price,\n",
    "                    'PredictedReturn': predicted_return,\n",
    "                    'ActualReturn': actual_return * 100\n",
    "                })\n",
    "            holdings.clear()\n",
    "\n",
    "            week_predictions = []\n",
    "            for etf in etf_list:\n",
    "                feat_path = os.path.join(self.data_dir, f\"{etf}_combined.csv\")\n",
    "                mask_path = os.path.join(self.data_dir, f\"{etf}_mask.csv\")\n",
    "                if not os.path.exists(feat_path) or not os.path.exists(mask_path):\n",
    "                    continue\n",
    "\n",
    "                feat_df = pd.read_csv(feat_path, index_col=0, parse_dates=True)\n",
    "                mask_df = pd.read_csv(mask_path, index_col=0, parse_dates=True)\n",
    "\n",
    "                if not all(d in feat_df.index for d in prev_dates):\n",
    "                    continue\n",
    "\n",
    "                x_seq = torch.tensor(feat_df.loc[prev_dates].values.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "                x_mask = torch.tensor(mask_df.loc[prev_dates].values.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "\n",
    "                scores, preds = [], []\n",
    "                for j in range(1, 6):\n",
    "                    model_path = os.path.join(self.model_dir, f\"{etf}_top{j}.pt\")\n",
    "                    if not os.path.exists(model_path):\n",
    "                        continue\n",
    "                    checkpoint = torch.load(model_path, map_location=self.device)\n",
    "                    model = DualTransformerModel(num_features=x_seq.shape[-1]).to(self.device)\n",
    "                    model.load_state_dict(checkpoint['weights'])\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        pred = model(x_seq, x_mask).item()\n",
    "                    preds.append(pred)\n",
    "                    scores.append(checkpoint['weight'])\n",
    "\n",
    "                if preds:\n",
    "                    pred_return = sum(p * w for p, w in zip(preds, scores))\n",
    "                    try:\n",
    "                        price = float(price_df.loc[date, etf])\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    if not np.isnan(price):\n",
    "                        week_predictions.append({\n",
    "                            'ETF': etf,\n",
    "                            'BuyPrice': price,\n",
    "                            'PredictedReturn': float(pred_return)\n",
    "                        })\n",
    "\n",
    "            week_predictions = sorted(week_predictions, key=lambda x: -x['PredictedReturn'])[:2]\n",
    "\n",
    "            budget_per_etf = cash / 2\n",
    "            for item in week_predictions:\n",
    "                etf = item['ETF']\n",
    "                price = item['BuyPrice']\n",
    "                shares = int(budget_per_etf // price)\n",
    "                cost = shares * price\n",
    "                if shares <= 0 or cost > cash:\n",
    "                    continue\n",
    "                cash -= cost\n",
    "                holdings[etf] = {'Shares': shares, 'BuyPrice': price, 'PredictedReturn': item['PredictedReturn']}\n",
    "                trade_log.append({\n",
    "                    'Date': date.strftime('%Y-%m-%d'),\n",
    "                    'ETF': etf,\n",
    "                    'Action': 'Buy',\n",
    "                    'Price': price,\n",
    "                    'Shares': shares,\n",
    "                    'Value': cost,\n",
    "                    'PredictedReturn': item['PredictedReturn']\n",
    "                })\n",
    "\n",
    "            value = cash\n",
    "            for etf, info in holdings.items():\n",
    "                try:\n",
    "                    price = float(price_df.loc[next_date, etf])\n",
    "                except Exception:\n",
    "                    price = 0\n",
    "                value += info['Shares'] * price\n",
    "\n",
    "            prev_value = portfolio_values[-1] if portfolio_values else self.initial_cash\n",
    "            change_pct = ((value - prev_value) / prev_value) * 100\n",
    "\n",
    "            portfolio_values.append(value)\n",
    "            portfolio_changes.append(change_pct)\n",
    "            dates.append(next_date.strftime('%Y-%m-%d'))\n",
    "            value_log.append({\n",
    "                'Date': next_date.strftime('%Y-%m-%d'),\n",
    "                'Value': value,\n",
    "                'Cash': cash,\n",
    "                'Change%': change_pct,\n",
    "                'Holdings': {k: dict(v) for k, v in holdings.items()}\n",
    "            })\n",
    "\n",
    "            if self.benchmark_symbol in price_df.columns:\n",
    "                base_price = price_df[self.benchmark_symbol].iloc[0]\n",
    "                current_price = price_df.loc[next_date, self.benchmark_symbol]\n",
    "                benchmark_values.append(self.initial_cash * (current_price / base_price))\n",
    "            else:\n",
    "                benchmark_values.append(value)\n",
    "\n",
    "        result_df = pd.DataFrame({\n",
    "            'Date': dates,\n",
    "            'PortfolioValue': portfolio_values,\n",
    "            'BenchmarkValue': benchmark_values,\n",
    "            'PortfolioChange%': portfolio_changes\n",
    "        })\n",
    "        result_df['Date'] = pd.to_datetime(result_df['Date'], errors='coerce')\n",
    "        result_df.set_index('Date', inplace=True)\n",
    "\n",
    "        pd.DataFrame(trade_log).to_csv(\"../dataset/backtest_trade_log.csv\", index=False)\n",
    "        pd.DataFrame(value_log).to_csv(\"../dataset/backtest_value_log.csv\", index=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(result_df['PortfolioValue'], label='Strategy Portfolio')\n",
    "        plt.plot(result_df['BenchmarkValue'], label=f'{self.benchmark_symbol} Benchmark')\n",
    "        plt.title(\"Backtest Result: Strategy vs. Benchmark\")\n",
    "        plt.ylabel(\"Portfolio Value (USD)\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        xticks_idx = result_df.index[::52]\n",
    "        xticks_labels = [d.strftime('%Y') for d in xticks_idx]\n",
    "        plt.xticks(xticks_idx, xticks_labels)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nüí∞ Final Portfolio Value: ${result_df['PortfolioValue'].iloc[-1]:,.2f}\")\n",
    "        print(f\"üìà Final Benchmark Value: ${result_df['BenchmarkValue'].iloc[-1]:,.2f}\")\n",
    "        print(f\"üéØ Strategy Return: {((result_df['PortfolioValue'].iloc[-1] / self.initial_cash - 1) * 100):.2f}%\")\n",
    "        print(f\"üìä Benchmark Return: {((result_df['BenchmarkValue'].iloc[-1] / self.initial_cash - 1) * 100):.2f}%\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "# === Execute\n",
    "backtester = ETFBacktester(\n",
    "    model_dir=\"../model_weights\",\n",
    "    data_dir=\"../dataset/normalized_matrix\",\n",
    "    price_path=\"../dataset/etf_prices_weekly.csv\",\n",
    "    initial_cash=2000,\n",
    "    benchmark_symbol=\"SPY\",\n",
    "    seq_len=4\n",
    ")\n",
    "\n",
    "result_df = backtester.run()  # ‚¨ÖÔ∏è THIS must be included!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e71f08-ed44-4637-8b03-5d26a303f1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stock_pred)",
   "language": "python",
   "name": "stock_pred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
