{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a81306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ETF list\n",
    "etf_list = [\n",
    "    'XLK', 'XLF', 'XLV', 'XLE', 'XLI', 'XLY', 'XLP', 'XLRE', 'XLU', 'XLB', 'XLC',\n",
    "    'SOXX', 'SH', 'DOG', 'RWM', 'ITA', 'JETS', 'PSQ', 'VNQ', 'SPY'\n",
    "]\n",
    "\n",
    "# Date range: from 10 years ago to now\n",
    "today = datetime.today()\n",
    "back_time = today - timedelta(weeks=52 * 10)\n",
    "start_date = back_time.strftime('%Y-%m-%d')\n",
    "end_date = today.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"ðŸ—•ï¸ Downloading weekly data from {start_date} to {end_date}\")\n",
    "\n",
    "# Ensure dataset/ exists\n",
    "dataset_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset'))\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# Containers\n",
    "adjclose_data, volume_data, high_data, low_data = {}, {}, {}, {}\n",
    "\n",
    "# Download each ETF\n",
    "for symbol in etf_list:\n",
    "    print(f\"â¬‡ï¸ Downloading {symbol}...\")\n",
    "    data = yf.download(\n",
    "        symbol,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        interval='1wk',  # back to Yahoo's default weekly format (week ending on Friday)\n",
    "        auto_adjust=False,\n",
    "        progress=False\n",
    "    )\n",
    "    if not data.empty:\n",
    "        data = data[~data.index.duplicated(keep='first')].sort_index()\n",
    "        adjclose_data[symbol] = data[['Adj Close']].rename(columns={'Adj Close': symbol})\n",
    "        volume_data[symbol] = data[['Volume']].rename(columns={'Volume': symbol})\n",
    "        high_data[symbol] = data[['High']].rename(columns={'High': symbol})\n",
    "        low_data[symbol] = data[['Low']].rename(columns={'Low': symbol})\n",
    "\n",
    "# Merge and save\n",
    "def combine_and_save(data_dict, filename):\n",
    "    df = pd.concat(data_dict.values(), axis=1)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "    df = df[~df.index.duplicated(keep='first')].sort_index()\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    path = os.path.join(dataset_path, filename)\n",
    "    df.to_csv(path)\n",
    "    print(f\"âœ… Saved: {filename}\")\n",
    "    return df\n",
    "\n",
    "# Save all\n",
    "price_df = combine_and_save(adjclose_data, 'etf_prices_weekly.csv')\n",
    "volume_df = combine_and_save(volume_data, 'etf_volume_weekly.csv')\n",
    "high_df = combine_and_save(high_data, 'etf_high_weekly.csv')\n",
    "low_df = combine_and_save(low_data, 'etf_low_weekly.csv')\n",
    "\n",
    "# Preview\n",
    "price_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ecccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Macro indicator tickers on Yahoo Finance\n",
    "macro_tickers = {\n",
    "    'VIX': '^VIX',               # Volatility Index\n",
    "    '10Y_Yield': '^TNX',         # 10-Year Treasury Yield (multiply by 0.1)\n",
    "    'USD_Index': 'DX-Y.NYB',     # U.S. Dollar Index (ICE Dollar Index)\n",
    "    'WTI_Crude': 'CL=F'          # Crude Oil (WTI Futures)\n",
    "}\n",
    "\n",
    "# Align with ETF backtest window using back_time\n",
    "start_date = back_time.strftime('%Y-%m-%d')\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"ðŸ“Š Downloading macro data from {start_date} to {end_date}\")\n",
    "\n",
    "# Download weekly data\n",
    "macro_data = {}\n",
    "for name, ticker in macro_tickers.items():\n",
    "    print(f\"â¬‡ï¸ Downloading {name} ({ticker})...\")\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, interval='1wk', auto_adjust=False, progress=False)\n",
    "    if not data.empty:\n",
    "        macro_data[name] = data[['Close']].rename(columns={'Close': name})\n",
    "\n",
    "# Combine all macro indicators into one DataFrame\n",
    "macro_df = pd.concat(macro_data.values(), axis=1)\n",
    "\n",
    "# Fix 10Y yield scale\n",
    "if '10Y_Yield' in macro_df.columns:\n",
    "    macro_df['10Y_Yield'] = macro_df['10Y_Yield'] * 0.1\n",
    "\n",
    "# Clean and format\n",
    "macro_df = macro_df.apply(pd.to_numeric, errors='coerce')\n",
    "macro_df.index = pd.to_datetime(macro_df.index)\n",
    "macro_df = macro_df[~macro_df.index.duplicated(keep='first')]\n",
    "macro_df.sort_index(inplace=True)\n",
    "macro_df.dropna(inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "macro_save_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'macro_indicators_weekly.csv'))\n",
    "macro_df.to_csv(macro_save_path)\n",
    "print(f\"âœ… Macro indicators saved to: {macro_save_path}\")\n",
    "\n",
    "# Preview\n",
    "macro_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e27fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from ta import momentum, trend, volume\n",
    "\n",
    "# === Paths ===\n",
    "price_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'etf_prices_weekly.csv'))\n",
    "volume_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'etf_volume_weekly.csv'))\n",
    "\n",
    "# === Helper to load ETF CSVs ===\n",
    "def load_etf_csv(path, name='[unknown]'):\n",
    "    try:\n",
    "        header_row = pd.read_csv(path, header=None, nrows=2)\n",
    "        columns = header_row.iloc[1].tolist()[1:]\n",
    "        df = pd.read_csv(path, skiprows=3, header=None)\n",
    "        df = df.iloc[:, :len(columns) + 1]\n",
    "        df.columns = ['Date'] + columns\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        df = df.set_index('Date')\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "        print(f\"âœ… Loaded {name} with {len(columns)} tickers\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load {name}: {e}\")\n",
    "        raise\n",
    "\n",
    "# === Load data ===\n",
    "price_df = load_etf_csv(price_path, name='ETF Prices')\n",
    "volume_df = load_etf_csv(volume_path, name='ETF Volume')\n",
    "\n",
    "# === Feature storage ===\n",
    "features_all = []\n",
    "skipped = []\n",
    "\n",
    "# === Feature generation loop ===\n",
    "for symbol in price_df.columns:\n",
    "    if symbol not in volume_df.columns:\n",
    "        print(f\"âš ï¸ Skipping {symbol}: volume data missing.\")\n",
    "        skipped.append(symbol)\n",
    "        continue\n",
    "\n",
    "    df = pd.DataFrame(index=price_df.index)\n",
    "    df['close'] = price_df[symbol]\n",
    "    df['volume'] = volume_df[symbol]\n",
    "\n",
    "    try:\n",
    "        # === Return-based technical indicators ===\n",
    "        df[f'{symbol}_ret_1w'] = df['close'].pct_change(1)\n",
    "        df[f'{symbol}_ret_3w'] = df['close'].pct_change(3)\n",
    "        df[f'{symbol}_ret_6w'] = df['close'].pct_change(6)\n",
    "\n",
    "        high = df['close'].rolling(window=14).max()\n",
    "        low = df['close'].rolling(window=14).min()\n",
    "        df[f'{symbol}_stoch_k'] = 100 * (df['close'] - low) / (high - low)\n",
    "        df[f'{symbol}_stoch_d'] = df[f'{symbol}_stoch_k'].rolling(window=3).mean()\n",
    "        df[f'{symbol}_williams_r'] = -100 * (high - df['close']) / (high - low)\n",
    "\n",
    "        df[f'{symbol}_cci'] = trend.cci(high=df['close'], low=df['close'], close=df['close'], window=20)\n",
    "        df[f'{symbol}_rsi'] = momentum.rsi(df['close'], window=14)\n",
    "        df[f'{symbol}_obv'] = volume.on_balance_volume(df['close'], df['volume'])\n",
    "\n",
    "        df[f'{symbol}_macd'] = trend.macd(df['close'])\n",
    "        df[f'{symbol}_macd_signal'] = trend.macd_signal(df['close'])\n",
    "        df[f'{symbol}_macd_diff'] = trend.macd_diff(df['close'])\n",
    "\n",
    "        # === Add price and volume percent change (variation) ===\n",
    "        df[f'{symbol}_price_change'] = df['close'].pct_change(1)\n",
    "        df[f'{symbol}_volume_change'] = df['volume'].pct_change(1)\n",
    "\n",
    "        derived_cols = df.columns.difference(['close', 'volume'])\n",
    "        feature_df = df[derived_cols].copy()\n",
    "\n",
    "        features_all.append(feature_df)\n",
    "        print(f\"ðŸ“ˆ {symbol}: {feature_df.dropna(how='all').shape[0]} valid rows\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {symbol}: {e}\")\n",
    "        skipped.append(symbol)\n",
    "\n",
    "# === Final merge ===\n",
    "if len(features_all) == 0:\n",
    "    raise ValueError(\"ðŸ›‘ No valid ETF features generated.\")\n",
    "\n",
    "features_df = pd.concat(features_all, axis=1).sort_index()\n",
    "features_df = features_df[~features_df.index.duplicated(keep='first')]\n",
    "\n",
    "print(\"ðŸ“€ Final feature shape:\", features_df.shape)\n",
    "\n",
    "# === Save to CSV ===\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset'))\n",
    "features_df.to_csv(os.path.join(base_dir, 'weekly_etf_tech_feature.csv'))\n",
    "print(f\"âœ… Saved features to: weekly_etf_tech_feature.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b6cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Paths ===\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset'))\n",
    "feat_path = os.path.join(base_dir, 'weekly_etf_tech_feature.csv')\n",
    "macro_path = os.path.join(base_dir, 'macro_indicators_weekly.csv')\n",
    "price_path = os.path.join(base_dir, 'etf_prices_weekly.csv')\n",
    "volume_path = os.path.join(base_dir, 'etf_volume_weekly.csv')\n",
    "\n",
    "# === Load main features ===\n",
    "feat_df = pd.read_csv(feat_path, index_col=0, parse_dates=True)\n",
    "\n",
    "# === Load and parse macro indicators ===\n",
    "macro_raw = pd.read_csv(macro_path, header=None)\n",
    "macro_start = macro_raw.index[macro_raw[0] == 'Price'].tolist()[0]\n",
    "macro_cols = macro_raw.iloc[macro_start].tolist()\n",
    "macro_df = pd.read_csv(macro_path, skiprows=macro_start + 1, header=None)\n",
    "macro_df.columns = macro_cols\n",
    "macro_df['Price'] = pd.to_datetime(macro_df['Price'], errors='coerce')  # 'Price' is actually the 'Date'\n",
    "macro_df = macro_df.rename(columns={'Price': 'Date'}).set_index('Date')\n",
    "macro_df = macro_df.apply(pd.to_numeric, errors='coerce')\n",
    "macro_df = macro_df[~macro_df.index.duplicated()].sort_index()\n",
    "\n",
    "# === Load ETF price and volume ===\n",
    "ticker_row = pd.read_csv(price_path, header=None, nrows=2).iloc[1, 1:].tolist()\n",
    "price_df = pd.read_csv(price_path, skiprows=3, header=None)\n",
    "volume_df = pd.read_csv(volume_path, skiprows=3, header=None)\n",
    "\n",
    "price_df = price_df.iloc[:, :len(ticker_row)+1]\n",
    "price_df.columns = ['Date'] + ticker_row\n",
    "price_df['Date'] = pd.to_datetime(price_df['Date'])\n",
    "price_df = price_df.set_index('Date').apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "volume_df = volume_df.iloc[:, :len(ticker_row)+1]\n",
    "volume_df.columns = ['Date'] + ticker_row\n",
    "volume_df['Date'] = pd.to_datetime(volume_df['Date'])\n",
    "volume_df = volume_df.set_index('Date').apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# === Output Directory ===\n",
    "norm_dir = os.path.join(base_dir, 'normalized_matrix')\n",
    "os.makedirs(norm_dir, exist_ok=True)\n",
    "\n",
    "# === Normalize & Save per ETF ===\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for ticker in ticker_row:\n",
    "    feat_cols = [col for col in feat_df.columns if col.startswith(f'{ticker}_')]\n",
    "    if len(feat_cols) == 0:\n",
    "        print(f\"âš ï¸ No features found for {ticker}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    df = feat_df[feat_cols].copy()\n",
    "    df.columns = [col.replace(f'{ticker}_', '') for col in df.columns]\n",
    "\n",
    "    df['price'] = price_df[ticker].pct_change()\n",
    "    df['volume'] = volume_df[ticker].pct_change()\n",
    "\n",
    "    # === Merge with true macro indicators ===\n",
    "    df = df.join(macro_df, how='left')\n",
    "\n",
    "    # === Build missing mask BEFORE filling\n",
    "    mask = df.isna().astype(float)\n",
    "\n",
    "    # === Fill and normalize only technical + pct change features, not macro\n",
    "    macro_cols = macro_df.columns\n",
    "    norm_cols = df.columns.difference(macro_cols)\n",
    "    df[norm_cols] = scaler.fit_transform(df[norm_cols].fillna(0.0))\n",
    "    df[macro_cols] = df[macro_cols].fillna(0.0)  # keep raw values for macro features\n",
    "\n",
    "    # === Save\n",
    "    df.to_csv(os.path.join(norm_dir, f'{ticker}_combined.csv'))\n",
    "    mask.to_csv(os.path.join(norm_dir, f'{ticker}_mask.csv'))\n",
    "    print(f\"âœ… Saved {ticker}: {df.shape[0]} rows, {df.shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e3617",
   "metadata": {},
   "source": [
    "# start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'normalized_matrix'))\n",
    "\n",
    "\n",
    "# === Dataset ===\n",
    "class ETFDataset(Dataset):\n",
    "    def __init__(self, combined_csv, mask_csv):\n",
    "        self.X = pd.read_csv(combined_csv, index_col=0).values.astype(np.float32)\n",
    "        self.M = pd.read_csv(mask_csv, index_col=0).values.astype(np.float32)\n",
    "        self.y = self.X[1:, 0]  # assume target is 1-week return (first col), shifted\n",
    "        self.X = self.X[:-1]\n",
    "        self.M = self.M[:-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.X[idx]),\n",
    "            torch.tensor(self.M[idx]),\n",
    "            torch.tensor(self.y[idx])\n",
    "        )\n",
    "\n",
    "# === Model ===\n",
    "class DualTransformerModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=num_features,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=hidden_dim,\n",
    "                batch_first=True  # ðŸ‘ˆ this avoids the warning\n",
    "            ),\n",
    "\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.output = nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x_masked = x * (1 - mask)  # mask==1 means missing\n",
    "        x_seq = x_masked.unsqueeze(1)  # fake sequence length = 1\n",
    "        enc = self.encoder(x_seq).squeeze(1)\n",
    "        return self.output(enc).squeeze(-1)\n",
    "\n",
    "# === Training Params ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 0.001\n",
    "max_epochs = 3000\n",
    "\n",
    "# === Training Function ===\n",
    "def train_one_etf(etf):\n",
    "    print(f\"\\nðŸš€ Training {etf}\")\n",
    "    feat_path = os.path.join(data_dir, f\"{etf}_combined.csv\")\n",
    "    mask_path = os.path.join(data_dir, f\"{etf}_mask.csv\")\n",
    "    dataset = ETFDataset(feat_path, mask_path)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    X, _, _ = next(iter(loader))\n",
    "    model = DualTransformerModel(num_features=X.shape[-1]).to(device)\n",
    "    model.device = device  # Ensure we store the device inside the model\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    mae_loss = nn.L1Loss()\n",
    "    \n",
    "    best_models = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss, total_mae, total_win = 0, 0, 0\n",
    "\n",
    "        for X, M, y in loader:\n",
    "            X, M, y = X.to(model.device), M.to(model.device), y.to(model.device)\n",
    "            pred = model(X, M)\n",
    "            # Main losses\n",
    "            mse = mse_loss(pred, y)\n",
    "            mae = mae_loss(pred, y)\n",
    "            direction_loss = 1 - (torch.sign(pred) == torch.sign(y)).float().mean()\n",
    "\n",
    "            # Temporal lead loss: penalize lag by comparing shifted pred\n",
    "            if len(pred) > 1:\n",
    "                shifted_pred = pred[:-1]\n",
    "                shifted_target = y[1:]  # make sure lengths match\n",
    "                temporal_loss = mse_loss(shifted_pred, shifted_target)\n",
    "            else:\n",
    "                temporal_loss = 0.0\n",
    "\n",
    "            # Total combined loss\n",
    "            loss = mse + 0.5 * mae + 0.5 * direction_loss + 0.2 * temporal_loss\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            mae = (pred - y).abs().mean().item()\n",
    "            win = (torch.sign(pred) == torch.sign(y)).float().mean().item()\n",
    "            total_loss += loss.item()\n",
    "            total_mae += mae\n",
    "            total_win += win\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        avg_mae = total_mae / len(loader)\n",
    "        avg_win = total_win / len(loader)\n",
    "        score = avg_win * 0.6 + (1 - avg_mae) * 0.4\n",
    "        best_models.append((score, model.state_dict(), avg_win, avg_mae))\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, MAE={avg_mae:.4f}, WinRate={avg_win:.4f}\")\n",
    "\n",
    "    # Save top 5 models\n",
    "    top5 = sorted(best_models, key=lambda x: -x[0])[:5]\n",
    "    weight_sum = sum(x[0] for x in top5)\n",
    "    for i, (score, weights, win, mae) in enumerate(top5):\n",
    "        out = {\n",
    "            'weights': weights,\n",
    "            'score': score,\n",
    "            'win_rate': win,\n",
    "            'mae': mae,\n",
    "            'weight': score / weight_sum\n",
    "        }\n",
    "        save_path = os.path.join(\"..\", \"model_weights\", f\"{etf}_top{i+1}.pt\")\n",
    "        torch.save(out, save_path)\n",
    "\n",
    "# === Execute ===\n",
    "for etf in etf_list:\n",
    "    train_one_etf(etf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb75d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Config ===\n",
    "weights_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'model_weights'))\n",
    "feature_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'normalized_matrix'))\n",
    "\n",
    "\n",
    "# === Storage ===\n",
    "factor_weights = {}\n",
    "\n",
    "# === Extract input dimension ===\n",
    "first_etf = etf_list[0]\n",
    "example_path = os.path.join(feature_dir, f'{first_etf}_combined.csv')\n",
    "example_features = pd.read_csv(example_path, index_col=0)\n",
    "factor_names = example_features.columns.tolist()\n",
    "\n",
    "# === Aggregate top 5 weights for each ETF\n",
    "for etf in etf_list:\n",
    "    weights = []\n",
    "    for i in range(1, 6):\n",
    "        model_path = os.path.join(weights_dir, f\"{etf}_top{i}.pt\")\n",
    "        if os.path.exists(model_path):\n",
    "            data = torch.load(model_path, map_location='cpu')\n",
    "            linear_weights = data['weights']['output.weight'].squeeze().numpy()\n",
    "            weights.append(linear_weights * data['weight'])  # weighted by performance\n",
    "    if weights:\n",
    "        factor_weights[etf] = sum(weights)\n",
    "\n",
    "# === Create DataFrame for heatmap\n",
    "weight_df = pd.DataFrame(factor_weights, index=factor_names)\n",
    "weight_df = weight_df.T  # shape: (ETF, Factors)\n",
    "\n",
    "# === Plot ===\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(weight_df, cmap=\"coolwarm\", annot=False, center=0)\n",
    "plt.title(\"ðŸŽ¯ Factor Importance per ETF (Weighted Top 5 Models)\", fontsize=14)\n",
    "plt.xlabel(\"Factors\")\n",
    "plt.ylabel(\"ETFs\")\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608bc50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# === Paths ===\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataset', 'normalized_matrix'))\n",
    "model_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'model_weights'))\n",
    "\n",
    "\n",
    "\n",
    "# === Gather predictions\n",
    "plot_data = []\n",
    "\n",
    "\n",
    "for etf in etf_list:\n",
    "    try:\n",
    "        feat_path = os.path.join(data_dir, f\"{etf}_combined.csv\")\n",
    "        mask_path = os.path.join(data_dir, f\"{etf}_mask.csv\")\n",
    "        model_path = os.path.join(model_dir, f\"{etf}_top1.pt\")\n",
    "\n",
    "        X = pd.read_csv(feat_path, index_col=0).astype('float32')\n",
    "        M = pd.read_csv(mask_path, index_col=0).astype('float32')\n",
    "        y = X.iloc[1:, 0].values\n",
    "        X = X.iloc[:-1]\n",
    "        M = M.iloc[:-1]\n",
    "\n",
    "        model = DualTransformerModel(num_features=X.shape[1])\n",
    "        model.load_state_dict(torch.load(model_path)['weights'])\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(torch.tensor(X.values), torch.tensor(M.values)).numpy()\n",
    "\n",
    "        df = pd.DataFrame({'Date': X.index, 'Predicted': pred, 'Actual': y}).set_index('Date')\n",
    "        plot_data.append((etf, df.iloc[-50:]))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error loading/predicting {etf}: {e}\")\n",
    "\n",
    "# === Plotting\n",
    "fig = plt.figure(figsize=(20, 4 * ((len(plot_data) + 3) // 4)))\n",
    "gs = gridspec.GridSpec((len(plot_data) + 3) // 4, 4, figure=fig)\n",
    "\n",
    "for i, (etf, df) in enumerate(plot_data):\n",
    "    ax = fig.add_subplot(gs[i])\n",
    "    ax.plot(df.index, df['Actual'], label='Actual', color='blue')\n",
    "    ax.plot(df.index, df['Predicted'], label='Predicted', color='red')\n",
    "    ax.set_title(etf)\n",
    "    ax.legend()\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "fig.suptitle(\"ETF Weekly Return Prediction vs. Actual\", fontsize=18)\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# === Prediction Class ===\n",
    "class WeeklyETFPredictor:\n",
    "    def __init__(self, model_dir, data_dir, record_dir, price_path, device=None):\n",
    "        self.model_dir = model_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.record_dir = record_dir\n",
    "        self.price_path = price_path\n",
    "        os.makedirs(self.record_dir, exist_ok=True)\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def predict(self):\n",
    "        # Get nearest past Monday\n",
    "        today = datetime.today()\n",
    "        days_back = today.weekday() % 7\n",
    "        monday = today - timedelta(days=days_back)\n",
    "        monday_dt = pd.to_datetime(monday.strftime(\"%Y-%m-%d\"))\n",
    "        date_str = monday_dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Load ETF price data for actual price reference\n",
    "        price_df = pd.read_csv(\n",
    "            self.price_path,\n",
    "            index_col=0,\n",
    "            parse_dates=True,\n",
    "            date_parser=lambda x: pd.to_datetime(x, format=\"%Y-%m-%d\", errors='coerce')\n",
    "        )\n",
    "        price_df = price_df[price_df.index.notna()]\n",
    "        price_df = price_df[~price_df.index.duplicated()].sort_index()\n",
    "\n",
    "        print(f\"ðŸ” Available price dates: {price_df.index[-5:].to_list()}\")\n",
    "        print(f\"ðŸ“… Looking for Monday: {monday_dt}\")\n",
    "\n",
    "        summary = []\n",
    "\n",
    "        for fname in os.listdir(self.data_dir):\n",
    "            if not fname.endswith(\"_combined.csv\"): continue\n",
    "            etf = fname.replace(\"_combined.csv\", \"\")\n",
    "\n",
    "            feat_path = os.path.join(self.data_dir, f\"{etf}_combined.csv\")\n",
    "            mask_path = os.path.join(self.data_dir, f\"{etf}_mask.csv\")\n",
    "\n",
    "            try:\n",
    "                df_feat = pd.read_csv(feat_path, index_col=0, parse_dates=True)\n",
    "                df_mask = pd.read_csv(mask_path, index_col=0, parse_dates=True)\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed to load data for {etf}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if monday_dt not in df_feat.index:\n",
    "                print(f\"âš ï¸ Missing feature/mask data for {etf} on {monday_dt.date()}\")\n",
    "                continue\n",
    "\n",
    "            if etf not in price_df.columns:\n",
    "                print(f\"âš ï¸ ETF {etf} not found in price_df columns.\")\n",
    "                continue\n",
    "\n",
    "            if monday_dt not in price_df.index:\n",
    "                print(f\"âš ï¸ Missing price for {etf} on {monday_dt.date()}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                x_real = torch.tensor(df_feat.loc[monday_dt].values.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "                x_mask = torch.tensor(df_mask.loc[monday_dt].values.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Tensor conversion failed for {etf}: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                price = float(price_df.loc[monday_dt, etf])\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Price parsing error for {etf} on {monday_dt.date()}: {e}\")\n",
    "                price = None\n",
    "\n",
    "            scores, preds, maes, winrates = [], [], [], []\n",
    "            for i in range(1, 6):\n",
    "                path = os.path.join(self.model_dir, f\"{etf}_top{i}.pt\")\n",
    "                if not os.path.exists(path): continue\n",
    "                try:\n",
    "                    checkpoint = torch.load(path, map_location=self.device)\n",
    "                    model = DualTransformerModel(num_features=x_real.shape[-1]).to(self.device)\n",
    "                    model.load_state_dict(checkpoint['weights'])\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        pred = model(x_real, x_mask).item()\n",
    "                    preds.append(pred)\n",
    "                    maes.append(checkpoint['mae'])\n",
    "                    winrates.append(checkpoint['win_rate'])\n",
    "                    scores.append(checkpoint['weight'])\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Model loading/prediction failed for {etf} top{i}: {e}\")\n",
    "\n",
    "            if len(preds) == 0:\n",
    "                print(f\"âš ï¸ No predictions available for {etf}\")\n",
    "                continue\n",
    "\n",
    "            pred_return = sum(p * w for p, w in zip(preds, scores))\n",
    "            avg_mae = sum(maes) / len(maes)\n",
    "            avg_win = sum(winrates) / len(winrates)\n",
    "\n",
    "            if price is not None:\n",
    "                if pred_return >= 0:\n",
    "                    target_up = round(price * (1 + pred_return/100 + pred_return*avg_mae/100), 2)\n",
    "                    stop_down = round(price * (1 - pred_return*avg_mae/100), 2)\n",
    "                    buy_price = round(price, 2)\n",
    "                else:\n",
    "                    target_up = stop_down = buy_price = \"X\"\n",
    "            else:\n",
    "                target_up = stop_down = buy_price = \"X\"\n",
    "\n",
    "            summary.append({\n",
    "                'ETF': etf,\n",
    "                'PredictedReturn': round(pred_return, 4),\n",
    "                'MAE': round(avg_mae, 4),\n",
    "                'WinRate': round(avg_win, 4),\n",
    "                'BuyPrice': buy_price,\n",
    "                'Targetâ†‘': target_up,\n",
    "                'Stopâ†“': stop_down\n",
    "            })\n",
    "\n",
    "        summary_df = pd.DataFrame(summary)\n",
    "\n",
    "        if summary_df.empty:\n",
    "            print(f\"âš ï¸ No prediction results available for {date_str}. Check model weights and input data.\")\n",
    "            return summary_df\n",
    "\n",
    "        summary_df = summary_df.sort_values(by='PredictedReturn', ascending=False)\n",
    "\n",
    "        # === Print to console\n",
    "        print(f\"\\nðŸ“Š Weekly ETF Prediction Summary for {date_str}:\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "\n",
    "        # === Save to file\n",
    "        output_path = os.path.join(self.record_dir, f\"{today.strftime('%Y-%m-%d')}_predict_record.csv\")\n",
    "        summary_df.to_csv(output_path, index=False)\n",
    "        print(f\"ðŸ“ Prediction CSV saved to: {output_path}\")\n",
    "        return summary_df\n",
    "\n",
    "# === Example usage ===\n",
    "predictor = WeeklyETFPredictor(\n",
    "    model_dir=\"../model_weights\",\n",
    "    data_dir=\"../dataset/normalized_matrix\",\n",
    "    record_dir=\"../dataset/predict_record\",\n",
    "    price_path=\"../dataset/etf_prices_weekly.csv\"\n",
    ")\n",
    "predictor.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de35d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class ETFBacktester:\n",
    "    def __init__(self, model_dir, data_dir, price_path, initial_cash=2000, benchmark_symbol='SPY'):\n",
    "        self.model_dir = model_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.price_path = price_path\n",
    "        self.initial_cash = initial_cash\n",
    "        self.benchmark_symbol = benchmark_symbol\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def run(self):\n",
    "        print(\"ðŸ” Loading price data...\")\n",
    "        price_df = pd.read_csv(self.price_path, index_col=0)\n",
    "        price_df.index = pd.to_datetime(price_df.index, errors='coerce')\n",
    "        price_df = price_df.apply(pd.to_numeric, errors='coerce')\n",
    "        price_df = price_df[~price_df.index.duplicated()].sort_index()\n",
    "        print(f\"ðŸ“ˆ Loaded price data with {len(price_df)} entries.\")\n",
    "\n",
    "        etf_list = [fname.replace('_combined.csv', '') for fname in os.listdir(self.data_dir) if fname.endswith('_combined.csv')]\n",
    "        print(f\"ðŸ” ETF list found: {etf_list}\")\n",
    "\n",
    "        cash = self.initial_cash\n",
    "        holdings = {}\n",
    "        portfolio_values = []\n",
    "        benchmark_values = []\n",
    "        dates = []\n",
    "        trade_log = []\n",
    "        value_log = []\n",
    "\n",
    "        for i in range(len(price_df) - 2):\n",
    "            date = price_df.index[i]\n",
    "            next_date = price_df.index[i + 1]\n",
    "            print(f\"\\nðŸ“… Processing week: {date.date()} -> {next_date.date()}\")\n",
    "\n",
    "            # Sell all current holdings at the end of the week\n",
    "            for etf, info in list(holdings.items()):\n",
    "                if etf not in price_df.columns or next_date not in price_df.index:\n",
    "                    continue\n",
    "                try:\n",
    "                    sell_price = float(price_df.loc[next_date, etf])\n",
    "                except (KeyError, ValueError, TypeError):\n",
    "                    continue\n",
    "                shares = info['Shares']\n",
    "                buy_price = info['BuyPrice']\n",
    "                sell_value = shares * sell_price\n",
    "                actual_return = (sell_price - buy_price) / buy_price\n",
    "                predicted_return = info.get('PredictedReturn', None)\n",
    "                cash += sell_value\n",
    "                trade_log.append({\n",
    "                    'Date': next_date.strftime('%Y-%m-%d'),\n",
    "                    'ETF': etf,\n",
    "                    'Action': 'Sell',\n",
    "                    'Price': sell_price,\n",
    "                    'Shares': shares,\n",
    "                    'Value': sell_value,\n",
    "                    'BuyPrice': buy_price,\n",
    "                    'PredictedReturn': predicted_return,\n",
    "                    'ActualReturn': actual_return*100\n",
    "                })\n",
    "            holdings.clear()\n",
    "\n",
    "            # Predict top 2 ETFs\n",
    "            week_predictions = []\n",
    "            for etf in etf_list:\n",
    "                feat_path = os.path.join(self.data_dir, f\"{etf}_combined.csv\")\n",
    "                mask_path = os.path.join(self.data_dir, f\"{etf}_mask.csv\")\n",
    "                if not os.path.exists(feat_path) or not os.path.exists(mask_path):\n",
    "                    continue\n",
    "\n",
    "                feat_df = pd.read_csv(feat_path, index_col=0, parse_dates=True)\n",
    "                mask_df = pd.read_csv(mask_path, index_col=0, parse_dates=True)\n",
    "                if date not in feat_df.index:\n",
    "                    continue\n",
    "\n",
    "                x_real = torch.tensor(feat_df.loc[date].values.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "                x_mask = torch.tensor(mask_df.loc[date].values.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "\n",
    "                scores, preds = [], []\n",
    "                for j in range(1, 6):\n",
    "                    model_path = os.path.join(self.model_dir, f\"{etf}_top{j}.pt\")\n",
    "                    if not os.path.exists(model_path): continue\n",
    "                    checkpoint = torch.load(model_path, map_location=self.device, weights_only=True)\n",
    "                    model = DualTransformerModel(num_features=x_real.shape[-1]).to(self.device)\n",
    "                    model.load_state_dict(checkpoint['weights'])\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        pred = model(x_real, x_mask).item()\n",
    "                    preds.append(pred)\n",
    "                    scores.append(checkpoint['weight'])\n",
    "\n",
    "                if preds:\n",
    "                    pred_return = sum(p * w for p, w in zip(preds, scores))\n",
    "                    try:\n",
    "                        price = float(price_df.loc[date, etf]) if etf in price_df.columns else None\n",
    "                    except (KeyError, ValueError, TypeError):\n",
    "                        continue\n",
    "\n",
    "                    if price is not None and not np.isnan(price):\n",
    "                        week_predictions.append({\n",
    "                            'ETF': etf,\n",
    "                            'BuyPrice': price,\n",
    "                            'PredictedReturn': float(pred_return)\n",
    "                        })\n",
    "\n",
    "            week_predictions = sorted(week_predictions, key=lambda x: -x['PredictedReturn'])[:2]\n",
    "\n",
    "            budget_per_etf = cash / 2  # pre-divide to avoid compounding error\n",
    "            for item in week_predictions:\n",
    "                etf = item['ETF']\n",
    "                price = item['BuyPrice']\n",
    "                shares = int(budget_per_etf // price)\n",
    "                cost = shares * price\n",
    "                if shares <= 0 or cost > cash:\n",
    "                    continue\n",
    "                cash -= cost\n",
    "                holdings[etf] = {'Shares': shares, 'BuyPrice': price, 'PredictedReturn': item['PredictedReturn']}\n",
    "                trade_log.append({\n",
    "                    'Date': date.strftime('%Y-%m-%d'),\n",
    "                    'ETF': etf,\n",
    "                    'Action': 'Buy',\n",
    "                    'Price': price,\n",
    "                    'Shares': shares,\n",
    "                    'Value': cost,\n",
    "                    'PredictedReturn': item['PredictedReturn']\n",
    "                })\n",
    "\n",
    "            # Weekly portfolio valuation\n",
    "            value = cash\n",
    "            for etf, info in holdings.items():\n",
    "                try:\n",
    "                    price = float(price_df.loc[next_date, etf]) if etf in price_df.columns else 0\n",
    "                except (KeyError, ValueError, TypeError):\n",
    "                    price = 0\n",
    "                value += info['Shares'] * price\n",
    "\n",
    "            portfolio_values.append(value)\n",
    "            dates.append(next_date.strftime('%Y-%m-%d'))\n",
    "            value_log.append({\n",
    "                'Date': next_date.strftime('%Y-%m-%d'),\n",
    "                'Value': value,\n",
    "                'Cash': cash,\n",
    "                'Holdings': {k: dict(v) for k, v in holdings.items()}\n",
    "            })\n",
    "\n",
    "            if self.benchmark_symbol in price_df.columns:\n",
    "                base_price = price_df[self.benchmark_symbol].iloc[0]\n",
    "                current_price = price_df.loc[next_date, self.benchmark_symbol]\n",
    "                benchmark_values.append(self.initial_cash * (current_price / base_price))\n",
    "            else:\n",
    "                benchmark_values.append(value)\n",
    "\n",
    "        # Save logs\n",
    "        result_df = pd.DataFrame({\n",
    "            'Date': dates,\n",
    "            'PortfolioValue': portfolio_values,\n",
    "            'BenchmarkValue': benchmark_values\n",
    "        })\n",
    "        result_df['Date'] = pd.to_datetime(result_df['Date'], errors='coerce')\n",
    "        result_df.set_index('Date', inplace=True)\n",
    "\n",
    "        pd.DataFrame(trade_log).to_csv(\"../dataset/backtest_trade_log.csv\", index=False)\n",
    "        pd.DataFrame(value_log).to_csv(\"../dataset/backtest_value_log.csv\", index=False)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(result_df['PortfolioValue'], label='Strategy Portfolio')\n",
    "        plt.plot(result_df['BenchmarkValue'], label=f'{self.benchmark_symbol} Benchmark')\n",
    "        plt.title(\"Backtest Result: Strategy vs. Benchmark\")\n",
    "        plt.ylabel(\"Portfolio Value (USD)\")\n",
    "        plt.xlabel(\"Date\")\n",
    "\n",
    "        xticks_idx = result_df.index[::52]\n",
    "        xticks_labels = [str(d)[:4] if isinstance(d, str) else d.strftime('%Y') for d in xticks_idx]\n",
    "        plt.xticks(xticks_idx, xticks_labels)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nðŸ’° Final Portfolio Value: ${result_df['PortfolioValue'].iloc[-1]:,.2f}\")\n",
    "        print(f\"ðŸ“ˆ Final Benchmark Value: ${result_df['BenchmarkValue'].iloc[-1]:,.2f}\")\n",
    "        print(f\"ðŸŽ¯ Strategy Return: {((result_df['PortfolioValue'].iloc[-1] / self.initial_cash - 1) * 100):.2f}%\")\n",
    "        print(f\"ðŸ“Š Benchmark Return: {((result_df['BenchmarkValue'].iloc[-1] / self.initial_cash - 1) * 100):.2f}%\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# === Run the backtest\n",
    "backtester = ETFBacktester(\n",
    "    model_dir=\"../model_weights\",\n",
    "    data_dir=\"../dataset/normalized_matrix\",\n",
    "    price_path=\"../dataset/etf_prices_weekly.csv\",\n",
    "    initial_cash=2000,\n",
    "    benchmark_symbol=\"SPY\"\n",
    ")\n",
    "\n",
    "# Execute the backtest logic\n",
    "result_df = backtester.run()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stock_pred)",
   "language": "python",
   "name": "stock_pred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
